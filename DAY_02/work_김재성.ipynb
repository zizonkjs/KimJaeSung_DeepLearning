{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 보스턴 집값 예측\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "import torch                    # 텐서 및 수치 계산 함수 관련 모듈\n",
    "import torch.nn as nn           # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F    # 손실, 거리 등 함 수관련 모듈\n",
    "import torch.optim as optimizer # 최적화 기법 관련 모듈\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요약\n",
    "torch.nn.functional 모듈은 신경망을 구성하고 학습하는 데 필요한 함수형 API를 제공합니다. 활성화 함수, 손실 함수, 컨볼루션, 풀링, 드롭아웃 등 다양한 딥러닝 관련 연산을 함수형으로 직접 호출하여 사용할 수 있습니다.\n",
    "클래스 형태로 선언하는 방식(torch.nn)과 달리, torch.nn.functional은 필요한 연산을 함수로 직접 호출하는 방식을 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273.0   \n",
       "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273.0   \n",
       "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273.0   \n",
       "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273.0   \n",
       "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 준비\n",
    "FILE = r'C:\\Users\\zizonkjs\\머신러닝,딥러닝\\data\\boston.csv'\n",
    "bostondf = pd.read_csv(FILE)\n",
    "bostondf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[weight] Parameter containing:\n",
      "tensor([[-0.2330, -0.1675,  0.2681, -0.1396,  0.0639,  0.1008, -0.0245, -0.0416,\n",
      "         -0.2755, -0.2201,  0.1794, -0.1878, -0.0337]], requires_grad=True) \n",
      "\n",
      "[bias] Parameter containing:\n",
      "tensor([-0.0166], requires_grad=True) \n",
      "\n",
      "torch.Size([506, 13])\n",
      "torch.Size([506, 1])\n",
      "Train torch.Size([323, 13]), TEST torch.Size([102, 13]), VAL torch.Size([81, 13])\n",
      "Train torch.Size([323, 1]), TEST torch.Size([102, 1]), VAL torch.Size([81, 1])\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(13,1)\n",
    "\n",
    "# 가중치와 절편 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'[{name}] {param} \\n')\n",
    "\n",
    "# 최적화 인스턴스준비\n",
    "## 모델의 가중치와 절편을 최적화 ==> 인스턴스에 전달\n",
    "adam_optim = optimizer.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# 데이터셋 Tensor화 진행\n",
    "featurets = torch.from_numpy(bostondf[bostondf.columns[:-1]].values).float()\n",
    "print(featurets.shape)\n",
    "\n",
    "targetts = torch.from_numpy(bostondf[['MEDV']].values).float()\n",
    "print(targetts.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(featurets, targetts, test_size=0.2)\n",
    "\n",
    "# 검증용 만들기\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(f'Train {X_train.shape}, TEST {X_test.shape}, VAL {X_val.shape}')\n",
    "print(f'Train {y_train.shape}, TEST {y_test.shape}, VAL {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트/검증 함수 만들기\n",
    "def testing(test_ts, targetts, kind='val'):\n",
    "\n",
    "    with torch.no_grad(): # 가중치 및 절편 업데이트 진행 금지\n",
    "        # 1. 학습진행 forward\n",
    "        pre_y = model(test_ts)\n",
    "\n",
    "        # 2. 오차 계산 - 손실함수\n",
    "        loss = F.mse_loss(pre_y, targetts)\n",
    "\n",
    "        # 3. 학습결과 출력 및 저장\n",
    "        print(f'[{kind}] {kind} LOSS : {loss}')\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] val LOSS : 19.853435516357422\n",
      "[0/100] LOSS : 30.023555755615234\n",
      "[val] val LOSS : 19.84362030029297\n",
      "[1/100] LOSS : 30.016653060913086\n",
      "[val] val LOSS : 19.833803176879883\n",
      "[2/100] LOSS : 30.0097713470459\n",
      "[val] val LOSS : 19.824007034301758\n",
      "[3/100] LOSS : 30.002899169921875\n",
      "[val] val LOSS : 19.814220428466797\n",
      "[4/100] LOSS : 29.996047973632812\n",
      "[val] val LOSS : 19.804466247558594\n",
      "[5/100] LOSS : 29.98920249938965\n",
      "[val] val LOSS : 19.79473114013672\n",
      "[6/100] LOSS : 29.982370376586914\n",
      "[val] val LOSS : 19.785001754760742\n",
      "[7/100] LOSS : 29.975566864013672\n",
      "[val] val LOSS : 19.77528190612793\n",
      "[8/100] LOSS : 29.96877098083496\n",
      "[val] val LOSS : 19.76558494567871\n",
      "[9/100] LOSS : 29.961986541748047\n",
      "[val] val LOSS : 19.75591278076172\n",
      "[10/100] LOSS : 29.955217361450195\n",
      "[val] val LOSS : 19.746257781982422\n",
      "[11/100] LOSS : 29.948463439941406\n",
      "[val] val LOSS : 19.736608505249023\n",
      "[12/100] LOSS : 29.941720962524414\n",
      "[val] val LOSS : 19.72696876525879\n",
      "[13/100] LOSS : 29.93499755859375\n",
      "[val] val LOSS : 19.717361450195312\n",
      "[14/100] LOSS : 29.928281784057617\n",
      "[val] val LOSS : 19.707773208618164\n",
      "[15/100] LOSS : 29.921585083007812\n",
      "[val] val LOSS : 19.69819450378418\n",
      "[16/100] LOSS : 29.91490364074707\n",
      "[val] val LOSS : 19.688623428344727\n",
      "[17/100] LOSS : 29.908233642578125\n",
      "[val] val LOSS : 19.679065704345703\n",
      "[18/100] LOSS : 29.901578903198242\n",
      "[val] val LOSS : 19.66954231262207\n",
      "[19/100] LOSS : 29.894933700561523\n",
      "[val] val LOSS : 19.660030364990234\n",
      "[20/100] LOSS : 29.888309478759766\n",
      "[val] val LOSS : 19.650535583496094\n",
      "[21/100] LOSS : 29.881696701049805\n",
      "[val] val LOSS : 19.64104652404785\n",
      "[22/100] LOSS : 29.875097274780273\n",
      "[val] val LOSS : 19.63157081604004\n",
      "[23/100] LOSS : 29.868515014648438\n",
      "[val] val LOSS : 19.622129440307617\n",
      "[24/100] LOSS : 29.861942291259766\n",
      "[val] val LOSS : 19.612699508666992\n",
      "[25/100] LOSS : 29.855390548706055\n",
      "[val] val LOSS : 19.60328483581543\n",
      "[26/100] LOSS : 29.848840713500977\n",
      "[val] val LOSS : 19.593875885009766\n",
      "[27/100] LOSS : 29.842313766479492\n",
      "[val] val LOSS : 19.584487915039062\n",
      "[28/100] LOSS : 29.835803985595703\n",
      "[val] val LOSS : 19.57512092590332\n",
      "[29/100] LOSS : 29.829303741455078\n",
      "[val] val LOSS : 19.565773010253906\n",
      "[30/100] LOSS : 29.82281494140625\n",
      "[val] val LOSS : 19.55643653869629\n",
      "[31/100] LOSS : 29.816343307495117\n",
      "[val] val LOSS : 19.547107696533203\n",
      "[32/100] LOSS : 29.809879302978516\n",
      "[val] val LOSS : 19.537799835205078\n",
      "[33/100] LOSS : 29.803442001342773\n",
      "[val] val LOSS : 19.528512954711914\n",
      "[34/100] LOSS : 29.797008514404297\n",
      "[val] val LOSS : 19.519245147705078\n",
      "[35/100] LOSS : 29.790592193603516\n",
      "[val] val LOSS : 19.509984970092773\n",
      "[36/100] LOSS : 29.784189224243164\n",
      "[val] val LOSS : 19.500741958618164\n",
      "[37/100] LOSS : 29.77779769897461\n",
      "[val] val LOSS : 19.491512298583984\n",
      "[38/100] LOSS : 29.771427154541016\n",
      "[val] val LOSS : 19.48230743408203\n",
      "[39/100] LOSS : 29.76506233215332\n",
      "[val] val LOSS : 19.473114013671875\n",
      "[40/100] LOSS : 29.75872039794922\n",
      "[val] val LOSS : 19.463939666748047\n",
      "[41/100] LOSS : 29.75238037109375\n",
      "[val] val LOSS : 19.45476722717285\n",
      "[42/100] LOSS : 29.746057510375977\n",
      "[val] val LOSS : 19.44561767578125\n",
      "[43/100] LOSS : 29.739748001098633\n",
      "[val] val LOSS : 19.436487197875977\n",
      "[44/100] LOSS : 29.733455657958984\n",
      "[val] val LOSS : 19.427379608154297\n",
      "[45/100] LOSS : 29.7271728515625\n",
      "[val] val LOSS : 19.418283462524414\n",
      "[46/100] LOSS : 29.720911026000977\n",
      "[val] val LOSS : 19.409191131591797\n",
      "[47/100] LOSS : 29.714658737182617\n",
      "[val] val LOSS : 19.40011978149414\n",
      "[48/100] LOSS : 29.708412170410156\n",
      "[val] val LOSS : 19.39106559753418\n",
      "[49/100] LOSS : 29.702190399169922\n",
      "[val] val LOSS : 19.382034301757812\n",
      "[50/100] LOSS : 29.695980072021484\n",
      "[val] val LOSS : 19.373014450073242\n",
      "[51/100] LOSS : 29.68977928161621\n",
      "[val] val LOSS : 19.364002227783203\n",
      "[52/100] LOSS : 29.68359375\n",
      "[val] val LOSS : 19.35501480102539\n",
      "[53/100] LOSS : 29.677419662475586\n",
      "[val] val LOSS : 19.346040725708008\n",
      "[54/100] LOSS : 29.671266555786133\n",
      "[val] val LOSS : 19.337081909179688\n",
      "[55/100] LOSS : 29.665117263793945\n",
      "[val] val LOSS : 19.328140258789062\n",
      "[56/100] LOSS : 29.65898895263672\n",
      "[val] val LOSS : 19.319210052490234\n",
      "[57/100] LOSS : 29.65286636352539\n",
      "[val] val LOSS : 19.3102970123291\n",
      "[58/100] LOSS : 29.646764755249023\n",
      "[val] val LOSS : 19.301403045654297\n",
      "[59/100] LOSS : 29.640676498413086\n",
      "[val] val LOSS : 19.292526245117188\n",
      "[60/100] LOSS : 29.634593963623047\n",
      "[val] val LOSS : 19.283658981323242\n",
      "[61/100] LOSS : 29.628530502319336\n",
      "[val] val LOSS : 19.274805068969727\n",
      "[62/100] LOSS : 29.622482299804688\n",
      "[val] val LOSS : 19.265972137451172\n",
      "[63/100] LOSS : 29.616439819335938\n",
      "[val] val LOSS : 19.257152557373047\n",
      "[64/100] LOSS : 29.610414505004883\n",
      "[val] val LOSS : 19.248348236083984\n",
      "[65/100] LOSS : 29.604408264160156\n",
      "[val] val LOSS : 19.239564895629883\n",
      "[66/100] LOSS : 29.598403930664062\n",
      "[val] val LOSS : 19.230783462524414\n",
      "[67/100] LOSS : 29.592422485351562\n",
      "[val] val LOSS : 19.222023010253906\n",
      "[68/100] LOSS : 29.586450576782227\n",
      "[val] val LOSS : 19.213279724121094\n",
      "[69/100] LOSS : 29.58049201965332\n",
      "[val] val LOSS : 19.204553604125977\n",
      "[70/100] LOSS : 29.574542999267578\n",
      "[val] val LOSS : 19.195842742919922\n",
      "[71/100] LOSS : 29.568613052368164\n",
      "[val] val LOSS : 19.18714714050293\n",
      "[72/100] LOSS : 29.562692642211914\n",
      "[val] val LOSS : 19.1784610748291\n",
      "[73/100] LOSS : 29.55678367614746\n",
      "[val] val LOSS : 19.169801712036133\n",
      "[74/100] LOSS : 29.55088996887207\n",
      "[val] val LOSS : 19.161149978637695\n",
      "[75/100] LOSS : 29.54500961303711\n",
      "[val] val LOSS : 19.152509689331055\n",
      "[76/100] LOSS : 29.53914451599121\n",
      "[val] val LOSS : 19.143888473510742\n",
      "[77/100] LOSS : 29.53328514099121\n",
      "[val] val LOSS : 19.135284423828125\n",
      "[78/100] LOSS : 29.527442932128906\n",
      "[val] val LOSS : 19.126689910888672\n",
      "[79/100] LOSS : 29.5216121673584\n",
      "[val] val LOSS : 19.11811637878418\n",
      "[80/100] LOSS : 29.51580047607422\n",
      "[val] val LOSS : 19.109560012817383\n",
      "[81/100] LOSS : 29.509998321533203\n",
      "[val] val LOSS : 19.101011276245117\n",
      "[82/100] LOSS : 29.50420570373535\n",
      "[val] val LOSS : 19.092477798461914\n",
      "[83/100] LOSS : 29.498424530029297\n",
      "[val] val LOSS : 19.083959579467773\n",
      "[84/100] LOSS : 29.49266815185547\n",
      "[val] val LOSS : 19.075456619262695\n",
      "[85/100] LOSS : 29.486907958984375\n",
      "[val] val LOSS : 19.06697654724121\n",
      "[86/100] LOSS : 29.481172561645508\n",
      "[val] val LOSS : 19.058513641357422\n",
      "[87/100] LOSS : 29.475446701049805\n",
      "[val] val LOSS : 19.050052642822266\n",
      "[88/100] LOSS : 29.469736099243164\n",
      "[val] val LOSS : 19.04161262512207\n",
      "[89/100] LOSS : 29.464033126831055\n",
      "[val] val LOSS : 19.033185958862305\n",
      "[90/100] LOSS : 29.45834732055664\n",
      "[val] val LOSS : 19.02477264404297\n",
      "[91/100] LOSS : 29.452669143676758\n",
      "[val] val LOSS : 19.016382217407227\n",
      "[92/100] LOSS : 29.44700813293457\n",
      "[val] val LOSS : 19.00800132751465\n",
      "[93/100] LOSS : 29.44135856628418\n",
      "[val] val LOSS : 18.999629974365234\n",
      "[94/100] LOSS : 29.43572235107422\n",
      "[val] val LOSS : 18.99127769470215\n",
      "[95/100] LOSS : 29.430091857910156\n",
      "[val] val LOSS : 18.98293685913086\n",
      "[96/100] LOSS : 29.424484252929688\n",
      "[val] val LOSS : 18.974620819091797\n",
      "[97/100] LOSS : 29.41888427734375\n",
      "[val] val LOSS : 18.966320037841797\n",
      "[98/100] LOSS : 29.413297653198242\n",
      "[val] val LOSS : 18.958024978637695\n",
      "[99/100] LOSS : 29.407726287841797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[30.023555755615234,\n",
       "  30.016653060913086,\n",
       "  30.0097713470459,\n",
       "  30.002899169921875,\n",
       "  29.996047973632812,\n",
       "  29.98920249938965,\n",
       "  29.982370376586914,\n",
       "  29.975566864013672,\n",
       "  29.96877098083496,\n",
       "  29.961986541748047,\n",
       "  29.955217361450195,\n",
       "  29.948463439941406,\n",
       "  29.941720962524414,\n",
       "  29.93499755859375,\n",
       "  29.928281784057617,\n",
       "  29.921585083007812,\n",
       "  29.91490364074707,\n",
       "  29.908233642578125,\n",
       "  29.901578903198242,\n",
       "  29.894933700561523,\n",
       "  29.888309478759766,\n",
       "  29.881696701049805,\n",
       "  29.875097274780273,\n",
       "  29.868515014648438,\n",
       "  29.861942291259766,\n",
       "  29.855390548706055,\n",
       "  29.848840713500977,\n",
       "  29.842313766479492,\n",
       "  29.835803985595703,\n",
       "  29.829303741455078,\n",
       "  29.82281494140625,\n",
       "  29.816343307495117,\n",
       "  29.809879302978516,\n",
       "  29.803442001342773,\n",
       "  29.797008514404297,\n",
       "  29.790592193603516,\n",
       "  29.784189224243164,\n",
       "  29.77779769897461,\n",
       "  29.771427154541016,\n",
       "  29.76506233215332,\n",
       "  29.75872039794922,\n",
       "  29.75238037109375,\n",
       "  29.746057510375977,\n",
       "  29.739748001098633,\n",
       "  29.733455657958984,\n",
       "  29.7271728515625,\n",
       "  29.720911026000977,\n",
       "  29.714658737182617,\n",
       "  29.708412170410156,\n",
       "  29.702190399169922,\n",
       "  29.695980072021484,\n",
       "  29.68977928161621,\n",
       "  29.68359375,\n",
       "  29.677419662475586,\n",
       "  29.671266555786133,\n",
       "  29.665117263793945,\n",
       "  29.65898895263672,\n",
       "  29.65286636352539,\n",
       "  29.646764755249023,\n",
       "  29.640676498413086,\n",
       "  29.634593963623047,\n",
       "  29.628530502319336,\n",
       "  29.622482299804688,\n",
       "  29.616439819335938,\n",
       "  29.610414505004883,\n",
       "  29.604408264160156,\n",
       "  29.598403930664062,\n",
       "  29.592422485351562,\n",
       "  29.586450576782227,\n",
       "  29.58049201965332,\n",
       "  29.574542999267578,\n",
       "  29.568613052368164,\n",
       "  29.562692642211914,\n",
       "  29.55678367614746,\n",
       "  29.55088996887207,\n",
       "  29.54500961303711,\n",
       "  29.53914451599121,\n",
       "  29.53328514099121,\n",
       "  29.527442932128906,\n",
       "  29.5216121673584,\n",
       "  29.51580047607422,\n",
       "  29.509998321533203,\n",
       "  29.50420570373535,\n",
       "  29.498424530029297,\n",
       "  29.49266815185547,\n",
       "  29.486907958984375,\n",
       "  29.481172561645508,\n",
       "  29.475446701049805,\n",
       "  29.469736099243164,\n",
       "  29.464033126831055,\n",
       "  29.45834732055664,\n",
       "  29.452669143676758,\n",
       "  29.44700813293457,\n",
       "  29.44135856628418,\n",
       "  29.43572235107422,\n",
       "  29.430091857910156,\n",
       "  29.424484252929688,\n",
       "  29.41888427734375,\n",
       "  29.413297653198242,\n",
       "  29.407726287841797],\n",
       " [19.853435516357422,\n",
       "  19.84362030029297,\n",
       "  19.833803176879883,\n",
       "  19.824007034301758,\n",
       "  19.814220428466797,\n",
       "  19.804466247558594,\n",
       "  19.79473114013672,\n",
       "  19.785001754760742,\n",
       "  19.77528190612793,\n",
       "  19.76558494567871,\n",
       "  19.75591278076172,\n",
       "  19.746257781982422,\n",
       "  19.736608505249023,\n",
       "  19.72696876525879,\n",
       "  19.717361450195312,\n",
       "  19.707773208618164,\n",
       "  19.69819450378418,\n",
       "  19.688623428344727,\n",
       "  19.679065704345703,\n",
       "  19.66954231262207,\n",
       "  19.660030364990234,\n",
       "  19.650535583496094,\n",
       "  19.64104652404785,\n",
       "  19.63157081604004,\n",
       "  19.622129440307617,\n",
       "  19.612699508666992,\n",
       "  19.60328483581543,\n",
       "  19.593875885009766,\n",
       "  19.584487915039062,\n",
       "  19.57512092590332,\n",
       "  19.565773010253906,\n",
       "  19.55643653869629,\n",
       "  19.547107696533203,\n",
       "  19.537799835205078,\n",
       "  19.528512954711914,\n",
       "  19.519245147705078,\n",
       "  19.509984970092773,\n",
       "  19.500741958618164,\n",
       "  19.491512298583984,\n",
       "  19.48230743408203,\n",
       "  19.473114013671875,\n",
       "  19.463939666748047,\n",
       "  19.45476722717285,\n",
       "  19.44561767578125,\n",
       "  19.436487197875977,\n",
       "  19.427379608154297,\n",
       "  19.418283462524414,\n",
       "  19.409191131591797,\n",
       "  19.40011978149414,\n",
       "  19.39106559753418,\n",
       "  19.382034301757812,\n",
       "  19.373014450073242,\n",
       "  19.364002227783203,\n",
       "  19.35501480102539,\n",
       "  19.346040725708008,\n",
       "  19.337081909179688,\n",
       "  19.328140258789062,\n",
       "  19.319210052490234,\n",
       "  19.3102970123291,\n",
       "  19.301403045654297,\n",
       "  19.292526245117188,\n",
       "  19.283658981323242,\n",
       "  19.274805068969727,\n",
       "  19.265972137451172,\n",
       "  19.257152557373047,\n",
       "  19.248348236083984,\n",
       "  19.239564895629883,\n",
       "  19.230783462524414,\n",
       "  19.222023010253906,\n",
       "  19.213279724121094,\n",
       "  19.204553604125977,\n",
       "  19.195842742919922,\n",
       "  19.18714714050293,\n",
       "  19.1784610748291,\n",
       "  19.169801712036133,\n",
       "  19.161149978637695,\n",
       "  19.152509689331055,\n",
       "  19.143888473510742,\n",
       "  19.135284423828125,\n",
       "  19.126689910888672,\n",
       "  19.11811637878418,\n",
       "  19.109560012817383,\n",
       "  19.101011276245117,\n",
       "  19.092477798461914,\n",
       "  19.083959579467773,\n",
       "  19.075456619262695,\n",
       "  19.06697654724121,\n",
       "  19.058513641357422,\n",
       "  19.050052642822266,\n",
       "  19.04161262512207,\n",
       "  19.033185958862305,\n",
       "  19.02477264404297,\n",
       "  19.016382217407227,\n",
       "  19.00800132751465,\n",
       "  18.999629974365234,\n",
       "  18.99127769470215,\n",
       "  18.98293685913086,\n",
       "  18.974620819091797,\n",
       "  18.966320037841797,\n",
       "  18.958024978637695]]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "\n",
    "# 모델 학습 함수\n",
    "def training(featurets, targetts, valts, valtargetts):\n",
    "\n",
    "    loss_history= [[],[]]\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        # 1. 학습 진행 forward\n",
    "        pre_y = model(featurets)\n",
    "\n",
    "        # 2. 오차 계산 - 손실함수\n",
    "        loss = F.mse_loss(pre_y, targetts)\n",
    "        loss_history[0].append(loss.item())\n",
    "\n",
    "        # 3. 최적화 - 가중치, 절편 업데이트 backward\n",
    "        adam_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        adam_optim.step()\n",
    "\n",
    "        # 4. 검증 : 모델이 제대로 만들어지는 검사\n",
    "        val_loss = testing(valts, valtargetts, kind='val')\n",
    "        loss_history[1].append(val_loss.item())\n",
    "\n",
    "        # 5. 학습결과 출력 및 저장\n",
    "        print(f'[{epoch}/{EPOCH}] LOSS : {loss}')\n",
    "    \n",
    "    return loss_history\n",
    "loss = training(X_train, y_train, X_val, y_val)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 후 loss 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss[0], label='Train')\n",
    "plt.plot(loss[1], label='Val')\n",
    "plt.title('Train & Val')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 생소한 것들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adam_optim.zero_grad()는 PyTorch에서 Adam 옵티마이저를 사용할 때, 기울기(gradient)를 초기화하는 중요한 단계입니다. 이 함수는 역전파(backpropagation)를 통해 계산된 기울기 값을 누적하지 않고 매번 초기화하여, 각 미니배치마다 새로운 기울기를 계산하도록 해줍니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`adam_optim.step()`는 PyTorch에서 **Adam 옵티마이저**를 사용할 때, **모델의 파라미터(가중치)를 업데이트**하는 함수입니다. 학습 과정에서 **역전파(backpropagation)**로 계산된 기울기를 사용해 모델의 파라미터를 **최적화**하는 역할을 합니다.\n",
    "\n",
    "### 배경 설명\n",
    "\n",
    "딥러닝에서 학습은 **손실 함수**를 최소화하기 위해, 모델의 파라미터를 조정하는 과정입니다. 이 과정에서 **경사하강법(Gradient Descent)**을 사용하여 손실 함수의 기울기를 계산하고, 이를 바탕으로 파라미터를 업데이트합니다. Adam은 경사하강법의 변형 알고리즘 중 하나로, 가중치를 업데이트하는 **효율적이고 빠른 방법**입니다.\n",
    "\n",
    "### 코드 설명\n",
    "\n",
    "```python\n",
    "adam_optim.step()\n",
    "```\n",
    "\n",
    "- **`adam_optim`**: **Adam 옵티마이저** 객체입니다. `torch.optim.Adam()`으로 정의된 옵티마이저로, 모델의 파라미터를 업데이트하는 데 사용됩니다.\n",
    "  \n",
    "  ```python\n",
    "  adam_optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  ```\n",
    "\n",
    "- **`step()`**: **옵티마이저가 계산된 기울기**를 기반으로 모델의 파라미터를 업데이트하는 함수입니다. 이 함수는 `loss.backward()`로 역전파를 통해 계산된 기울기를 사용하여 **파라미터를 최적화**합니다.\n",
    "\n",
    "### 학습 과정에서의 역할\n",
    "\n",
    "1. **기울기 계산**:\n",
    "   - `loss.backward()`를 사용해 **손실 함수의 기울기**를 계산합니다. 이 기울기는 모델의 각 파라미터에 대해 계산되며, 모델이 어떻게 학습해야 하는지 방향을 제공합니다.\n",
    "\n",
    "2. **파라미터 업데이트**:\n",
    "   - `adam_optim.step()`는 계산된 기울기를 사용하여, 모델의 **가중치와 편향** 같은 파라미터를 업데이트합니다.\n",
    "   - Adam 옵티마이저는 파라미터의 기울기를 기반으로, 가중치를 어떻게 업데이트할지 자동으로 결정하며, **모멘텀**과 **적응형 학습률** 등의 기술을 적용합니다.\n",
    "\n",
    "### 학습 루프에서의 `step()`과 `zero_grad()`의 관계\n",
    "\n",
    "```python\n",
    "for data, target in dataloader:\n",
    "    adam_optim.zero_grad()  # 기울기 초기화\n",
    "    output = model(data)    # 모델의 예측값 계산\n",
    "    loss = loss_fn(output, target)  # 손실 계산\n",
    "    loss.backward()  # 역전파로 기울기 계산\n",
    "    adam_optim.step()  # 파라미터 업데이트\n",
    "```\n",
    "\n",
    "1. **`adam_optim.zero_grad()`**: 역전파 전에 파라미터의 **기울기를 0으로 초기화**합니다. 이전 미니배치에서 계산된 기울기가 누적되지 않도록 합니다.\n",
    "2. **`loss.backward()`**: 손실 함수의 기울기를 계산하여 각 파라미터에 대해 저장합니다.\n",
    "3. **`adam_optim.step()`**: 저장된 기울기를 사용하여 **파라미터를 업데이트**합니다. 이 단계에서 파라미터가 학습되고 최적화됩니다.\n",
    "\n",
    "### Adam 옵티마이저의 특징\n",
    "\n",
    "- **Adam(Adaptive Moment Estimation)**은 경사하강법의 변형 알고리즘으로, **모멘텀(Momentum)**과 **적응형 학습률(Adaptive Learning Rate)**을 결합한 방법입니다.\n",
    "  - **모멘텀**: 기울기의 과거 정보를 사용하여, 더 빠르게 최적화하는 방법입니다.\n",
    "  - **적응형 학습률**: 각 파라미터마다 학습률을 자동으로 조정하여, 변화가 많은 곳에서는 빠르게, 안정적인 곳에서는 느리게 학습합니다.\n",
    "\n",
    "Adam 옵티마이저는 일반적인 경사하강법(SGD)보다 **더 빠르게 수렴**하며, 다양한 문제에서 **잘 동작**하는 것으로 알려져 있습니다.\n",
    "\n",
    "### 요약:\n",
    "- **`adam_optim.step()`**는 **Adam 옵티마이저**를 사용해 **기울기를 기반으로 모델의 파라미터를 업데이트**하는 함수입니다.\n",
    "- 학습 과정에서 역전파로 계산된 기울기를 바탕으로, 모델의 가중치와 편향 등을 업데이트하여 모델이 학습하도록 돕습니다.\n",
    "- `zero_grad()`로 기울기를 초기화하고, `backward()`로 기울기를 계산한 후, `step()`을 통해 파라미터를 업데이트하는 흐름이 기본적인 학습 과정입니다.\n",
    "\n",
    "이 과정을 통해 모델이 데이터를 더 잘 학습하고, 최적의 성능을 낼 수 있도록 도와줍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
