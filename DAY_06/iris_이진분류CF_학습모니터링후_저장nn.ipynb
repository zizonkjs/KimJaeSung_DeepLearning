{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN 기반 이진분류 모델 구현\n",
    "- 사용되는 데이터셋 : iris.csv\n",
    "- feature : 3개\n",
    "- target : 1개 Setosa와 나머지\n",
    "- 학습방법 : 지도학습 -> 분류 > 이진분류\n",
    "- 알고리즘 : 인공신경망(ANN) -> 심층(은닉층) 신경망 -> MLP(층이여러개), DNN(은닉층이 많은 구성) \n",
    "- FramWork : Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "# 모델관련 모듈\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import F1Score, BinaryF1Score\n",
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "from torchinfo import summary\n",
    "\n",
    "# 데이터 전처리 및 시각화 모듈\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch  2.4.1\n",
      " pandas  2.0.3\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크\n",
    "def versioncheck():\n",
    "    print(f' torch  {torch.__version__}')\n",
    "    print(f' pandas  {pd.__version__}')\n",
    "\n",
    "versioncheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width    variety\n",
       "0             5.1          3.5           1.4          0.2     Setosa\n",
       "1             4.9          3.0           1.4          0.2     Setosa\n",
       "2             4.7          3.2           1.3          0.2     Setosa\n",
       "3             4.6          3.1           1.5          0.2     Setosa\n",
       "4             5.0          3.6           1.4          0.2     Setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  Virginica\n",
       "146           6.3          2.5           5.0          1.9  Virginica\n",
       "147           6.5          3.0           5.2          2.0  Virginica\n",
       "148           6.2          3.4           5.4          2.3  Virginica\n",
       "149           5.9          3.0           5.1          1.8  Virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILE = r'C:\\Users\\zizonkjs\\머신러닝,딥러닝\\data\\iris.csv'\n",
    "irisdf=pd.read_csv(DATA_FILE)\n",
    "irisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       True\n",
       "1       True\n",
       "2       True\n",
       "3       True\n",
       "4       True\n",
       "       ...  \n",
       "145    False\n",
       "146    False\n",
       "147    False\n",
       "148    False\n",
       "149    False\n",
       "Name: variety, Length: 150, dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 타겟 변경 => 정수화, 클래스 3개 => 2개\n",
    "irisdf['variety'].unique()\n",
    "irisdf['variety'] == 'Setosa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0             5.1          3.5           1.4          0.2        1\n",
       "1             4.9          3.0           1.4          0.2        1\n",
       "2             4.7          3.2           1.3          0.2        1\n",
       "3             4.6          3.1           1.5          0.2        1\n",
       "4             5.0          3.6           1.4          0.2        1\n",
       "..            ...          ...           ...          ...      ...\n",
       "145           6.7          3.0           5.2          2.3        0\n",
       "146           6.3          2.5           5.0          1.9        0\n",
       "147           6.5          3.0           5.2          2.0        0\n",
       "148           6.2          3.4           5.4          2.3        0\n",
       "149           5.9          3.0           5.1          1.8        0\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisdf['variety']=(irisdf['variety'] == 'Setosa')\n",
    "irisdf['variety']=irisdf['variety'].astype('int')\n",
    "irisdf['variety']\n",
    "irisdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] - 모델 클레스 설계 및 정의 - <hr>\n",
    "어떤 클래스를 만들까? 고려\n",
    "클래스 목적 : iris.DataSet 학습 후 추론\n",
    "클래스 이름 : IrisBCFModel\n",
    "부모 클래스 : nn.Module\n",
    "매개 변수   : 층별 입출력 갯수 고정하기 때문에 필요 없음\n",
    "클래스 속성 : featureDF, targetDF, n_rows, n_features\n",
    "클래스 기능 : __init__() : 모델 구조, forward() : 순방향 학습 <= 오버라이딩(overriding) 상속관계일 때\n",
    "\n",
    "클래스 구조  \n",
    "    -입력층 : 피쳐 4개  퍼셉트론 : 50개(보통 입력 때 많이 주고 갈수록 줄임) (4,10)\n",
    "    -은닉층 : 입력 10개     출력 5개   (10,5)\n",
    "    -출력층 : 입력5개      타겟(이진분류) 1개    (5,1)  \n",
    "  \n",
    "-손실 함수/ 활성화 함수\n",
    "    *클래스 형태 ==> nn.MESLoss, nn.ReLU ==> __init__() 메서드에 사용\n",
    "    *함수 형태 ==> torch.nn.functional => forward()메서드에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisBFCModel(nn.Module):\n",
    "\n",
    "    # 모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(4, 10)\n",
    "        self.hidden_layer=nn.Linear(10, 5)\n",
    "        self.out_layer=nn.Linear(5, 1)\n",
    "    \n",
    "    # 순방향 학습 진행 메서드\n",
    "    def forward(self, input_data):\n",
    "        # 입력층\n",
    "        y=self.in_layer(input_data) # f1w1+f2w2+f3w3+b 요런 식이 10개(숫자10개)\n",
    "        y=F.relu(y)                   # 범위 0이상\n",
    "        \n",
    "        # 은닉층 : 10개의 숫자 받아오기\n",
    "        y=self.hidden_layer(y)\n",
    "        y=F.relu(y)\n",
    "\n",
    "        # 출력층 : 5개의 숫자 값 => sigmoid 이진분류\n",
    "        return F.sigmoid(self.out_layer(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisBFCModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model = IrisBFCModel()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "IrisBFCModel                             --\n",
       "├─Linear: 1-1                            50\n",
       "├─Linear: 1-2                            55\n",
       "├─Linear: 1-3                            6\n",
       "=================================================================\n",
       "Total params: 111\n",
       "Trainable params: 111\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터셋 클래스 설계 및 정의 <hr>\n",
    "- 데이터셋 : iris.csv\n",
    "- 피쳐개수 : 3개\n",
    "- 타겟개수 : 1개\n",
    "- 클래스이름 : IrisDataSet\n",
    "- 부모클래스 : utils.data.DataSet\n",
    "- 속성__필드 : featureDF, targetDF, n_rows, n_featrues  \n",
    "- 필수 메서드:   \n",
    "    *__init__(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정  \n",
    "    *__len__(self) : 데이터의 개수 반환  \n",
    "    *__getItem__(self, index) : 특정 인덱스의 피쳐와 타겟 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF=featureDF\n",
    "        self.targetDF=targetDF\n",
    "        self.n_rows=featureDF.shape[0]\n",
    "        self.n_features=featureDF.shape[1]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 텐서화\n",
    "        featureTS=torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS=torch.FloatTensor(self.targetDF.iloc[index].values)\n",
    "        return featureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "# [테스트] 데이터셋 인스턴스 생성\n",
    "featureDF = irisdf[irisdf.columns[:-1]] # 2D (150,3)\n",
    "targetDF = irisdf[irisdf.columns[-1:]] # 1D(150,1)\n",
    "\n",
    "\n",
    "irisDS=IrisDataset(featureDF,targetDF)\n",
    "\n",
    "# 데이터로더 인스턴스 생성\n",
    "irisDL = DataLoader(irisDS, batch_size=1)\n",
    "for feature, label in irisDL:\n",
    "    print(feature.shape, label.shape, feature, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비\n",
    "- 학습 횟수 : EPOCH ( 처음부터 끝까지 공부할 횟수 )\n",
    "- 배치 크기 : BATCH_SIZE(학습량)\n",
    "- 위치 지정 : DEVICE (텐서 저장 및 실행 위치 GPU/CPU)\n",
    "- 학 습 률  : 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정 0.001~0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 관련 설정 값\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스 : 모델, 데이터 셋, 최적화 (, 성능지표) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n",
      "variety\n",
      "0          0.702381\n",
      "1          0.297619\n",
      "Name: count, dtype: float64 variety\n",
      "0          0.657895\n",
      "1          0.342105\n",
      "Name: count, dtype: float64 variety\n",
      "0          0.571429\n",
      "1          0.428571\n",
      "Name: count, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스\n",
    "# 학습용, 검증용, 테스트용 데이터 분리\n",
    "model = IrisBFCModel()\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(featureDF, targetDF, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1)\n",
    "print(f'{X_train.shape} {X_test.shape} {X_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "print(f'{y_train.value_counts()/y_train.shape[0]} {y_test.value_counts()/y_test.shape[0]} {y_val.value_counts()/y_val.shape[0]}')\n",
    "print(f'{type(X_train)} {type(X_test)} {type(X_val)}')\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
    "trainDS = IrisDataset(X_train, y_train)\n",
    "testDS = IrisDataset(X_test, y_test)\n",
    "valDS = IrisDataset(X_val, y_val)\n",
    "\n",
    "# 학습용 데이터 로더 인스턴스\n",
    "trainDL=DataLoader(trainDS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화 & 손실함수 인스턴스 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 최적화 인스턴스 => w, b model.parameter 전달\n",
    "optimizer=optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# 바이너리용 손실함수 인스턴스 => 분류\n",
    "# 예측값을 확률값으로 전달함 ==> sigmoid() AF 처리 후 전달\n",
    "reqLoss=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainDL), trainDL.__len__()\n",
    "## 모델 파일 관련\n",
    "\n",
    "### models 폴더 아래 프로젝트 폴더 아래 모델 파일을 저장\n",
    "import os\n",
    "\n",
    "# 저장 경로\n",
    "SAVE_PATH = '../models/iris/BCF/'\n",
    "\n",
    "# 저장 파일명\n",
    "SAVE_FILE = SAVE_PATH + 'model_train_wbs.pth'\n",
    "\n",
    "# 모델 구조 및 파라미터 모두 저장 파일명명\n",
    "SAVE_MODEL=SAVE_PATH+'model_all.pth'\n",
    "\n",
    "# 경로상 폴더 존재 여부 체크\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH) # 폴더 / 폴더 / ... 하위 폴더까지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT => 15.0\n",
      "성능 및 손실 개선이 없어서 학습 중단\n"
     ]
    }
   ],
   "source": [
    "# 학습의 효과 확인 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTORY=[[],[]], [[],[]]\n",
    "CNT = irisDS.n_rows / BATCH_SIZE\n",
    "print(f'CNT => {CNT}')\n",
    "\n",
    "# 학습 모니터링 / 스케쥴링 설정\n",
    "# => LOSS_HISTORY, SCORE_HISTORY 활용\n",
    "# => 임계기준 : 10번\n",
    "BREAK_CNT = 0\n",
    "LIMIT_VALUE = 9\n",
    "# 학습 모드로 모델 설정\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    # 학습 모드로 모델 설정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total = 0,0\n",
    "\n",
    "    for featureTS, targetTS in trainDL:\n",
    "\n",
    "        #학습 진행\n",
    "        pre_y=model(featureTS)\n",
    "\n",
    "        #손실 계산\n",
    "        loss=reqLoss(pre_y, targetTS)\n",
    "        loss_total += loss.item()\n",
    "\n",
    "\n",
    "        #성능평가 계산\n",
    "        score=F1Score(task='binary')(pre_y, targetTS)\n",
    "        score_total += score.item()\n",
    "\n",
    "        #최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에폭당 검증 기능을 키겠다.\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 검증 데이터셋\n",
    "        val_featureTS=torch.FloatTensor(valDS.featureDF.values)\n",
    "        val_targetTS=torch.FloatTensor(valDS.targetDF.values)\n",
    "        \n",
    "        #평가\n",
    "        pre_val=model(val_featureTS)\n",
    "\n",
    "        #손실\n",
    "        loss_val=reqLoss(pre_val, val_targetTS)\n",
    "\n",
    "        # 성능평가\n",
    "        score_val = F1Score(task='binary')(pre_val, val_targetTS)\n",
    "\n",
    "    # 에폭당 손실값과 성능평가 값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/BATCH_SIZE)\n",
    "    SCORE_HISTORY[0].append(score_total/BATCH_SIZE)\n",
    "\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "\n",
    "        # # 손실 기준\n",
    "    if len(LOSS_HISTORY[0]) >= 2:\n",
    "       if LOSS_HISTORY[1][-1] <= LOSS_HISTORY[1][-2] : BREAK_CNT +=1\n",
    "\n",
    "    # 성능 기준\n",
    "    if len(SCORE_HISTORY[1]) == 1: # 첫번째 횟수 저장\n",
    "       torch.save(model.state_dict(), SAVE_FILE)\n",
    "\n",
    "       # 모델 전체 저장\n",
    "       torch.save(model, SAVE_MODEL)\n",
    "    else:\n",
    "        if SCORE_HISTORY[1][-1] > max(SCORE_HISTORY[1][:-1]) : # 첫번째 점수랑 두번째 점수 비교 후 더 성능이 큰쪽을 저장\n",
    "            torch.save(model.state_dict(), SAVE_FILE)\n",
    "            torch.save(model, SAVE_MODEL)\n",
    "\n",
    "    # 성능이 좋은 학습 가중치 저장\n",
    "\n",
    "    # 학습중단\n",
    "    if BREAK_CNT >10:\n",
    "        print('성능 및 손실 개선이 없어서 학습 중단') \n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5720636069774627, 0.5653235971927643, 0.5592088580131531, 0.5531753659248352, 0.5468607395887375, 0.5392442911863327, 0.5270370841026306, 0.514256164431572, 0.5033300220966339, 0.49443804919719697, 0.48674654960632324, 0.47911277413368225, 0.47130192816257477, 0.4632317006587982, 0.45495554208755495, 0.44640432894229887, 0.4375766098499298, 0.4287372827529907, 0.42016775012016294, 0.41176167130470276, 0.4032977223396301, 0.3949081748723984, 0.386570480465889, 0.37834618985652924, 0.3702303320169449, 0.36220566034317014, 0.35438972562551496, 0.3467731952667236, 0.3390445336699486, 0.33218401968479155, 0.3258331149816513, 0.3199864149093628, 0.3143597334623337, 0.3089700400829315, 0.30375444889068604, 0.298791241645813, 0.2941936954855919, 0.2897902399301529, 0.28556986153125763, 0.28151207566261294, 0.2776397839188576, 0.27402557283639906, 0.27054073214530944, 0.2671940788626671, 0.2639585554599762, 0.2608431622385979, 0.25791520476341245, 0.25505183860659597, 0.25227373018860816, 0.24960597828030587, 0.2469853550195694, 0.24447233676910402, 0.24204462617635727, 0.23967305794358254, 0.2373596377670765, 0.23511796742677687, 0.2329430565237999, 0.23081664144992828, 0.22872767224907875, 0.22670701295137405, 0.22475622296333314, 0.22284286171197892, 0.22097093760967254, 0.21913596689701081, 0.21733218804001808, 0.21555466800928116, 0.21379977241158485, 0.2120650440454483, 0.210371932387352, 0.20877006351947786, 0.2070993773639202, 0.20552477464079857, 0.20394555404782294, 0.20241427794098854, 0.20087796077132225, 0.19943726137280465, 0.1979535661637783, 0.19651422947645186, 0.19512518048286437, 0.19370594769716262, 0.19233587235212327, 0.19099543541669844, 0.18963813483715058, 0.18835088461637498, 0.18706417605280876, 0.18575609102845192, 0.1844945214688778, 0.1832555539906025, 0.18203071728348733, 0.18076816722750663, 0.17962042465806008, 0.17842091023921966, 0.17719649747014046, 0.17607976943254472, 0.1749391607940197, 0.1737686075270176, 0.1726495325565338, 0.17155794203281402, 0.1704266533255577, 0.16934453696012497, 0.16827986389398575, 0.16718061342835427, 0.16613563522696495, 0.16507976725697518, 0.16405513286590576, 0.1630350187420845, 0.16198042407631874, 0.16099170073866845, 0.16000737324357034, 0.15901871994137765, 0.15803464278578758, 0.15703156404197216, 0.15607958920300008, 0.1551341149955988, 0.15418914146721363, 0.15324947722256183, 0.1523164790123701, 0.15139168165624142, 0.15047467835247516, 0.14956523664295673, 0.1486631166189909, 0.14776811972260476, 0.14688008017838, 0.1459988124668598, 0.1451242219656706, 0.14425619058310984, 0.1433946155011654, 0.1425394058227539, 0.14169048964977266, 0.14084777794778347, 0.14001196771860122, 0.13918186835944651, 0.1383576039224863, 0.13753924183547497, 0.13672675304114817, 0.13592010773718358, 0.1351192682981491, 0.1343241896480322, 0.13353919237852097, 0.1328067161142826, 0.132009656727314, 0.13122834116220475, 0.13045677170157433, 0.12969534620642661, 0.12899518199265003, 0.12822789549827576, 0.12747325450181962, 0.126729803532362, 0.1260512698441744, 0.12530503198504447, 0.12458012737333775, 0.12387544475495815, 0.12321750707924366, 0.12248801253736019, 0.1217686764895916, 0.1210963387042284, 0.12043783813714981, 0.11972835399210453, 0.11905820667743683, 0.11842116564512253, 0.11772455014288426, 0.11706858053803444, 0.1164423156529665, 0.11576251536607743, 0.11511818878352642, 0.11448380500078201, 0.11386790946125984, 0.11321227587759494, 0.11259355172514915, 0.11197740882635117, 0.1113743394613266, 0.11076187714934349, 0.11012007445096969, 0.10952892117202281, 0.10894107893109321, 0.10833427868783474, 0.10774629265069961, 0.10716033689677715, 0.10657249130308628, 0.10599899813532829, 0.10543902739882469, 0.10483614914119244, 0.10430478453636169, 0.1037040501832962, 0.10315844863653183, 0.10260245352983474, 0.10205070562660694, 0.10150142312049866, 0.10098376534879208, 0.10040213167667389, 0.09990242160856724, 0.0993339478969574, 0.0988318920135498, 0.09827857464551926, 0.09777591489255429, 0.09724579378962517, 0.09673358984291554, 0.09621934406459332, 0.09571919701993466, 0.09520257115364075, 0.09470628090202808, 0.09422656968235969, 0.09369277805089951, 0.09322214908897877, 0.09271428622305393, 0.09223273284733295, 0.09175174199044704, 0.09126734957098961, 0.09081135429441929, 0.09031752310693264, 0.08985286355018615, 0.08938162103295326, 0.0889120228588581, 0.08847102597355842, 0.0879939142614603, 0.0875432848930359, 0.08708461485803128, 0.0866353053599596, 0.08619929514825345, 0.08574639596045017, 0.0853111319243908, 0.0848839309066534, 0.08442341201007367, 0.08399806097149849, 0.08355884030461311, 0.08313429020345212, 0.08271392323076725, 0.08229249753057957, 0.08188009336590767, 0.08145915698260069, 0.08104656152427196, 0.08062357213348151, 0.08022242002189159, 0.07981715817004442, 0.0793922508135438, 0.07899403180927038, 0.07861413955688476, 0.07820063047111034, 0.0778126671910286, 0.07742727473378182, 0.07701675184071063, 0.07663096729665994, 0.0762661151587963, 0.0758681382983923, 0.07549508884549141, 0.07511881981045007, 0.07472631875425577, 0.07435821853578091, 0.0739933330565691, 0.07361286450177432, 0.07326001822948455, 0.07289064973592758, 0.07253645490854979, 0.07217478342354297, 0.07180112116038799, 0.07144919112324714, 0.0711099211126566, 0.07074358630925418, 0.07039490267634392, 0.07004658691585064, 0.06970144100487233, 0.06936113294214011, 0.06902159936726093, 0.06869044993072748, 0.0683431800454855, 0.06803156267851591, 0.06766567267477512, 0.06734159383922815, 0.06701698042452335, 0.06668512132018804, 0.06636755652725697, 0.06603498850017786, 0.06573762558400631, 0.065386145375669, 0.06507581043988467, 0.06475883759558201, 0.06443777959793806, 0.06413743887096643, 0.06381332147866488, 0.06350852083414793, 0.06320542767643929, 0.06289861276745796, 0.06259274482727051, 0.0622983630746603, 0.06199203561991453, 0.06169856376945972, 0.06139658130705357, 0.06111409515142441, 0.06080580074340105, 0.060519130900502205, 0.060229777731001376, 0.0599428329616785, 0.05966476257890463, 0.059373493678867816, 0.059089858643710615, 0.058804546110332014, 0.058532381802797316, 0.05824781507253647, 0.05797578636556864, 0.05769223757088184, 0.057426501996815205, 0.0571486447006464, 0.056883298046886924, 0.056606019288301467, 0.05634645279496908, 0.05607505496591329, 0.05581249892711639, 0.05554490126669407, 0.0552912924438715, 0.05502609070390463, 0.05476525537669659, 0.05450056977570057, 0.05425334591418505, 0.05399069990962744, 0.0537399934604764, 0.0534868473187089, 0.05324128568172455, 0.052989868260920045, 0.05274042580276728, 0.052515074238181116, 0.0522501653060317, 0.052015256322920325, 0.05176943875849247, 0.05153000820428133, 0.051281881704926494, 0.05105018522590399, 0.05080857817083597, 0.050572792254388335, 0.0503434095531702, 0.05010950695723295, 0.04988360833376646, 0.049648951180279254, 0.04942211918532848, 0.04918502867221832, 0.04896329157054424, 0.04873502813279629, 0.048509777151048185, 0.048289503529667856, 0.04806400910019874, 0.04785094726830721, 0.047623460739851, 0.04740982186049223, 0.04719044417142868, 0.04697683043777943, 0.04676749426871538, 0.04654290769249201, 0.046339326165616514, 0.046121135167777536, 0.04591338336467743, 0.04571138676255941, 0.045491442643105985, 0.045295318402349946, 0.04508119467645884, 0.04487922713160515, 0.04468204081058502, 0.04446886256337166, 0.04427679032087326, 0.04406964816153049, 0.04387308824807405, 0.04367918968200683, 0.04347361512482166, 0.0432839211076498, 0.043084985204041006, 0.042893620952963826, 0.042702066153287886, 0.04250466525554657, 0.04231614340096712, 0.042126229777932164, 0.04193989206105471, 0.04174998085945845, 0.041561067849397657, 0.04137431066483259, 0.04120175037533045, 0.04100274983793497, 0.04082189779728651, 0.04064656114205718, 0.040459757298231126, 0.0402841473929584, 0.04010409526526928, 0.039921884518116714, 0.03974605780094862, 0.03956496100872755, 0.03939189398661256, 0.039217423740774394, 0.03904629675671458, 0.03887051334604621, 0.038703966047614814, 0.03852594317868352, 0.03835992757230997, 0.038190347421914336, 0.0380179307423532, 0.03785102339461446, 0.03768101716414094, 0.037516594119369986, 0.037352912686765195, 0.037191356997936964, 0.03702534642070532, 0.03686439543962479, 0.03669576356187463, 0.03654190618544817, 0.03637735592201352, 0.03622077256441116, 0.03605991564691067, 0.035907516814768316, 0.03574446439743042, 0.03559249043464661, 0.03543631993234157, 0.035279303789138794, 0.0351235150359571, 0.03497932953760028, 0.034820192214101556, 0.034675643872469665, 0.034524235688149926, 0.0343706538900733, 0.03421690361574292, 0.034077596571296456, 0.03391896141692996, 0.03377592423930764, 0.03363015474751592, 0.03348138714209199, 0.03333490900695324, 0.033200524654239415, 0.03304955493658781, 0.03291358686983585, 0.03277113987132907, 0.03262662393972278, 0.03248191624879837, 0.032347343116998675, 0.03220142219215631, 0.03206677082926035, 0.03192811133340001, 0.03178941011428833, 0.031651475466787815, 0.03151786606758833, 0.031380531191825864, 0.031249982211738823, 0.031114985700696707, 0.03098303321748972, 0.03084457814693451, 0.03071290710940957, 0.030580383073538542, 0.030453478917479515, 0.030324436817318202, 0.03019535103812814, 0.03006537090986967, 0.029936126526445152, 0.029808305203914642, 0.029678924661129712, 0.029554635193198918, 0.029431386012583972, 0.029307550005614757, 0.02918106419965625, 0.0290561911650002, 0.02893308997154236, 0.0288076588883996, 0.028687296342104672, 0.028566727600991725, 0.02844872511923313, 0.028328637778759002, 0.02820667903870344, 0.028086247108876704, 0.02796577224507928, 0.027849032543599606, 0.027730434946715832, 0.027616403717547654, 0.027500120177865028, 0.027384022995829583, 0.027266576793044804, 0.027158684376627207, 0.02703671483322978, 0.026925490237772465, 0.02681136494502425, 0.0267014236189425, 0.026587292924523352, 0.026475398708134888, 0.02636616872623563, 0.02625222560018301, 0.02614520713686943, 0.02603343306109309, 0.025924093276262283, 0.02581992680206895, 0.025708737690001725, 0.025601085368543865, 0.025500648468732835, 0.02538798227906227, 0.025284565519541502, 0.025178339425474407, 0.025071377959102394, 0.024969766382128, 0.024863135535269976, 0.024760777689516546, 0.024657330196350812, 0.024556682165712117, 0.024453789554536344, 0.024351915623992682, 0.024249098263680935, 0.024150598607957364, 0.02404865073040128, 0.023951805476099253, 0.023852058034390212, 0.023755923472344877, 0.023656910005956888, 0.02355714254081249, 0.023457460477948187, 0.023360913526266812, 0.023264040425419807, 0.02317241337150335, 0.023074600845575333, 0.022982003074139355, 0.022886647935956718, 0.02279054308310151, 0.02269449960440397, 0.02260541534051299, 0.022507383953779936, 0.022417054418474434, 0.02232432533055544, 0.02223733803257346, 0.022142166923731565, 0.022051175776869058, 0.021959226485341786, 0.021871724911034108, 0.021779218595474957, 0.02169220196083188, 0.02160281715914607, 0.021514608617872, 0.021427202224731445, 0.02133946428075433, 0.0212507956661284, 0.02116392320021987, 0.021077201422303914, 0.020993325766175984, 0.020907112304121254, 0.020820282585918903, 0.020737805124372243, 0.020651219598948955, 0.0205669897608459, 0.020481944689527153, 0.020404976746067404, 0.020315432408824562, 0.02023496674373746, 0.020152283180505038, 0.020068980101495982, 0.01999068153090775, 0.019906751345843077, 0.01982592875137925, 0.0197464304510504, 0.01966592511162162, 0.019588390551507472, 0.019508612249046563, 0.019428208842873572, 0.0193478646222502, 0.019272663909941912, 0.019191908417269588, 0.019114309968426824, 0.019041047152131795, 0.018960685841739176, 0.01888624606654048, 0.018809659266844393, 0.018732463708147407, 0.018655299814417957, 0.018581993179395795, 0.018505545519292354, 0.01843100939877331, 0.018361776228994132, 0.01828348506242037, 0.01821201089769602, 0.018138431757688523, 0.01806427608244121, 0.01799015123397112, 0.017917630169540642, 0.017846254678443075, 0.017774651665240525, 0.017707957699894906, 0.017632891517132522, 0.017564248014241456, 0.01749356142245233, 0.017422283859923483, 0.017351048393175007, 0.017281180061399937, 0.017212194856256247, 0.017145252227783202, 0.017076414404436947, 0.01700750533491373, 0.016941089602187275, 0.016873474465683103, 0.01680508954450488, 0.016736655682325362, 0.016668761195614935, 0.016601968836039305, 0.016536834230646492, 0.016472000209614635, 0.016405867552384733, 0.016341694397851825, 0.016276264702901243, 0.016211583698168396, 0.016146189579740167, 0.016080730594694614, 0.016019558953121304, 0.015953055722638963, 0.01589150750078261, 0.015828207461163403, 0.015764410234987737, 0.015702919894829394, 0.0156400911975652, 0.015578170400112867, 0.01551558100618422, 0.015453277481719851, 0.015391409350559115, 0.015332666179165243, 0.015270334156230091, 0.015210652817040681, 0.015149945626035333, 0.015090394532307982, 0.015031103976070881, 0.014971838472411037, 0.014911907212808728, 0.014852745784446597, 0.014793002838268876, 0.014736909093335271, 0.014677054667845368, 0.014619896514341235, 0.014561751810833811, 0.014503402542322874, 0.014449191465973853, 0.014389851363375783, 0.014333338476717472, 0.014276941353455187, 0.014219913957640528, 0.0141652621794492, 0.014108894253149628, 0.01405413900502026, 0.013998430082574486, 0.013942506071180106, 0.013887452939525247, 0.013833696534857155, 0.013779527274891734, 0.013724997267127037, 0.013670806726440787, 0.013617210509255528, 0.013564423797652126, 0.013511953316628933, 0.013458557752892375, 0.013404932711273432, 0.013351424178108574, 0.013300106022506952, 0.013247523177415132, 0.013195835053920746, 0.013145466428250074, 0.013092144951224327, 0.013042911468073726, 0.012990635307505726, 0.012940591014921666, 0.012889645993709564, 0.01283848392777145, 0.01278748787008226, 0.012737319804728031, 0.012688148440793157, 0.012639918178319932, 0.012590936943888664, 0.01254130369052291, 0.012493893038481474, 0.01244318732060492, 0.012395285023376345, 0.01234659245237708, 0.012297737831249832, 0.012249842751771212, 0.012201162800192834, 0.012154446495696903, 0.01210705223493278, 0.012060660263523459, 0.012013450032100081, 0.011968991439789533, 0.011921021202579141, 0.011874913657084107, 0.011828268552199006, 0.011781559931114316, 0.01173593383282423, 0.011689280485734343, 0.011644410947337747, 0.011599392164498568, 0.011555099207907916, 0.011510011740028858, 0.011465475568547845, 0.011421719146892429, 0.011377672897651792, 0.011333108693361283, 0.011288495454937219, 0.011244750441983342, 0.011200327146798372, 0.011157030100002885, 0.011115942196920515, 0.011070798570290207, 0.011028588889166713, 0.010985692171379924, 0.010944294556975365, 0.010901701776310802, 0.010859818756580352, 0.010817434312775731, 0.010775003721937538, 0.010734609747305512, 0.010691151302307845, 0.010651052137836814, 0.010609477758407593, 0.010569218173623085, 0.01052823537029326, 0.010487079061567784, 0.010447333194315433, 0.010406982665881515, 0.010367115680128335, 0.010326784895732999, 0.010287131648510695, 0.010246748453937471, 0.010207396326586604, 0.010169112309813499, 0.010128963645547629, 0.010090584820136427, 0.010051548248156905, 0.010012359311804175, 0.009973796573467553, 0.009936120104975998, 0.009898171108216048, 0.009859776007942855, 0.009822429483756423, 0.009783587791025639, 0.00974613120779395, 0.009709819708950818, 0.009671465051360429, 0.009634931199252605, 0.009597761114127934, 0.009560466720722615, 0.009523218777030707, 0.009488285752013326, 0.009450887516140937, 0.009414904774166644, 0.009379489836283028, 0.009342664410360158, 0.00930702097248286, 0.009271688293665648, 0.009235934261232615, 0.00920113711617887, 0.009165741968899965, 0.009130202559754252, 0.00909473025240004, 0.00906040770933032, 0.009025000175461173, 0.008992006909102202, 0.00895816187839955, 0.008923712372779845, 0.00888913469389081, 0.008854618552140892, 0.008820760762318969, 0.008787740697152912, 0.008754479745402933, 0.008720780350267887, 0.008687011268921196, 0.008653320441953837, 0.00862100962549448, 0.00858686633873731, 0.008554241247475147, 0.008522790553979576, 0.008490014169365168, 0.008457929454743862, 0.00842546196654439, 0.008392915222793817, 0.008361051976680755, 0.008329768828116358, 0.008298340276814998, 0.00826650254894048, 0.008234593737870454, 0.008203454641625285, 0.008171538542956113, 0.00814057532697916, 0.008109350269660354, 0.008078998769633472, 0.008048422657884658, 0.00801807299721986, 0.007987357187084853, 0.0079565966501832, 0.007927025016397238, 0.007896058610640466, 0.007866553543135524, 0.007836557389236987, 0.007807152741588652, 0.007776847761124373, 0.007747482904233038, 0.007717842934653163, 0.007688174373470247, 0.0076602431247010825, 0.0076304992195218805, 0.007601813366636634, 0.00757277722004801, 0.007543985894881189, 0.007515390659682453, 0.007487393007613719, 0.00745891323313117, 0.007430536136962473, 0.0074022113345563415, 0.007374322880059481, 0.007346173608675599, 0.00731801032088697, 0.007289901096373797, 0.00726474248804152, 0.007236827583983541, 0.007210483262315393, 0.007183270528912544, 0.007155769248493015, 0.007128253672271967, 0.007100805081427098, 0.007073480146937072, 0.00704683056101203, 0.0070196296321228145, 0.0069932271493598815, 0.006966594257391989, 0.006939939293079078, 0.0069133504293859005, 0.006887249648571014, 0.006861726008355618, 0.006836054683662951, 0.006810088781639934, 0.006784073868766427, 0.006759266112931072, 0.006733586033806205, 0.006708464678376913, 0.006683013774454594, 0.006658673519268632, 0.00663243371527642, 0.006607578159309924, 0.006582482997328043, 0.006557354843243957, 0.00653230722527951, 0.006507344683632255, 0.006484473939053714, 0.006460194801911711, 0.006436804844997823, 0.006412641471251845, 0.006388229271396995, 0.006363792740739882, 0.006339417793788016, 0.00631570799741894, 0.006291386438533664, 0.006267840298824013, 0.006244066031649708, 0.006220281170681119, 0.006196543481200934, 0.0061729137087240815, 0.006150010298006237, 0.006126295239664614, 0.006103421957232058, 0.006080952472984791, 0.0060583460610359905, 0.00603583783376962, 0.006013037101365626, 0.005990193830803037, 0.005967592471279204, 0.005945847509428859, 0.005923786736093462, 0.005901413364335894, 0.005878986534662544, 0.0058565972140058875, 0.005834297277033329, 0.005812355969101191, 0.005790320597589016, 0.0057687507010996345, 0.005746980011463165, 0.005725200870074332, 0.005705877253785729, 0.005684000207111239, 0.00566355106420815, 0.005642401962541043, 0.005621038051322103, 0.0055996427778154615, 0.005578300915658474, 0.005557033536024392, 0.005535873142071069, 0.005514815892092883, 0.005493936687707901, 0.005473314295522868, 0.005452971532940864, 0.005432438920252025, 0.005411889078095556, 0.005391387385316193, 0.00537260661367327, 0.005352636356838048, 0.005333425826393068, 0.005313553870655597, 0.005293476954102516, 0.005273372237570584, 0.005253324448131025, 0.005233347439207137, 0.005213473201729357, 0.005193684669211507, 0.00517455218359828, 0.005154696630779654, 0.005135568510740996, 0.005116288701537996, 0.005096966121345759, 0.005077699036337435, 0.005059153982438147, 0.005040259100496769, 0.005021643952932209, 0.005002811644226312, 0.0049839450512081385, 0.004965117620304227, 0.00494716070825234, 0.004928608238697052, 0.0049104558303952215, 0.004892067308537662, 0.004873617459088564, 0.004855968267656862, 0.004837146319914609, 0.004819252900779248, 0.004801185382530093, 0.004783094744198024, 0.004765052639413625, 0.004747068672440946, 0.004730165202636272, 0.00471213567070663, 0.0046947750844992696, 0.004677213751710952, 0.0046596145606599745, 0.004642234090715647, 0.004624831711407751, 0.004607748240232468, 0.0045905083767138425, 0.004574614518787712, 0.004556902125477791, 0.004540189111139625, 0.0045232167118228975, 0.004506198933813721, 0.004489208944141865, 0.00447229496203363, 0.004455434309784323, 0.0044386472785845395, 0.00442247895989567, 0.004406040988396853, 0.0043898464646190405, 0.004373464314267039, 0.004357046412769705, 0.004341242939699441, 0.00432457544375211, 0.0043086491525173186, 0.004292559518944472, 0.004276754392776638, 0.0042612028191797435, 0.004245600732974708, 0.004229768551886082, 0.004213879874441773, 0.004198028729297221, 0.004182226001285016, 0.004166493995580822, 0.004150826937984675, 0.004135792015586048, 0.0041199263301678005, 0.004104766983073205, 0.004089544282760471, 0.00407488466007635, 0.004059962660539896, 0.004044835781678557, 0.0040296740480698645, 0.004014540417119861, 0.0039994569262489675, 0.003984780819155276, 0.003970236005261541, 0.003955724905245006, 0.003940991126000881, 0.003926232364028693, 0.003911493939813227, 0.003896804351825267, 0.0038821856724098325, 0.0038680891739204526, 0.003853329340927303, 0.0038391865557059644, 0.0038248977391049264, 0.0038106029969640076, 0.0037972277263179422, 0.0037827672553248704, 0.003768931853119284, 0.003754917741753161, 0.003740876982919872, 0.003726868238300085, 0.003712904406711459, 0.003699816227890551, 0.003685838228557259, 0.0036723918630741535, 0.003658755565993488, 0.003645089396741241, 0.0036314303171820937, 0.0036178219364956022, 0.003604795318096876, 0.003590987192001194, 0.0035778281628154216, 0.003564539074432105, 0.003551233978942037, 0.003537962166592479, 0.003525593993254006, 0.0035121524706482887, 0.0034993325476534664, 0.0034863568376749753, 0.003473345609381795, 0.0034603601787239315], [tensor(0.7159), tensor(0.7105), tensor(0.7052), tensor(0.6995), tensor(0.6931), tensor(0.6843), tensor(0.6711), tensor(0.6598), tensor(0.6484), tensor(0.6387), tensor(0.6303), tensor(0.6223), tensor(0.6138), tensor(0.6043), tensor(0.5945), tensor(0.5843), tensor(0.5736), tensor(0.5635), tensor(0.5542), tensor(0.5454), tensor(0.5366), tensor(0.5278), tensor(0.5190), tensor(0.5103), tensor(0.5016), tensor(0.4929), tensor(0.4842), tensor(0.4758), tensor(0.4676), tensor(0.4607), tensor(0.4546), tensor(0.4488), tensor(0.4431), tensor(0.4376), tensor(0.4322), tensor(0.4271), tensor(0.4223), tensor(0.4177), tensor(0.4132), tensor(0.4093), tensor(0.4053), tensor(0.4013), tensor(0.3975), tensor(0.3939), tensor(0.3905), tensor(0.3873), tensor(0.3839), tensor(0.3807), tensor(0.3774), tensor(0.3744), tensor(0.3715), tensor(0.3685), tensor(0.3655), tensor(0.3627), tensor(0.3599), tensor(0.3572), tensor(0.3545), tensor(0.3519), tensor(0.3493), tensor(0.3468), tensor(0.3442), tensor(0.3417), tensor(0.3393), tensor(0.3369), tensor(0.3346), tensor(0.3324), tensor(0.3302), tensor(0.3280), tensor(0.3257), tensor(0.3235), tensor(0.3214), tensor(0.3192), tensor(0.3170), tensor(0.3149), tensor(0.3128), tensor(0.3107), tensor(0.3087), tensor(0.3066), tensor(0.3047), tensor(0.3027), tensor(0.3007), tensor(0.2988), tensor(0.2969), tensor(0.2951), tensor(0.2931), tensor(0.2913), tensor(0.2895), tensor(0.2876), tensor(0.2858), tensor(0.2841), tensor(0.2823), tensor(0.2805), tensor(0.2788), tensor(0.2771), tensor(0.2753), tensor(0.2737), tensor(0.2720), tensor(0.2703), tensor(0.2687), tensor(0.2671), tensor(0.2654), tensor(0.2638), tensor(0.2622), tensor(0.2607), tensor(0.2591), tensor(0.2575), tensor(0.2560), tensor(0.2545), tensor(0.2530), tensor(0.2515), tensor(0.2500), tensor(0.2485), tensor(0.2471), tensor(0.2456), tensor(0.2442), tensor(0.2427), tensor(0.2413), tensor(0.2399), tensor(0.2385), tensor(0.2371), tensor(0.2358), tensor(0.2344), tensor(0.2331), tensor(0.2317), tensor(0.2304), tensor(0.2291), tensor(0.2278), tensor(0.2265), tensor(0.2252), tensor(0.2239), tensor(0.2227), tensor(0.2214), tensor(0.2202), tensor(0.2189), tensor(0.2177), tensor(0.2165), tensor(0.2153), tensor(0.2141), tensor(0.2128), tensor(0.2116), tensor(0.2104), tensor(0.2092), tensor(0.2081), tensor(0.2069), tensor(0.2057), tensor(0.2046), tensor(0.2035), tensor(0.2023), tensor(0.2012), tensor(0.2001), tensor(0.1990), tensor(0.1979), tensor(0.1967), tensor(0.1956), tensor(0.1946), tensor(0.1935), tensor(0.1924), tensor(0.1914), tensor(0.1903), tensor(0.1893), tensor(0.1882), tensor(0.1872), tensor(0.1862), tensor(0.1852), tensor(0.1842), tensor(0.1832), tensor(0.1822), tensor(0.1812), tensor(0.1802), tensor(0.1792), tensor(0.1782), tensor(0.1773), tensor(0.1763), tensor(0.1754), tensor(0.1744), tensor(0.1735), tensor(0.1725), tensor(0.1716), tensor(0.1707), tensor(0.1698), tensor(0.1689), tensor(0.1680), tensor(0.1671), tensor(0.1662), tensor(0.1653), tensor(0.1644), tensor(0.1635), tensor(0.1627), tensor(0.1618), tensor(0.1610), tensor(0.1601), tensor(0.1593), tensor(0.1584), tensor(0.1576), tensor(0.1568), tensor(0.1559), tensor(0.1551), tensor(0.1543), tensor(0.1535), tensor(0.1527), tensor(0.1519), tensor(0.1511), tensor(0.1503), tensor(0.1495), tensor(0.1488), tensor(0.1480), tensor(0.1472), tensor(0.1464), tensor(0.1457), tensor(0.1449), tensor(0.1442), tensor(0.1434), tensor(0.1427), tensor(0.1420), tensor(0.1412), tensor(0.1405), tensor(0.1398), tensor(0.1391), tensor(0.1384), tensor(0.1377), tensor(0.1369), tensor(0.1362), tensor(0.1356), tensor(0.1349), tensor(0.1342), tensor(0.1335), tensor(0.1328), tensor(0.1322), tensor(0.1315), tensor(0.1308), tensor(0.1301), tensor(0.1295), tensor(0.1288), tensor(0.1282), tensor(0.1275), tensor(0.1269), tensor(0.1262), tensor(0.1256), tensor(0.1250), tensor(0.1244), tensor(0.1237), tensor(0.1231), tensor(0.1225), tensor(0.1219), tensor(0.1213), tensor(0.1207), tensor(0.1201), tensor(0.1195), tensor(0.1189), tensor(0.1183), tensor(0.1177), tensor(0.1171), tensor(0.1166), tensor(0.1160), tensor(0.1154), tensor(0.1148), tensor(0.1143), tensor(0.1137), tensor(0.1132), tensor(0.1126), tensor(0.1120), tensor(0.1115), tensor(0.1110), tensor(0.1104), tensor(0.1099), tensor(0.1093), tensor(0.1088), tensor(0.1083), tensor(0.1077), tensor(0.1072), tensor(0.1067), tensor(0.1062), tensor(0.1056), tensor(0.1052), tensor(0.1046), tensor(0.1041), tensor(0.1036), tensor(0.1031), tensor(0.1026), tensor(0.1021), tensor(0.1016), tensor(0.1012), tensor(0.1007), tensor(0.1002), tensor(0.0997), tensor(0.0992), tensor(0.0987), tensor(0.0983), tensor(0.0978), tensor(0.0973), tensor(0.0969), tensor(0.0964), tensor(0.0960), tensor(0.0955), tensor(0.0950), tensor(0.0946), tensor(0.0942), tensor(0.0937), tensor(0.0933), tensor(0.0928), tensor(0.0924), tensor(0.0919), tensor(0.0915), tensor(0.0911), tensor(0.0907), tensor(0.0902), tensor(0.0898), tensor(0.0894), tensor(0.0890), tensor(0.0885), tensor(0.0881), tensor(0.0877), tensor(0.0873), tensor(0.0869), tensor(0.0865), tensor(0.0861), tensor(0.0857), tensor(0.0853), tensor(0.0849), tensor(0.0845), tensor(0.0841), tensor(0.0837), tensor(0.0833), tensor(0.0829), tensor(0.0825), tensor(0.0822), tensor(0.0818), tensor(0.0814), tensor(0.0810), tensor(0.0807), tensor(0.0803), tensor(0.0799), tensor(0.0795), tensor(0.0792), tensor(0.0788), tensor(0.0785), tensor(0.0781), tensor(0.0777), tensor(0.0774), tensor(0.0770), tensor(0.0767), tensor(0.0763), tensor(0.0760), tensor(0.0756), tensor(0.0753), tensor(0.0749), tensor(0.0746), tensor(0.0743), tensor(0.0739), tensor(0.0736), tensor(0.0732), tensor(0.0729), tensor(0.0726), tensor(0.0723), tensor(0.0719), tensor(0.0716), tensor(0.0713), tensor(0.0710), tensor(0.0706), tensor(0.0703), tensor(0.0700), tensor(0.0697), tensor(0.0694), tensor(0.0691), tensor(0.0688), tensor(0.0685), tensor(0.0681), tensor(0.0678), tensor(0.0675), tensor(0.0672), tensor(0.0669), tensor(0.0666), tensor(0.0663), tensor(0.0660), tensor(0.0657), tensor(0.0655), tensor(0.0652), tensor(0.0649), tensor(0.0646), tensor(0.0643), tensor(0.0640), tensor(0.0637), tensor(0.0634), tensor(0.0632), tensor(0.0629), tensor(0.0626), tensor(0.0623), tensor(0.0621), tensor(0.0618), tensor(0.0615), tensor(0.0612), tensor(0.0610), tensor(0.0607), tensor(0.0604), tensor(0.0602), tensor(0.0599), tensor(0.0596), tensor(0.0594), tensor(0.0591), tensor(0.0589), tensor(0.0586), tensor(0.0584), tensor(0.0581), tensor(0.0578), tensor(0.0576), tensor(0.0573), tensor(0.0571), tensor(0.0568), tensor(0.0566), tensor(0.0563), tensor(0.0561), tensor(0.0559), tensor(0.0556), tensor(0.0554), tensor(0.0551), tensor(0.0549), tensor(0.0547), tensor(0.0544), tensor(0.0542), tensor(0.0539), tensor(0.0537), tensor(0.0535), tensor(0.0533), tensor(0.0530), tensor(0.0528), tensor(0.0526), tensor(0.0523), tensor(0.0521), tensor(0.0519), tensor(0.0517), tensor(0.0514), tensor(0.0512), tensor(0.0510), tensor(0.0508), tensor(0.0506), tensor(0.0504), tensor(0.0501), tensor(0.0499), tensor(0.0497), tensor(0.0495), tensor(0.0493), tensor(0.0491), tensor(0.0489), tensor(0.0487), tensor(0.0485), tensor(0.0482), tensor(0.0480), tensor(0.0478), tensor(0.0476), tensor(0.0474), tensor(0.0472), tensor(0.0470), tensor(0.0468), tensor(0.0466), tensor(0.0464), tensor(0.0462), tensor(0.0460), tensor(0.0459), tensor(0.0457), tensor(0.0455), tensor(0.0453), tensor(0.0451), tensor(0.0449), tensor(0.0447), tensor(0.0445), tensor(0.0443), tensor(0.0441), tensor(0.0440), tensor(0.0438), tensor(0.0436), tensor(0.0434), tensor(0.0432), tensor(0.0430), tensor(0.0429), tensor(0.0427), tensor(0.0425), tensor(0.0423), tensor(0.0421), tensor(0.0420), tensor(0.0418), tensor(0.0416), tensor(0.0414), tensor(0.0413), tensor(0.0411), tensor(0.0409), tensor(0.0408), tensor(0.0406), tensor(0.0404), tensor(0.0403), tensor(0.0401), tensor(0.0399), tensor(0.0398), tensor(0.0396), tensor(0.0394), tensor(0.0393), tensor(0.0391), tensor(0.0389), tensor(0.0388), tensor(0.0386), tensor(0.0385), tensor(0.0383), tensor(0.0381), tensor(0.0380), tensor(0.0378), tensor(0.0377), tensor(0.0375), tensor(0.0374), tensor(0.0372), tensor(0.0371), tensor(0.0369), tensor(0.0367), tensor(0.0366), tensor(0.0364), tensor(0.0363), tensor(0.0361), tensor(0.0360), tensor(0.0359), tensor(0.0357), tensor(0.0356), tensor(0.0354), tensor(0.0353), tensor(0.0351), tensor(0.0350), tensor(0.0348), tensor(0.0347), tensor(0.0346), tensor(0.0344), tensor(0.0343), tensor(0.0341), tensor(0.0340), tensor(0.0339), tensor(0.0337), tensor(0.0336), tensor(0.0334), tensor(0.0333), tensor(0.0332), tensor(0.0330), tensor(0.0329), tensor(0.0328), tensor(0.0326), tensor(0.0325), tensor(0.0324), tensor(0.0322), tensor(0.0321), tensor(0.0320), tensor(0.0318), tensor(0.0317), tensor(0.0316), tensor(0.0315), tensor(0.0313), tensor(0.0312), tensor(0.0311), tensor(0.0310), tensor(0.0308), tensor(0.0307), tensor(0.0306), tensor(0.0305), tensor(0.0303), tensor(0.0302), tensor(0.0301), tensor(0.0300), tensor(0.0298), tensor(0.0297), tensor(0.0296), tensor(0.0295), tensor(0.0294), tensor(0.0293), tensor(0.0291), tensor(0.0290), tensor(0.0289), tensor(0.0288), tensor(0.0287), tensor(0.0286), tensor(0.0284), tensor(0.0283), tensor(0.0282), tensor(0.0281), tensor(0.0280), tensor(0.0279), tensor(0.0278), tensor(0.0277), tensor(0.0275), tensor(0.0274), tensor(0.0273), tensor(0.0272), tensor(0.0271), tensor(0.0270), tensor(0.0269), tensor(0.0268), tensor(0.0267), tensor(0.0266), tensor(0.0265), tensor(0.0264), tensor(0.0263), tensor(0.0262), tensor(0.0260), tensor(0.0259), tensor(0.0258), tensor(0.0257), tensor(0.0256), tensor(0.0255), tensor(0.0254), tensor(0.0253), tensor(0.0252), tensor(0.0251), tensor(0.0250), tensor(0.0249), tensor(0.0248), tensor(0.0247), tensor(0.0246), tensor(0.0245), tensor(0.0244), tensor(0.0243), tensor(0.0242), tensor(0.0242), tensor(0.0241), tensor(0.0240), tensor(0.0239), tensor(0.0238), tensor(0.0237), tensor(0.0236), tensor(0.0235), tensor(0.0234), tensor(0.0233), tensor(0.0232), tensor(0.0231), tensor(0.0230), tensor(0.0229), tensor(0.0229), tensor(0.0228), tensor(0.0227), tensor(0.0226), tensor(0.0225), tensor(0.0224), tensor(0.0223), tensor(0.0222), tensor(0.0221), tensor(0.0221), tensor(0.0220), tensor(0.0219), tensor(0.0218), tensor(0.0217), tensor(0.0216), tensor(0.0215), tensor(0.0215), tensor(0.0214), tensor(0.0213), tensor(0.0212), tensor(0.0211), tensor(0.0210), tensor(0.0210), tensor(0.0209), tensor(0.0208), tensor(0.0207), tensor(0.0206), tensor(0.0206), tensor(0.0205), tensor(0.0204), tensor(0.0203), tensor(0.0202), tensor(0.0202), tensor(0.0201), tensor(0.0200), tensor(0.0199), tensor(0.0198), tensor(0.0198), tensor(0.0197), tensor(0.0196), tensor(0.0195), tensor(0.0195), tensor(0.0194), tensor(0.0193), tensor(0.0192), tensor(0.0192), tensor(0.0191), tensor(0.0190), tensor(0.0189), tensor(0.0189), tensor(0.0188), tensor(0.0187), tensor(0.0186), tensor(0.0186), tensor(0.0185), tensor(0.0184), tensor(0.0184), tensor(0.0183), tensor(0.0182), tensor(0.0181), tensor(0.0181), tensor(0.0180), tensor(0.0179), tensor(0.0179), tensor(0.0178), tensor(0.0177), tensor(0.0177), tensor(0.0176), tensor(0.0175), tensor(0.0175), tensor(0.0174), tensor(0.0173), tensor(0.0173), tensor(0.0172), tensor(0.0171), tensor(0.0171), tensor(0.0170), tensor(0.0169), tensor(0.0169), tensor(0.0168), tensor(0.0167), tensor(0.0167), tensor(0.0166), tensor(0.0165), tensor(0.0165), tensor(0.0164), tensor(0.0163), tensor(0.0163), tensor(0.0162), tensor(0.0162), tensor(0.0161), tensor(0.0160), tensor(0.0160), tensor(0.0159), tensor(0.0159), tensor(0.0158), tensor(0.0157), tensor(0.0157), tensor(0.0156), tensor(0.0156), tensor(0.0155), tensor(0.0154), tensor(0.0154), tensor(0.0153), tensor(0.0153), tensor(0.0152), tensor(0.0151), tensor(0.0151), tensor(0.0150), tensor(0.0150), tensor(0.0149), tensor(0.0149), tensor(0.0148), tensor(0.0147), tensor(0.0147), tensor(0.0146), tensor(0.0146), tensor(0.0145), tensor(0.0145), tensor(0.0144), tensor(0.0143), tensor(0.0143), tensor(0.0142), tensor(0.0142), tensor(0.0141), tensor(0.0141), tensor(0.0140), tensor(0.0140), tensor(0.0139), tensor(0.0139), tensor(0.0138), tensor(0.0138), tensor(0.0137), tensor(0.0137), tensor(0.0136), tensor(0.0135), tensor(0.0135), tensor(0.0134), tensor(0.0134), tensor(0.0133), tensor(0.0133), tensor(0.0132), tensor(0.0132), tensor(0.0131), tensor(0.0131), tensor(0.0130), tensor(0.0130), tensor(0.0129), tensor(0.0129), tensor(0.0128), tensor(0.0128), tensor(0.0127), tensor(0.0127), tensor(0.0126), tensor(0.0126), tensor(0.0126), tensor(0.0125), tensor(0.0125), tensor(0.0124), tensor(0.0124), tensor(0.0123), tensor(0.0123), tensor(0.0122), tensor(0.0122), tensor(0.0121), tensor(0.0121), tensor(0.0120), tensor(0.0120), tensor(0.0119), tensor(0.0119), tensor(0.0119), tensor(0.0118), tensor(0.0118), tensor(0.0117), tensor(0.0117), tensor(0.0116), tensor(0.0116), tensor(0.0116), tensor(0.0115), tensor(0.0115), tensor(0.0114), tensor(0.0114), tensor(0.0113), tensor(0.0113), tensor(0.0112), tensor(0.0112), tensor(0.0112), tensor(0.0111), tensor(0.0111), tensor(0.0110), tensor(0.0110), tensor(0.0110), tensor(0.0109), tensor(0.0109), tensor(0.0108), tensor(0.0108), tensor(0.0107), tensor(0.0107), tensor(0.0107), tensor(0.0106), tensor(0.0106), tensor(0.0105), tensor(0.0105), tensor(0.0105), tensor(0.0104), tensor(0.0104), tensor(0.0104), tensor(0.0103), tensor(0.0103), tensor(0.0102), tensor(0.0102), tensor(0.0102), tensor(0.0101), tensor(0.0101), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0099), tensor(0.0099), tensor(0.0099), tensor(0.0098), tensor(0.0098), tensor(0.0097), tensor(0.0097), tensor(0.0097), tensor(0.0096), tensor(0.0096), tensor(0.0096), tensor(0.0095), tensor(0.0095), tensor(0.0095), tensor(0.0094), tensor(0.0094), tensor(0.0093), tensor(0.0093), tensor(0.0093), tensor(0.0092), tensor(0.0092), tensor(0.0092), tensor(0.0091), tensor(0.0091), tensor(0.0091), tensor(0.0090), tensor(0.0090), tensor(0.0090), tensor(0.0089), tensor(0.0089), tensor(0.0089), tensor(0.0088), tensor(0.0088), tensor(0.0088), tensor(0.0087), tensor(0.0087), tensor(0.0087), tensor(0.0086), tensor(0.0086), tensor(0.0086), tensor(0.0085), tensor(0.0085), tensor(0.0085), tensor(0.0084), tensor(0.0084), tensor(0.0084), tensor(0.0084), tensor(0.0083), tensor(0.0083), tensor(0.0083), tensor(0.0082), tensor(0.0082), tensor(0.0082), tensor(0.0081), tensor(0.0081), tensor(0.0081), tensor(0.0080), tensor(0.0080), tensor(0.0080), tensor(0.0080), tensor(0.0079), tensor(0.0079), tensor(0.0079), tensor(0.0078), tensor(0.0078), tensor(0.0078), tensor(0.0078), tensor(0.0077), tensor(0.0077), tensor(0.0077), tensor(0.0076), tensor(0.0076), tensor(0.0076), tensor(0.0076), tensor(0.0075), tensor(0.0075), tensor(0.0075), tensor(0.0074), tensor(0.0074), tensor(0.0074), tensor(0.0074), tensor(0.0073), tensor(0.0073), tensor(0.0073), tensor(0.0072), tensor(0.0072), tensor(0.0072), tensor(0.0072), tensor(0.0071), tensor(0.0071), tensor(0.0071), tensor(0.0071), tensor(0.0070), tensor(0.0070), tensor(0.0070), tensor(0.0070), tensor(0.0069), tensor(0.0069), tensor(0.0069), tensor(0.0069), tensor(0.0068), tensor(0.0068), tensor(0.0068), tensor(0.0068), tensor(0.0067), tensor(0.0067), tensor(0.0067), tensor(0.0067), tensor(0.0066), tensor(0.0066), tensor(0.0066), tensor(0.0066), tensor(0.0065), tensor(0.0065), tensor(0.0065), tensor(0.0065), tensor(0.0064), tensor(0.0064), tensor(0.0064), tensor(0.0064), tensor(0.0063), tensor(0.0063), tensor(0.0063), tensor(0.0063), tensor(0.0062), tensor(0.0062), tensor(0.0062), tensor(0.0062), tensor(0.0062), tensor(0.0061), tensor(0.0061), tensor(0.0061), tensor(0.0061), tensor(0.0060), tensor(0.0060), tensor(0.0060), tensor(0.0060), tensor(0.0060), tensor(0.0059), tensor(0.0059), tensor(0.0059), tensor(0.0059), tensor(0.0058), tensor(0.0058), tensor(0.0058), tensor(0.0058), tensor(0.0058), tensor(0.0057), tensor(0.0057), tensor(0.0057), tensor(0.0057), tensor(0.0057), tensor(0.0056), tensor(0.0056), tensor(0.0056)]]\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.8857142865657807, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9], [tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.)]]\n"
     ]
    }
   ],
   "source": [
    "print(LOSS_HISTORY)\n",
    "print(SCORE_HISTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] 학습결과 시각화\n",
    "- 학습과 검증의 Loss 변화, 성능 변화 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
