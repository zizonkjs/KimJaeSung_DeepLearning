{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor 이해 - 생성\n",
    "- Tensor란? \n",
    "    * pytorch에서 데이터를 저장하는 객체\n",
    "    * 모델에 입력/출력 데이터 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch는 오픈 소스 머신러닝 라이브러리로, 특히 **딥러닝** 및 **인공신경망**을 구축하고 학습시키는 데 널리 사용됩니다. 페이스북(현 메타)에 의해 개발되었으며, 유연한 계산 그래프와 직관적인 인터페이스 덕분에 연구자와 개발자들 사이에서 매우 인기가 있습니다.\n",
    "\n",
    "PyTorch의 주요 특징들을 살펴보면:\n",
    "\n",
    "### 1. **동적 계산 그래프(Dynamic Computation Graph)**\n",
    "   - **텐서플로우(TensorFlow)**와 같은 다른 라이브러리는 정적 계산 그래프(static computation graph)를 사용하지만, PyTorch는 동적 계산 그래프를 사용합니다. 이를 통해 코드를 실행하면서 그래프가 생성되고 수정될 수 있습니다. 즉, 프로그램이 실행되는 동안 그래프가 동적으로 변경될 수 있어 디버깅과 모델 설계가 훨씬 직관적입니다.\n",
    "   \n",
    "### 2. **텐서 연산(Tensors)**\n",
    "   - PyTorch의 중심에는 텐서(tensor)가 있습니다. 텐서는 N차원 배열이며, NumPy 배열과 비슷하지만 GPU 가속을 지원하여 대규모 수치 계산을 빠르게 처리할 수 있습니다. 텐서는 주로 데이터나 파라미터를 저장하는 데 사용되며, 다양한 수학적 연산이 가능합니다.\n",
    "\n",
    "### 3. **GPU 가속**\n",
    "   - PyTorch는 GPU를 활용한 연산을 매우 쉽게 구현할 수 있습니다. GPU에서의 연산을 지원하기 위해 `.to(device)` 메서드를 통해 텐서나 모델을 GPU로 이동시킬 수 있습니다.\n",
    "   \n",
    "   ```python\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   tensor = tensor.to(device)\n",
    "   ```\n",
    "\n",
    "### 4. **Autograd (자동 미분)**\n",
    "   - PyTorch는 자동으로 미분을 계산해주는 **Autograd**라는 기능을 제공합니다. 이는 신경망의 역전파(backpropagation) 알고리즘을 구현할 때 매우 유용합니다. 모델의 파라미터에 대한 기울기(gradient)를 자동으로 계산하여 학습을 보다 간편하게 할 수 있습니다.\n",
    "\n",
    "### 5. **신경망 모듈: torch.nn**\n",
    "   - PyTorch는 `torch.nn` 모듈을 통해 다양한 신경망 계층(layers)을 제공합니다. 이를 사용하여 복잡한 신경망 모델을 쉽게 설계하고 구축할 수 있습니다.\n",
    "   - 예를 들어, `nn.Linear`, `nn.Conv2d`, `nn.LSTM`과 같은 다양한 레이어를 제공하여 딥러닝 모델을 쉽게 구축할 수 있습니다.\n",
    "\n",
    "### 6. **생태계**\n",
    "   - PyTorch는 다양한 추가 라이브러리 및 도구와 함께 사용할 수 있는 방대한 생태계를 가지고 있습니다. 예를 들어, 컴퓨터 비전을 위한 **Torchvision**, 자연어 처리를 위한 **TorchText**, 강화 학습을 위한 **TorchRL** 등이 있습니다.\n",
    "   - 또한, PyTorch는 **ONNX(Open Neural Network Exchange)**와 호환되어 다른 플랫폼으로 모델을 내보내거나 가져올 수 있습니다.\n",
    "\n",
    "### 7. **커뮤니티와 발전**\n",
    "   - PyTorch는 대규모 오픈 소스 커뮤니티가 활발히 개발하고 있으며, 최신 연구 및 실험을 지원하기 위한 빠른 업데이트와 새로운 기능들이 자주 추가됩니다. 특히, 연구자들이 새로운 아이디어를 테스트하고 구현하기에 적합한 라이브러리로 평가받고 있습니다.\n",
    "\n",
    "### PyTorch의 기본 코드 예시:\n",
    "아래는 간단한 신경망 모델을 PyTorch로 구축하고 학습하는 예시입니다.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 신경망 모델 정의\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 생성\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 과정의 한 루프\n",
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        output = model(data)   # 모델에 입력 전달\n",
    "        loss = criterion(output, target)  # 손실 계산\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 옵티마이저 업데이트\n",
    "```\n",
    "\n",
    "이 예시는 PyTorch로 간단한 신경망을 정의하고, 손실 함수와 옵티마이저를 설정하며, 학습 과정을 보여줍니다.\n",
    "\n",
    "PyTorch는 연구, 실험, 그리고 상용 딥러닝 애플리케이션에서 강력한 도구로 자리 잡고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 PyTorch를 사용하여 간단한 신경망 모델을 정의하고 학습하는 과정을 보여줍니다. 각 요소를 설명해 드리겠습니다.\n",
    "\n",
    "### 1. **라이브러리 임포트**\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "```\n",
    "- `torch`: PyTorch의 핵심 라이브러리로, 텐서 연산, 자동 미분, 신경망 구축 등을 지원합니다.\n",
    "- `torch.nn`: 신경망 계층을 정의하기 위한 PyTorch 모듈로, 신경망의 구조와 계층들을 정의하는 데 사용됩니다.\n",
    "- `torch.optim`: 옵티마이저(optimizer)를 정의하기 위한 모듈로, 신경망의 파라미터를 업데이트하여 학습을 진행합니다.\n",
    "\n",
    "### 2. **신경망 모델 정의**\n",
    "```python\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # 첫 번째 완전 연결 계층\n",
    "        self.fc2 = nn.Linear(128, 10)       # 두 번째 완전 연결 계층\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)             # 입력을 일차원으로 펼침 (배치 크기를 제외한 모든 차원)\n",
    "        x = torch.relu(self.fc1(x))         # 첫 번째 계층에 ReLU 활성화 함수 적용\n",
    "        x = self.fc2(x)                     # 두 번째 계층을 통과\n",
    "        return x\n",
    "```\n",
    "- **SimpleNN 클래스**: `nn.Module`을 상속받아 신경망 모델을 정의합니다.\n",
    "  - `__init__`: 모델이 초기화될 때 실행되며, 신경망의 레이어를 정의합니다.\n",
    "    - `self.fc1`: 28x28 크기의 이미지를 128차원의 벡터로 매핑하는 완전 연결 계층입니다. 여기서 28x28은 일반적으로 MNIST와 같은 손글씨 숫자 이미지 크기입니다.\n",
    "    - `self.fc2`: 128차원의 벡터를 10개의 클래스로 매핑하는 완전 연결 계층입니다. 10은 MNIST 데이터셋의 10가지 숫자(0-9)를 분류하기 위한 출력 차원입니다.\n",
    "  - `forward`: 신경망의 순방향 전파(forward pass)를 정의하는 메서드입니다. 입력이 주어졌을 때 각 레이어를 통과하면서 출력값을 계산합니다.\n",
    "    - `torch.flatten(x, 1)`: 입력 텐서를 일차원 벡터로 변환합니다. 이미지 데이터를 신경망의 입력으로 사용할 때, 2D 또는 3D 형태(채널, 높이, 너비)를 1D 벡터로 변환하여 완전 연결 계층에 입력합니다.\n",
    "    - `torch.relu`: ReLU(Rectified Linear Unit) 활성화 함수는 비선형 변환을 적용하여 신경망의 학습을 돕습니다. 음수는 0으로, 양수는 그대로 유지하는 함수입니다.\n",
    "\n",
    "### 3. **모델, 손실 함수, 옵티마이저 생성**\n",
    "```python\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "- **`model = SimpleNN()`**: 앞서 정의한 `SimpleNN` 클래스의 인스턴스를 생성하여 신경망 모델을 초기화합니다.\n",
    "- **`criterion = nn.CrossEntropyLoss()`**: 분류 문제에서 사용되는 손실 함수입니다. **크로스 엔트로피 손실(Cross Entropy Loss)**는 출력값(로짓)과 실제 레이블 간의 차이를 측정하여 신경망 학습에 필요한 손실을 계산합니다.\n",
    "- **`optimizer = optim.Adam(model.parameters(), lr=0.001)`**: **Adam 옵티마이저**는 신경망의 파라미터를 학습시키는 데 사용되는 방법입니다. `model.parameters()`는 모델의 모든 학습 가능한 파라미터(가중치와 편향)를 반환하며, `lr=0.001`은 학습률(learning rate)로, 파라미터 업데이트 시 조정하는 값의 크기를 결정합니다.\n",
    "\n",
    "### 4. **학습 과정**\n",
    "```python\n",
    "def train(model, train_loader):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        output = model(data)   # 모델에 입력 전달 (순방향 전파)\n",
    "        loss = criterion(output, target)  # 손실 계산\n",
    "        loss.backward()  # 역전파: 기울기 계산\n",
    "        optimizer.step()  # 옵티마이저가 모델의 파라미터를 업데이트\n",
    "```\n",
    "- **`model.train()`**: 모델을 **학습 모드**로 전환합니다. PyTorch는 `train()`과 `eval()` 두 가지 모드를 가지고 있으며, `train()`은 학습을 위한 모드로, 드롭아웃(dropout)과 같은 레이어에서 다르게 동작할 수 있습니다.\n",
    "- **`for batch_idx, (data, target) in enumerate(train_loader)`**: 데이터로더(`train_loader`)에서 데이터를 배치 단위로 가져옵니다. `data`는 입력 이미지, `target`은 실제 레이블입니다.\n",
    "- **`optimizer.zero_grad()`**: 매번 배치가 처리될 때마다 옵티마이저의 기울기(gradient)를 0으로 초기화합니다. PyTorch는 기울기를 누적하는 방식이므로, 이전 배치의 기울기가 남아 있지 않도록 매번 초기화해야 합니다.\n",
    "- **`output = model(data)`**: 신경망 모델에 입력 데이터를 전달하여 순방향 전파(forward pass)를 수행하고, 예측값을 계산합니다.\n",
    "- **`loss = criterion(output, target)`**: 모델의 출력값과 실제 레이블을 비교하여 손실(loss)을 계산합니다. 여기서 `criterion`은 크로스 엔트로피 손실 함수입니다.\n",
    "- **`loss.backward()`**: 역전파(backpropagation)를 수행하여 각 파라미터에 대한 기울기(gradient)를 계산합니다.\n",
    "- **`optimizer.step()`**: 옵티마이저가 계산된 기울기를 바탕으로 모델의 파라미터(가중치)를 업데이트하여 학습을 진행합니다.\n",
    "\n",
    "### 요약:\n",
    "이 코드는 PyTorch를 사용하여 간단한 신경망 모델을 정의하고, 손실 함수와 옵티마이저를 설정하여 데이터의 각 배치(batch)를 처리하면서 모델을 학습시키는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [[9.99420951e-01]\n",
      " [2.64702581e-04]\n",
      " [2.66880681e-04]\n",
      " [2.05514195e-04]\n",
      " [8.15944098e-05]]\n",
      "Actual values: [[ True]\n",
      " [False]\n",
      " [False]\n",
      " [False]\n",
      " [False]]\n",
      "Ridge Regression MSE: 1.717302898975675e-07\n",
      "Ridge Regression MAE: 0.0003547241578653682\n",
      "Ridge Regression R^2: 0.9999993130017601\n"
     ]
    }
   ],
   "source": [
    "# Ridge 모델을 이용한 예측\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 학습된 Ridge 모델로 예측 수행\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# 테스트 데이터를 사용하여 예측\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# 예측 결과 확인\n",
    "print(\"Predicted values:\", y_pred[:5])  # 예측된 상위 5개 값을 출력\n",
    "print(\"Actual values:\", y_test[:5].values)  # 실제 상위 5개 값을 출력\n",
    "\n",
    "# 성능 평가\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Ridge Regression MSE: {mse}\")\n",
    "print(f\"Ridge Regression MAE: {mae}\")\n",
    "print(f\"Ridge Regression R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape 형태가 안맞아서 Error 많이 발생한다고함\n",
    "# NumPy 사용\n",
    "# 배열(ARRAY)사용\n",
    "\n",
    "# Memory\n",
    "# 정수 int \n",
    "# byte -> 8bit -> 256개  -> int8 / uint8\n",
    "# - 부호(- / +) : -128 ~ 127\n",
    "# - 부호(X) : 0 ~ 255 ===> imgdata에 많이 사용 픽셀 색상값으로 사용함\n",
    "\n",
    "# short (2byte) -> 16bit -> -2^15 ~ 2^15-1 --> int16 / uint16\n",
    "\n",
    "# int --> 32bit --> int32 / uint32\n",
    "# long int -> 64bit --> int64 / uint64\n",
    "\n",
    "# 왜이게 중요하냐?\n",
    "# 데이터타입 변수명 = 데이터\n",
    "# int age = 10 C#, Java에서 사용\n",
    "\n",
    "# age=10 -> python\n",
    "\n",
    "# 기본 데이터 타입 -> 숫자, 글자 ---> int, float, char, bool \n",
    "# - C/C++/C#/Java ==> stack에 바로 저장\n",
    "# - Python ==> 힙(클래스기반 객체)으로 저장\n",
    "\n",
    "# 확장 데이터 타입 -> 여러개의 기본 데이터 타입 모여서 하나의 데이터 표현 ---> string\n",
    "# - C/C++/C#/Java/Python => 힙에 저장함.\n",
    "\n",
    "# C언어의 경우 => 구조체\n",
    "# Java 외 OOP(객체지향언어) => 구조체 + 함수 ==> class \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴퓨터가 실수를 표현하는 방법 --> ex) 3.14 => 정수부3, 실수부 12\n",
    "# 실수 float => 소수점 이하 자릿수\n",
    "# float -> 7/8 -> int32\n",
    "# double -> float 소수점 2배 -> int 64\n",
    "# 이렇게 나누어져있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch v.2.4.1\n"
     ]
    }
   ],
   "source": [
    "# 텐서 생성\n",
    "import torch\n",
    "print(f'torch v.{torch.__version__}')\n",
    "def printInfo(obj, obj_name):\n",
    "    print(f'\\n[{obj_name}]')\n",
    "    print(f'shape : {obj.shape}')\n",
    "    print(f'ndim : {obj.ndim}차원')\n",
    "    print(f'dtype : {obj.dtype}')\n",
    "    print(f'device : {obj.device}')\n",
    "    print(f'data : \\n{obj.data}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[dataTF]\n",
      "shape : torch.Size([])\n",
      "ndim : 0차원\n",
      "dtype : torch.int64\n",
      "device : cpu\n",
      "data : \n",
      "10\n",
      "\n",
      "[dataTF]\n",
      "shape : torch.Size([])\n",
      "ndim : 0차원\n",
      "dtype : torch.uint8\n",
      "device : cpu\n",
      "data : \n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 데이터 1개 즉, 스칼라\n",
    "dataTF=torch.tensor(10)\n",
    "printInfo(dataTF, 'dataTF')\n",
    "\n",
    "dataTF=torch.tensor(10, dtype=torch.uint8)\n",
    "printInfo(dataTF, 'dataTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[dataTF]\n",
      "shape : torch.Size([1])\n",
      "ndim : 1차원\n",
      "dtype : torch.int64\n",
      "device : cpu\n",
      "data : \n",
      "tensor([10])\n",
      "\n",
      "[dataTF]\n",
      "shape : torch.Size([1])\n",
      "ndim : 1차원\n",
      "dtype : torch.uint8\n",
      "device : cpu\n",
      "data : \n",
      "tensor([10], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 1개 , 벡터 1D\n",
    "dataTF=torch.tensor([10])\n",
    "printInfo(dataTF, 'dataTF')\n",
    "\n",
    "dataTF=torch.tensor([10], dtype=torch.uint8)\n",
    "printInfo(dataTF, 'dataTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[dataTF]\n",
      "shape : torch.Size([1, 1])\n",
      "ndim : 2차원\n",
      "dtype : torch.int64\n",
      "device : cpu\n",
      "data : \n",
      "tensor([[10]])\n",
      "\n",
      "[dataTF]\n",
      "shape : torch.Size([1, 2])\n",
      "ndim : 2차원\n",
      "dtype : torch.uint8\n",
      "device : cpu\n",
      "data : \n",
      "tensor([[10, 20]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 1개 , 행렬 2D\n",
    "dataTF=torch.tensor([[10]])\n",
    "printInfo(dataTF, 'dataTF')\n",
    "\n",
    "dataTF=torch.tensor([[10,20]], dtype=torch.uint8)\n",
    "printInfo(dataTF, 'dataTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[dataTF]\n",
      "shape : torch.Size([1, 1, 1])\n",
      "ndim : 3차원\n",
      "dtype : torch.int64\n",
      "device : cpu\n",
      "data : \n",
      "tensor([[[10]]])\n",
      "\n",
      "[dataTF]\n",
      "shape : torch.Size([1, 2, 2, 2])\n",
      "ndim : 4차원\n",
      "dtype : torch.uint8\n",
      "device : cpu\n",
      "data : \n",
      "tensor([[[[10, 20],\n",
      "          [10, 30]],\n",
      "\n",
      "         [[30, 40],\n",
      "          [50, 60]]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 1개 , 행렬 ND 3차원은 대괄호 3개, 4차원은 대괄호 4개, n차원은 대괄호 n개\n",
    "dataTF=torch.tensor([[[10]]])\n",
    "printInfo(dataTF, 'dataTF')\n",
    "\n",
    "dataTF=torch.tensor([[[[10,20],[10,30]],[[30,40],[50,60]]]], dtype=torch.uint8)\n",
    "printInfo(dataTF, 'dataTF')\n",
    "\n",
    "# [1,1,1,1]\n",
    "# [axis 0번축, axis 1번축, 행, 열]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
