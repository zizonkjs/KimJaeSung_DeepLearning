{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서 `Tensor`는 데이터와 연산을 다루는 기본 자료구조입니다. `Tensor`는 N차원 배열로, numpy 배열과 유사하지만 GPU 가속을 활용할 수 있는 점이 특징입니다. PyTorch의 `Tensor`는 다양한 속성을 가지며, 이러한 속성들을 통해 텐서의 모양, 크기, 데이터 타입 등을 확인할 수 있습니다.\n",
    "\n",
    "다음은 PyTorch `Tensor`의 주요 속성에 대한 설명입니다:\n",
    "\n",
    "### 1. **`shape` 또는 `size()`**\n",
    "   - **설명**: 텐서의 크기(차원)를 반환합니다. 각 차원마다 얼마나 많은 요소가 있는지를 나타냅니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "     print(t.shape)  # 출력: torch.Size([3, 2])\n",
    "     print(t.size()) # 출력: torch.Size([3, 2])\n",
    "     ```\n",
    "   - **결과**: `[3, 2]`는 텐서가 3개의 행과 2개의 열을 가지고 있음을 나타냅니다.\n",
    "\n",
    "### 2. **`dtype`**\n",
    "   - **설명**: 텐서의 데이터 타입을 나타냅니다. 텐서가 어떤 데이터 형식으로 저장되고 있는지를 나타내며, 일반적으로 `float32`, `int64`, `bool` 등의 타입이 사용됩니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([1.0, 2.0])\n",
    "     print(t.dtype)  # 출력: torch.float32\n",
    "     ```\n",
    "   - **결과**: `float32`는 텐서의 값들이 32비트 부동소수점으로 저장되고 있음을 나타냅니다.\n",
    "\n",
    "### 3. **`device`**\n",
    "   - **설명**: 텐서가 저장된 장치(GPU 또는 CPU)를 나타냅니다. `Tensor`는 CPU 또는 GPU에 저장될 수 있으며, 이를 통해 연산을 가속할 수 있습니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([1, 2])\n",
    "     print(t.device)  # 출력: cpu\n",
    "\n",
    "     t_gpu = torch.tensor([1, 2], device='cuda')  # GPU에 저장된 텐서\n",
    "     print(t_gpu.device)  # 출력: cuda:0\n",
    "     ```\n",
    "   - **결과**: `cuda:0`은 첫 번째 GPU에 텐서가 저장되어 있음을 나타내며, `cpu`는 CPU에 텐서가 저장된 상태입니다.\n",
    "\n",
    "### 4. **`requires_grad`**\n",
    "   - **설명**: `True`일 경우, 텐서의 연산에 대한 **기울기(gradient)**를 자동으로 계산하여 역전파를 지원합니다. 주로 신경망 학습 과정에서 모델 파라미터의 기울기를 계산할 때 사용됩니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "     print(t.requires_grad)  # 출력: True\n",
    "     ```\n",
    "   - **결과**: 텐서가 역전파에 필요한 기울기 계산에 사용될 수 있음을 나타냅니다.\n",
    "\n",
    "### 5. **`is_leaf`**\n",
    "   - **설명**: 텐서가 **연산 그래프의 루트**인지 여부를 나타냅니다. 텐서가 직접 생성된 것이면 `True`, 다른 텐서로부터 계산된 것이면 `False`입니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "     print(t.is_leaf)  # 출력: True\n",
    "\n",
    "     t2 = t + 2\n",
    "     print(t2.is_leaf)  # 출력: False\n",
    "     ```\n",
    "   - **결과**: 텐서 `t`는 직접 생성된 텐서이기 때문에 `True`, `t2`는 `t`를 기반으로 계산된 값이므로 `False`입니다.\n",
    "\n",
    "### 6. **`grad`**\n",
    "   - **설명**: `requires_grad=True`인 텐서에 대해 역전파(backpropagation)가 수행된 후 **기울기(gradient)**를 저장하는 속성입니다. 주로 모델 학습 중에 사용됩니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "     out = t.sum()\n",
    "     out.backward()\n",
    "     print(t.grad)  # 출력: tensor([1., 1., 1.])\n",
    "     ```\n",
    "   - **결과**: 텐서 `t`에 대해 계산된 기울기가 출력됩니다.\n",
    "\n",
    "### 7. **`numel()`**\n",
    "   - **설명**: 텐서의 **전체 요소 수**를 반환합니다. 이는 배열의 모든 차원을 곱한 값입니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "     print(t.numel())  # 출력: 6\n",
    "     ```\n",
    "   - **결과**: 이 2차원 텐서에는 총 6개의 요소가 있습니다.\n",
    "\n",
    "### 8. **`ndimension()`**\n",
    "   - **설명**: 텐서의 **차원 수**를 반환합니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "     print(t.ndimension())  # 출력: 2\n",
    "     ```\n",
    "   - **결과**: 이 텐서는 2차원 텐서입니다.\n",
    "\n",
    "### 9. **`stride()`**\n",
    "   - **설명**: 텐서의 각 차원에서 다음 요소로 이동하기 위해 건너뛰어야 하는 **바이트 수**를 반환합니다. 이는 텐서의 메모리 배치를 설명하는 속성입니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "     print(t.stride())  # 출력: (2, 1)\n",
    "     ```\n",
    "   - **결과**: `(2, 1)`은 첫 번째 차원(행)을 넘어가기 위해 2개의 요소를 건너뛰어야 하고, 두 번째 차원(열)을 넘어가려면 1개의 요소를 건너뛰어야 함을 의미합니다.\n",
    "\n",
    "### 10. **`storage_offset()`**\n",
    "   - **설명**: 텐서가 메모리에서 **저장된 위치**를 나타냅니다. 텐서가 기본적으로 메모리에서 어디에서 시작되는지를 설명합니다.\n",
    "   - **예시**:\n",
    "     ```python\n",
    "     t = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "     print(t.storage_offset())  # 출력: 0\n",
    "     ```\n",
    "   - **결과**: `0`은 이 텐서가 메모리의 시작 지점에서 저장되었음을 나타냅니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 예시 코드:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "t = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True)\n",
    "print(\"Shape:\", t.shape)              # 텐서의 모양\n",
    "print(\"Data type:\", t.dtype)          # 데이터 타입\n",
    "print(\"Device:\", t.device)            # 장치 정보 (GPU 또는 CPU)\n",
    "print(\"Requires grad:\", t.requires_grad)  # 기울기 계산 여부\n",
    "print(\"Is leaf:\", t.is_leaf)          # 연산 그래프의 루트인지 여부\n",
    "print(\"Number of elements:\", t.numel())   # 텐서의 총 요소 수\n",
    "print(\"Strides:\", t.stride())         # 텐서의 메모리 상의 stride\n",
    "```\n",
    "\n",
    "이 예시에서는 텐서의 여러 속성들이 출력되며, 이를 통해 텐서의 특성을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드에서는 PyTorch 텐서의 다양한 속성을 출력하는 `printInfo`라는 함수가 정의되어 있습니다. 함수의 목적은 텐서의 주요 속성들을 보기 좋게 출력하는 것입니다. 각 요소를 설명해 드리겠습니다.\n",
    "\n",
    "### 전체 코드\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 함수기능: 텐서 속성 정보 출력 함수\n",
    "# --------------------------------------------------\n",
    "def printInfo(obj, obj_name):\n",
    "    print(f'\\n[{obj_name}]')                     # 텐서 이름 출력\n",
    "    print(f'shape : {obj.shape}')                # 텐서의 모양 (shape)\n",
    "    print(f'ndim : {obj.ndim}차원')              # 텐서의 차원 수\n",
    "    print(f'dtype : {obj.dtype}')                # 텐서의 데이터 타입\n",
    "    print(f'device : {obj.device}')              # 텐서가 저장된 장치 (CPU or GPU)\n",
    "    print(f'data :\\n{obj.data}')                 # 텐서의 데이터 출력\n",
    "```\n",
    "\n",
    "### 코드 설명\n",
    "\n",
    "#### 1. **`import torch`**\n",
    "   - PyTorch 라이브러리를 임포트합니다. PyTorch는 `Tensor` 객체를 다루기 위한 핵심 라이브러리입니다.\n",
    "\n",
    "#### 2. **`def printInfo(obj, obj_name):`**\n",
    "   - **함수 정의**: `printInfo` 함수는 텐서 객체의 속성들을 출력하기 위한 함수입니다.\n",
    "   - **인자**:\n",
    "     - `obj`: 텐서 객체를 전달받습니다.\n",
    "     - `obj_name`: 텐서의 이름이나 식별자를 문자열로 전달받습니다. 이 이름은 출력을 보기 쉽게 만드는 데 사용됩니다.\n",
    "   \n",
    "#### 3. **`print(f'\\n[{obj_name}]')`**\n",
    "   - **텐서 이름 출력**: \n",
    "     - 텐서 이름을 `[ ]` 안에 넣어 출력합니다.\n",
    "     - `\\n`은 줄바꿈을 의미하며, 텐서 정보가 시작되기 전에 한 줄을 비우는 역할을 합니다.\n",
    "   - 예시:\n",
    "     ```python\n",
    "     [텐서 이름]\n",
    "     ```\n",
    "\n",
    "#### 4. **`print(f'shape : {obj.shape}')`**\n",
    "   - **텐서의 모양(차원 크기) 출력**:\n",
    "     - `obj.shape`는 텐서의 **모양(shape)**을 반환합니다. 이는 각 차원에서 몇 개의 요소가 있는지를 나타내는 튜플입니다.\n",
    "     - 예시:\n",
    "       - 만약 텐서가 `[ [1, 2], [3, 4], [5, 6] ]`라면, `shape`는 `(3, 2)`가 출력됩니다.\n",
    "     - 출력 예시:\n",
    "       ```python\n",
    "       shape : torch.Size([3, 2])\n",
    "       ```\n",
    "\n",
    "#### 5. **`print(f'ndim : {obj.ndim}차원')`**\n",
    "   - **텐서의 차원 수 출력**:\n",
    "     - `obj.ndim`은 텐서의 **차원 수**를 반환합니다.\n",
    "     - 예시:\n",
    "       - 2차원 텐서인 경우, `ndim`은 `2`가 됩니다.\n",
    "     - 출력 예시:\n",
    "       ```python\n",
    "       ndim : 2차원\n",
    "       ```\n",
    "\n",
    "#### 6. **`print(f'dtype : {obj.dtype}')`**\n",
    "   - **텐서의 데이터 타입 출력**:\n",
    "     - `obj.dtype`는 텐서의 **자료형(dtype)**을 반환합니다. PyTorch는 `float32`, `int64`, `bool` 등의 자료형을 지원합니다.\n",
    "     - 예시:\n",
    "       - 텐서가 `torch.float32`로 저장되어 있으면, `dtype`은 `torch.float32`로 출력됩니다.\n",
    "     - 출력 예시:\n",
    "       ```python\n",
    "       dtype : torch.float32\n",
    "       ```\n",
    "\n",
    "#### 7. **`print(f'device : {obj.device}')`**\n",
    "   - **텐서가 저장된 장치 출력**:\n",
    "     - `obj.device`는 텐서가 저장된 **장치(CPU 또는 GPU)**를 나타냅니다. `cuda`는 GPU를 의미하고, `cpu`는 CPU를 의미합니다.\n",
    "     - 예시:\n",
    "       - 텐서가 CPU에 저장되어 있으면, `device`는 `cpu`가 출력됩니다.\n",
    "       - 텐서가 GPU에 저장되어 있으면, `device`는 `cuda:0` (첫 번째 GPU)로 출력됩니다.\n",
    "     - 출력 예시:\n",
    "       ```python\n",
    "       device : cpu\n",
    "       ```\n",
    "\n",
    "#### 8. **`print(f'data :\\n{obj.data}')`**\n",
    "   - **텐서의 실제 데이터 출력**:\n",
    "     - `obj.data`는 텐서의 **실제 값**들을 반환합니다. 텐서 자체는 여러 메타데이터(자료형, 장치, 기울기 등)를 포함할 수 있지만, `data`는 텐서의 값을 직접 보여줍니다.\n",
    "     - 예시:\n",
    "       - 만약 텐서가 `[ [1, 2], [3, 4], [5, 6] ]`이라면, `data`는 같은 값을 출력합니다.\n",
    "     - 출력 예시:\n",
    "       ```python\n",
    "       data :\n",
    "       tensor([[1., 2.],\n",
    "               [3., 4.],\n",
    "               [5., 6.]])\n",
    "       ```\n",
    "\n",
    "### 함수 예시 사용\n",
    "\n",
    "이 함수는 텐서의 속성을 한 번에 쉽게 확인할 수 있게 도와줍니다. 예를 들어, 다음과 같이 사용할 수 있습니다:\n",
    "\n",
    "```python\n",
    "t = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)\n",
    "printInfo(t, \"2D Tensor\")\n",
    "```\n",
    "\n",
    "### 출력 예시:\n",
    "```\n",
    "[2D Tensor]\n",
    "shape : torch.Size([2, 2])\n",
    "ndim : 2차원\n",
    "dtype : torch.float32\n",
    "device : cpu\n",
    "data :\n",
    "tensor([[1., 2.],\n",
    "        [3., 4.]])\n",
    "```\n",
    "\n",
    "### 요약:\n",
    "- **`printInfo` 함수**는 PyTorch 텐서의 주요 속성(모양, 차원, 자료형, 장치, 데이터)을 출력하는 함수입니다.\n",
    "- 함수는 텐서의 상태를 쉽게 확인하는 데 유용하며, 텐서의 속성을 빠르게 파악할 수 있도록 돕습니다.\n",
    "\n",
    "이 코드의 각 요소에 대해 설명해 드렸습니다! 추가로 궁금한 점이 있으면 언제든지 질문해 주세요! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch의 `Tensor`는 다양한 데이터 유형(자료형)을 지원하며, 각 데이터 유형은 텐서의 요소들이 메모리에서 어떻게 저장되고 처리되는지를 결정합니다. 텐서의 자료형은 **`dtype`** 속성으로 지정하거나 확인할 수 있습니다. 자료형은 주로 데이터의 **정확성**과 **메모리 사용량**에 영향을 미치며, 적절한 자료형을 선택하는 것은 모델 성능에 중요한 요소가 될 수 있습니다.\n",
    "\n",
    "### 주요 텐서 자료형 (dtypes)\n",
    "\n",
    "다음은 PyTorch에서 지원하는 주요 텐서 자료형들입니다.\n",
    "\n",
    "#### 1. **정수형 텐서 (Integer Tensor)**\n",
    "\n",
    "- **`torch.int8`**: 8비트 정수형 (부호 있음: -128 ~ 127)\n",
    "  - 메모리 절약이 필요한 경우 사용하지만, 값의 범위가 매우 제한적입니다.\n",
    "  \n",
    "- **`torch.uint8`**: 8비트 정수형 (부호 없음: 0 ~ 255)\n",
    "  - 주로 이미지 데이터 (0~255 범위의 픽셀 값) 처리에 사용됩니다.\n",
    "  \n",
    "- **`torch.int16` (또는 `torch.short`)**: 16비트 정수형 (부호 있음: -32,768 ~ 32,767)\n",
    "  - 좀 더 넓은 범위를 다룰 수 있으나 여전히 메모리 사용량이 작습니다.\n",
    "  \n",
    "- **`torch.int32` (또는 `torch.int`)**: 32비트 정수형 (부호 있음: 약 -21억 ~ 21억)\n",
    "  - 일반적으로 많이 사용되는 자료형입니다.\n",
    "  \n",
    "- **`torch.int64` (또는 `torch.long`)**: 64비트 정수형 (부호 있음: 약 -9경 ~ 9경)\n",
    "  - 매우 큰 숫자 범위를 처리해야 하는 경우 사용됩니다.\n",
    "  - PyTorch에서 기본적으로 인덱스를 나타낼 때 사용하는 자료형입니다.\n",
    "\n",
    "#### 2. **부동소수점형 텐서 (Floating Point Tensor)**\n",
    "\n",
    "- **`torch.float16` (또는 `torch.half`)**: 16비트 부동소수점 (반정밀도)\n",
    "  - 메모리 사용량이 적으며, GPU 연산에서 속도를 높일 수 있습니다. 특히 NVIDIA GPU에서 **AMP(Automatic Mixed Precision)** 학습에 자주 사용됩니다.\n",
    "  - 값의 정확도는 낮을 수 있지만, 학습 시 자주 사용됩니다.\n",
    "\n",
    "- **`torch.float32` (또는 `torch.float`)**: 32비트 부동소수점 (단정밀도)\n",
    "  - 딥러닝에서 가장 많이 사용되는 자료형입니다. 메모리 효율성과 충분한 정확성을 모두 갖추고 있어 일반적인 연산에 적합합니다.\n",
    "  \n",
    "- **`torch.float64` (또는 `torch.double`)**: 64비트 부동소수점 (배정밀도)\n",
    "  - 매우 높은 정확도가 필요할 때 사용되지만, 메모리 사용량이 많고 연산 속도가 느릴 수 있습니다.\n",
    "  \n",
    "#### 3. **논리형 텐서 (Boolean Tensor)**\n",
    "\n",
    "- **`torch.bool`**: 1비트 논리형 (True 또는 False)\n",
    "  - 주로 조건에 따라 텐서의 값을 선택할 때 사용됩니다.\n",
    "  - 예시: 마스크 연산에 자주 사용됩니다.\n",
    "\n",
    "#### 4. **복소수형 텐서 (Complex Tensor)**\n",
    "\n",
    "- **`torch.complex64`**: 64비트 복소수형 (32비트 실수 부분 + 32비트 허수 부분)\n",
    "  - 복소수 연산이 필요한 신호 처리나 물리학 계산에 사용됩니다.\n",
    "  \n",
    "- **`torch.complex128`**: 128비트 복소수형 (64비트 실수 부분 + 64비트 허수 부분)\n",
    "  - 높은 정밀도가 요구되는 복소수 계산에 사용됩니다.\n",
    "\n",
    "### 자료형을 확인하고 설정하는 방법\n",
    "\n",
    "1. **자료형 확인**:\n",
    "   - 텐서의 자료형은 `.dtype` 속성을 통해 확인할 수 있습니다.\n",
    "   \n",
    "   ```python\n",
    "   t = torch.tensor([1.0, 2.0, 3.0])\n",
    "   print(t.dtype)  # 출력: torch.float32\n",
    "   ```\n",
    "\n",
    "2. **자료형 설정**:\n",
    "   - 텐서를 생성할 때 `dtype` 인자를 사용하여 자료형을 지정할 수 있습니다.\n",
    "   \n",
    "   ```python\n",
    "   t = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "   print(t.dtype)  # 출력: torch.int32\n",
    "   ```\n",
    "\n",
    "3. **자료형 변환**:\n",
    "   - 이미 생성된 텐서의 자료형을 변환하려면 `to()` 메서드를 사용합니다.\n",
    "   \n",
    "   ```python\n",
    "   t = torch.tensor([1.0, 2.0, 3.0])\n",
    "   t_int = t.to(torch.int32)\n",
    "   print(t_int.dtype)  # 출력: torch.int32\n",
    "   ```\n",
    "\n",
    "### 자료형 선택의 중요성\n",
    "\n",
    "- **메모리 효율성**: `float32`나 `float64` 같은 자료형은 많은 메모리를 차지할 수 있습니다. 만약 고정 소수점 연산이 충분하다면 `float16`을 사용하여 메모리를 절약할 수 있습니다.\n",
    "  \n",
    "- **정확성**: 복잡한 계산에서는 `float64`를 사용해야 할 수도 있지만, 대부분의 딥러닝 모델에서는 `float32`가 충분한 정확도를 제공합니다.\n",
    "\n",
    "- **성능**: `float16`은 GPU에서 빠른 연산을 가능하게 하지만, 그에 따른 정확도 손실도 발생할 수 있습니다. 따라서 학습에서는 성능과 정확도 사이의 균형을 고려해야 합니다.\n",
    "\n",
    "### 자료형 관련 예시 코드\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 기본 자료형은 float32\n",
    "t_default = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(t_default.dtype)  # 출력: torch.float32\n",
    "\n",
    "# int64 자료형 텐서\n",
    "t_int = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "print(t_int.dtype)  # 출력: torch.int64\n",
    "\n",
    "# float16 자료형 텐서\n",
    "t_half = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float16)\n",
    "print(t_half.dtype)  # 출력: torch.float16\n",
    "\n",
    "# bool 자료형 텐서\n",
    "t_bool = torch.tensor([True, False, True], dtype=torch.bool)\n",
    "print(t_bool.dtype)  # 출력: torch.bool\n",
    "\n",
    "# 자료형 변환\n",
    "t_float64 = t_default.to(torch.float64)\n",
    "print(t_float64.dtype)  # 출력: torch.float64\n",
    "```\n",
    "\n",
    "### 요약\n",
    "- PyTorch의 텐서는 다양한 자료형을 지원하며, 각 자료형은 메모리 사용량, 연산 성능, 정확성에 영향을 미칩니다.\n",
    "- `float32`는 딥러닝에서 가장 일반적으로 사용되며, 메모리 절약과 성능 향상을 위해 `float16`이 종종 사용됩니다.\n",
    "- `dtype` 속성을 통해 텐서의 자료형을 확인하고, 자료형을 지정하거나 변환할 수 있습니다.\n",
    "\n",
    "이렇게 텐서의 자료형에 대해 설명해 드렸습니다! 추가로 궁금한 사항이 있으면 언제든지 질문해 주세요! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 `torch.eye()` 함수를 사용하여 **단위 행렬**과 **직사각형 단위 행렬**을 생성하고, 앞서 정의한 `printInfo` 함수를 사용해 이 텐서들의 속성을 출력하는 예제입니다. 각 요소에 대해 설명해 드리겠습니다.\n",
    "\n",
    "### 전체 코드\n",
    "```python\n",
    "# -----------------------------------------------------------\n",
    "# 1로 채운 텐서 생성: torch.eye(행 [, 열])\n",
    "# ------------------------------------------------------------\n",
    "eten1 = torch.eye(3)\n",
    "eten2 = torch.eye(3, 5)\n",
    "printInfo(eten1, 'eten1')\n",
    "printInfo(eten2, 'eten2')\n",
    "```\n",
    "\n",
    "### 각 요소 설명\n",
    "\n",
    "#### 1. **`torch.eye()`**\n",
    "   - `torch.eye()`는 **단위 행렬(identity matrix)**을 생성하는 함수입니다. 단위 행렬은 대각선의 값이 1이고, 나머지 요소는 0으로 채워진 2차원 배열입니다.\n",
    "   - 이 함수의 기본 형식은 `torch.eye(행[, 열])`이며, 주어진 행과 열 크기에 맞게 단위 행렬을 생성합니다.\n",
    "   - 인자 설명:\n",
    "     - `행`: 생성할 행렬의 행 수를 지정합니다.\n",
    "     - `열` (선택적): 생성할 행렬의 열 수를 지정합니다. 제공하지 않으면 행과 같은 크기로 설정됩니다(즉, 정사각형 행렬을 생성).\n",
    "\n",
    "#### 2. **`eten1 = torch.eye(3)`**\n",
    "   - `torch.eye(3)`는 3x3 **정사각형 단위 행렬**을 생성합니다. 이 행렬은 대각선이 1이고, 나머지 요소는 0으로 채워집니다.\n",
    "   - 생성된 텐서는 다음과 같은 형태입니다:\n",
    "     ```python\n",
    "     tensor([[1., 0., 0.],\n",
    "             [0., 1., 0.],\n",
    "             [0., 0., 1.]])\n",
    "     ```\n",
    "\n",
    "#### 3. **`eten2 = torch.eye(3, 5)`**\n",
    "   - `torch.eye(3, 5)`는 3x5 **직사각형 단위 행렬**을 생성합니다. 이 행렬은 여전히 대각선이 1로 채워지고, 나머지는 0입니다.\n",
    "   - 생성된 텐서는 다음과 같은 형태입니다:\n",
    "     ```python\n",
    "     tensor([[1., 0., 0., 0., 0.],\n",
    "             [0., 1., 0., 0., 0.],\n",
    "             [0., 0., 1., 0., 0.]])\n",
    "     ```\n",
    "\n",
    "#### 4. **`printInfo(eten1, 'eten1')`**\n",
    "   - 이 함수 호출은 `eten1` 텐서의 속성을 출력합니다. `'eten1'`은 텐서의 이름으로 사용되며, `printInfo` 함수가 텐서의 모양, 차원 수, 자료형, 장치 정보, 실제 데이터 등을 출력합니다.\n",
    "   \n",
    "   - **출력 예시**:\n",
    "     ```\n",
    "     [eten1]\n",
    "     shape : torch.Size([3, 3])\n",
    "     ndim : 2차원\n",
    "     dtype : torch.float32\n",
    "     device : cpu\n",
    "     data :\n",
    "     tensor([[1., 0., 0.],\n",
    "             [0., 1., 0.],\n",
    "             [0., 0., 1.]])\n",
    "     ```\n",
    "\n",
    "#### 5. **`printInfo(eten2, 'eten2')`**\n",
    "   - 이 함수 호출은 `eten2` 텐서의 속성을 출력합니다. `'eten2'`은 텐서의 이름으로 사용됩니다.\n",
    "   \n",
    "   - **출력 예시**:\n",
    "     ```\n",
    "     [eten2]\n",
    "     shape : torch.Size([3, 5])\n",
    "     ndim : 2차원\n",
    "     dtype : torch.float32\n",
    "     device : cpu\n",
    "     data :\n",
    "     tensor([[1., 0., 0., 0., 0.],\n",
    "             [0., 1., 0., 0., 0.],\n",
    "             [0., 0., 1., 0., 0.]])\n",
    "     ```\n",
    "\n",
    "### 출력 설명\n",
    "\n",
    "1. **eten1 (3x3 단위 행렬)**:\n",
    "   - **shape**: `(3, 3)`로 3x3 행렬입니다.\n",
    "   - **ndim**: 2차원 텐서입니다.\n",
    "   - **dtype**: `float32` 자료형으로 저장되었습니다.\n",
    "   - **device**: CPU에 저장되어 있습니다.\n",
    "   - **data**: 단위 행렬로, 대각선이 1로 채워져 있고 나머지는 0입니다.\n",
    "\n",
    "2. **eten2 (3x5 직사각형 단위 행렬)**:\n",
    "   - **shape**: `(3, 5)`로 3x5 행렬입니다.\n",
    "   - **ndim**: 2차원 텐서입니다.\n",
    "   - **dtype**: `float32` 자료형으로 저장되었습니다.\n",
    "   -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.linspace()` 함수는 주어진 범위 내에서 **등간격**으로 나뉜 값들을 생성하는 PyTorch 함수입니다. 이 함수는 선형적으로 증가하는 값을 생성하는 데 사용되며, 시작점과 끝점을 포함하여 일정한 간격으로 배열을 생성할 수 있습니다.\n",
    "\n",
    "### 함수 정의\n",
    "\n",
    "```python\n",
    "torch.linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "```\n",
    "\n",
    "- **start**: 생성할 값들의 **시작점**입니다.\n",
    "- **end**: 생성할 값들의 **끝점**입니다.\n",
    "- **steps**: **생성할 값의 개수**입니다. 이 수만큼의 값을 등간격으로 생성합니다. 기본적으로 `start`부터 `end`까지 `steps`개의 값을 선형적으로 나누어 생성합니다.\n",
    "- **dtype** (선택적): 생성할 텐서의 데이터 타입을 지정할 수 있습니다. 예: `torch.float32`, `torch.int64` 등.\n",
    "- **device** (선택적): 텐서를 생성할 장치(CPU 또는 GPU)를 지정할 수 있습니다.\n",
    "- **requires_grad** (선택적): `True`로 설정하면 생성된 텐서에 대한 연산에서 기울기(gradient)를 계산할 수 있습니다.\n",
    "\n",
    "### 기본 사용 예시\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 0부터 10까지 5개의 값 생성\n",
    "t = torch.linspace(0, 10, steps=5)\n",
    "print(t)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "\n",
    "```\n",
    "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])\n",
    "```\n",
    "\n",
    "위 예제에서, `torch.linspace(0, 10, steps=5)`는 0에서 10까지의 범위에서 5개의 값을 등간격으로 생성합니다. 출력된 값은 `[0, 2.5, 5.0, 7.5, 10.0]`으로 0부터 10까지 5개의 값이 선형적으로 나뉘어 있습니다.\n",
    "\n",
    "### 주요 요소 설명\n",
    "\n",
    "1. **`start`**: 값의 시작점입니다. 위 예제에서는 0부터 시작합니다.\n",
    "2. **`end`**: 값의 끝점입니다. 예제에서는 10까지 값을 생성합니다.\n",
    "3. **`steps`**: 생성할 값의 개수입니다. 여기서는 5개의 값을 생성합니다.\n",
    "4. **등간격**: 각 값의 간격은 자동으로 계산되며, `start`와 `end`를 `steps` 수만큼 선형적으로 나눈 값을 반환합니다.\n",
    "\n",
    "### 추가 예시\n",
    "\n",
    "#### 1. 3개의 값 생성\n",
    "\n",
    "```python\n",
    "t = torch.linspace(-1, 1, steps=3)\n",
    "print(t)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "\n",
    "```\n",
    "tensor([-1.0000,  0.0000,  1.0000])\n",
    "```\n",
    "\n",
    "- `start=-1`에서 `end=1`까지 3개의 값을 생성하므로, 출력은 `[-1, 0, 1]`이 됩니다.\n",
    "\n",
    "#### 2. 10개의 값 생성\n",
    "\n",
    "```python\n",
    "t = torch.linspace(0, 2, steps=10)\n",
    "print(t)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "\n",
    "```\n",
    "tensor([0.0000, 0.2222, 0.4444, 0.6667, 0.8889, 1.1111, 1.3333, 1.5556, 1.7778, 2.0000])\n",
    "```\n",
    "\n",
    "- 0부터 2까지 10개의 값을 선형적으로 나누어 생성한 결과입니다.\n",
    "\n",
    "### 추가적인 파라미터\n",
    "\n",
    "- **`dtype`**: 데이터 타입을 명시적으로 설정할 수 있습니다.\n",
    "  ```python\n",
    "  t = torch.linspace(0, 1, steps=5, dtype=torch.float64)\n",
    "  print(t)\n",
    "  ```\n",
    "\n",
    "- **`device`**: 텐서를 특정 장치(CPU 또는 GPU)에 할당할 수 있습니다.\n",
    "  ```python\n",
    "  t = torch.linspace(0, 1, steps=5, device='cuda')\n",
    "  print(t)\n",
    "  ```\n",
    "\n",
    "- **`requires_grad`**: `True`로 설정하면 역전파 시 기울기를 계산할 수 있습니다.\n",
    "  ```python\n",
    "  t = torch.linspace(0, 1, steps=5, requires_grad=True)\n",
    "  print(t)\n",
    "  ```\n",
    "\n",
    "### 요약\n",
    "\n",
    "- **`torch.linspace()`**는 시작점과 끝점 사이를 **선형적으로 등간격**으로 나눈 값을 반환하는 함수입니다.\n",
    "- **`steps`**는 생성할 값의 개수를 지정합니다.\n",
    "- 필요에 따라 **데이터 타입**이나 **장치**를 지정할 수 있으며, **기울기 계산** 여부도 설정할 수 있습니다.\n",
    "\n",
    "이 함수는 데이터 전처리, 그래프 그리기, 신경망 입력 생성 등 다양한 상황에서 유용하게 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch의 `torch.as_tensor()` 함수는 데이터를 **텐서(tensor)**로 변환하는 데 사용됩니다. 이 함수는 기존의 데이터(리스트, 배열, numpy 배열 등)를 **복사하지 않고** PyTorch 텐서로 변환하는 방식으로 동작합니다. 데이터가 이미 텐서 형태일 경우에는 별도의 복사 없이 원본 데이터를 그대로 반환합니다.\n",
    "\n",
    "이 함수는 **기존 데이터를 텐서로 변환하는 상황**에서 유용하며, 특히 Numpy 배열과의 상호 호환성이 중요한 경우 유리합니다.\n",
    "\n",
    "### `torch.as_tensor()` 함수 정의\n",
    "\n",
    "```python\n",
    "torch.as_tensor(data, dtype=None, device=None)\n",
    "```\n",
    "\n",
    "#### 매개변수\n",
    "- **`data`**: 텐서로 변환할 입력 데이터. 리스트, 튜플, Numpy 배열, 다른 PyTorch 텐서 등이 가능합니다.\n",
    "- **`dtype`** (선택적): 텐서의 데이터 유형(자료형). 기본적으로 입력 데이터의 자료형을 유지하지만, 필요에 따라 자료형을 명시적으로 변경할 수 있습니다. 예: `torch.float32`, `torch.int64` 등.\n",
    "- **`device`** (선택적): 텐서를 저장할 장치(CPU 또는 GPU)를 지정합니다. 기본값은 입력 데이터가 저장된 장치와 동일한 장치에 저장됩니다.\n",
    "\n",
    "### 주요 특징\n",
    "- **복사가 일어나지 않음**: `torch.as_tensor()`는 가능한 경우 **데이터를 복사하지 않고** 메모리 참조만 사용하여 원래 데이터를 기반으로 텐서를 만듭니다.\n",
    "- **기존 메모리 공유**: Numpy 배열을 입력으로 제공하면, 텐서와 Numpy 배열이 같은 메모리 공간을 공유하게 됩니다. 즉, 텐서의 값을 변경하면 Numpy 배열의 값도 변경됩니다.\n",
    "\n",
    "### 예시 코드\n",
    "\n",
    "#### 1. 리스트를 텐서로 변환\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "data = [1, 2, 3, 4]\n",
    "tensor = torch.as_tensor(data)\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "\n",
    "```\n",
    "tensor([1, 2, 3, 4])\n",
    "```\n",
    "\n",
    "- 리스트 `[1, 2, 3, 4]`가 텐서로 변환되었습니다.\n",
    "\n",
    "#### 2. Numpy 배열을 텐서로 변환\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np_array = np.array([5.0, 6.0, 7.0])\n",
    "tensor = torch.as_tensor(np_array)\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "\n",
    "```\n",
    "tensor([5., 6., 7.], dtype=torch.float64)\n",
    "```\n",
    "\n",
    "- Numpy 배열이 텐서로 변환되었습니다. 이때 **데이터 복사가 일어나지 않으며**, 텐서와 Numpy 배열은 동일한 메모리를 공유합니다.\n",
    "\n",
    "#### 3. 데이터 변경 확인 (Numpy 배열과 PyTorch 텐서)\n",
    "\n",
    "```python\n",
    "np_array[0] = 10.0\n",
    "print(tensor)  # 텐서 값도 변경됨\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "\n",
    "```\n",
    "tensor([10.,  6.,  7.], dtype=torch.float64)\n",
    "```\n",
    "\n",
    "- Numpy 배열의 값을 변경하면, 해당 값을 기반으로 생성된 텐서의 값도 변경됩니다. 이는 **메모리 공유**가 이루어졌음을 보여줍니다.\n",
    "\n",
    "#### 4. 데이터 타입(`dtype`) 지정\n",
    "\n",
    "```python\n",
    "tensor = torch.as_tensor(data, dtype=torch.float32)\n",
    "print(tensor)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "\n",
    "```\n",
    "tensor([1., 2., 3., 4.])\n",
    "```\n",
    "\n",
    "- `dtype` 인자를 사용하여 텐서의 데이터 타입을 `float32`로 지정할 수 있습니다.\n",
    "\n",
    "#### 5. 텐서를 GPU에 할당\n",
    "\n",
    "```python\n",
    "tensor = torch.as_tensor(data, device='cuda')  # 텐서를 GPU에 할당\n",
    "print(tensor.device)  # 출력: cuda:0\n",
    "```\n",
    "\n",
    "- `device` 인자를 사용하여 텐서를 GPU로 할당할 수 있습니다. GPU에서 연산을 하고자 할 때 유용합니다.\n",
    "\n",
    "### `torch.tensor()`와의 차이점\n",
    "\n",
    "- **`torch.as_tensor()`**:\n",
    "  - 입력 데이터의 **복사가 일어나지 않으며**, 입력이 Numpy 배열일 경우 텐서와 배열은 메모리를 공유합니다.\n",
    "  - 이미 텐서인 경우 **새로운 텐서를 만들지 않고** 동일한 객체를 반환합니다.\n",
    "  \n",
    "- **`torch.tensor()`**:\n",
    "  - 항상 **새로운 텐서**를 생성하며, 데이터를 복사하여 텐서를 만듭니다.\n",
    "  - 복사본을 생성하기 때문에 원본 데이터와 독립적인 메모리 공간을 차지하게 됩니다.\n",
    "\n",
    "#### 차이 예시:\n",
    "\n",
    "```python\n",
    "# torch.as_tensor: 데이터 복사 없음\n",
    "np_array = np.array([1, 2, 3])\n",
    "t_as = torch.as_tensor(np_array)\n",
    "t_as[0] = 100\n",
    "print(np_array)  # Numpy 배열도 함께 변경됨\n",
    "\n",
    "# torch.tensor: 데이터 복사 발생\n",
    "t_tensor = torch.tensor(np_array)\n",
    "t_tensor[0] = 500\n",
    "print(np_array)  # Numpy 배열은 변경되지 않음\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "\n",
    "```\n",
    "[100   2   3]\n",
    "[100   2   3]  # Numpy 배열은 변경되지 않음\n",
    "```\n",
    "\n",
    "- `torch.as_tensor()`는 Numpy 배열과 메모리를 공유하므로 텐서를 변경하면 Numpy 배열도 변경됩니다.\n",
    "- `torch.tensor()`는 데이터를 복사하므로 Numpy 배열과 독립적으로 동작합니다.\n",
    "\n",
    "### 요약\n",
    "\n",
    "- **`torch.as_tensor()`**: 입력 데이터를 복사하지 않고 텐서로 변환하며, 데이터가 이미 텐서일 경우에는 새로 생성하지 않고 원래 텐서를 반환합니다. 주로 성능 최적화가 필요한 상황에서 사용됩니다.\n",
    "- **사용 시점**: 데이터가 이미 Numpy 배열이거나 텐서인 경우, **불필요한 메모리 복사**를 피하고 성능을 향상시키기 위해 사용할 수 있습니다.\n",
    "\n",
    "이 함수는 Numpy 배열에서 텐서로 변환할 때 **메모리 효율성**을 극대화하는 데 매우 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서 **`Storage`**는 **텐서(Tensor)**를 구성하는 **기본 데이터 저장 공간**입니다. 텐서가 데이터를 저장하는 방식을 이해하려면 `Storage` 개념을 알아야 합니다. `Storage`는 단순히 **1차원 배열**로, 텐서의 실제 데이터를 메모리상에 저장하는 역할을 합니다. 텐서가 N차원 구조를 가지더라도, 내부적으로는 `Storage`에 데이터를 저장하고, 텐서는 이 `Storage`의 데이터를 특정 규칙에 따라 참조합니다.\n",
    "\n",
    "### 주요 개념\n",
    "\n",
    "- **`Storage`는 저수준 데이터 구조**: 텐서의 데이터를 저장하는 역할을 하는 1차원 배열입니다. 텐서가 고차원일 수 있지만, 데이터는 `Storage`에 1차원으로 저장됩니다.\n",
    "- **`Tensor`는 `Storage`에 대한 뷰(View)**: 텐서는 `Storage`에 저장된 데이터를 **N차원 형태로 참조**하는 역할을 합니다.\n",
    "- **동일한 `Storage`를 공유할 수 있음**: 여러 텐서가 동일한 `Storage`를 참조할 수 있습니다. 이때 텐서는 `Storage`의 다른 부분을 참조하여 각기 다른 데이터를 나타낼 수 있습니다.\n",
    "\n",
    "### `Tensor`와 `Storage`의 관계\n",
    "\n",
    "1. **`Storage`는 데이터가 담긴 1차원 배열**입니다.\n",
    "2. **`Tensor`는 이 `Storage`를 특정한 방식으로 참조하여 다차원 데이터를 나타냅니다**. 텐서는 데이터를 참조하기 위해 **오프셋(offset)**과 **stride(스트라이드)** 정보를 사용합니다.\n",
    "   - **오프셋**: `Storage`의 어디에서 데이터를 읽기 시작할지를 나타냅니다.\n",
    "   - **스트라이드**: `Storage` 내에서 각 차원의 요소들이 떨어진 간격을 나타냅니다.\n",
    "\n",
    "### 간단한 예시\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 5개의 요소를 가지는 텐서를 생성합니다.\n",
    "t = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# 텐서의 Storage 확인\n",
    "print(t.storage())\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "```\n",
    " 1\n",
    " 2\n",
    " 3\n",
    " 4\n",
    " 5\n",
    "[torch.FloatStorage of size 5]\n",
    "```\n",
    "\n",
    "- 이 출력에서 알 수 있듯이, `Storage`는 텐서의 데이터를 1차원 배열로 저장합니다. `Storage`는 단순히 데이터의 **저장 공간**입니다.\n",
    "  \n",
    "### `Storage`를 참조하는 텐서\n",
    "\n",
    "여러 텐서가 동일한 `Storage`를 공유할 수 있습니다. 예를 들어, 하나의 `Storage`를 기반으로 서로 다른 뷰(view)를 가진 텐서를 만들 수 있습니다.\n",
    "\n",
    "```python\n",
    "# 원본 텐서\n",
    "t = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# 원본 텐서와 동일한 스토리지를 참조하는 텐서 생성 (슬라이싱)\n",
    "t_view = t[1:4]\n",
    "\n",
    "# 각 텐서의 Storage 확인\n",
    "print(t.storage())\n",
    "print(t_view.storage())\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "```\n",
    " 1\n",
    " 2\n",
    " 3\n",
    " 4\n",
    " 5\n",
    "[torch.FloatStorage of size 5]\n",
    "\n",
    " 1\n",
    " 2\n",
    " 3\n",
    " 4\n",
    " 5\n",
    "[torch.FloatStorage of size 5]\n",
    "```\n",
    "\n",
    "- 여기서 **`t`**와 **`t_view`**는 같은 `Storage`를 공유하고 있습니다. 두 텐서는 모두 같은 `Storage`를 참조하지만, **`t_view`**는 일부만 참조하고 있습니다.\n",
    "- **슬라이싱된 텐서 `t_view`**는 `t`와 동일한 `Storage`를 사용하지만, **오프셋(offset)**에 의해 **다른 부분**을 참조합니다.\n",
    "\n",
    "### `Storage`의 활용 예시\n",
    "\n",
    "1. **`Storage`로 직접 접근하여 데이터 변경**:\n",
    "   - `Storage`를 직접 수정하면 이를 참조하는 모든 텐서에 반영됩니다.\n",
    "\n",
    "```python\n",
    "# 원본 텐서\n",
    "t = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# 같은 스토리지를 참조하는 뷰 생성\n",
    "t_view = t[1:4]\n",
    "\n",
    "# t_view를 통해 storage의 데이터를 변경\n",
    "t_view.storage()[1] = 100\n",
    "\n",
    "# 원본 텐서와 뷰 텐서 출력\n",
    "print(t)  # tensor([  1, 100,   3,   4,   5])\n",
    "print(t_view)  # tensor([100,   3,   4])\n",
    "```\n",
    "\n",
    "- **`t_view.storage()[1] = 100`**: `Storage`의 두 번째 요소를 100으로 변경합니다.\n",
    "- **결과**: `t` 텐서의 **데이터가 변경**됩니다. 이는 `t_view`가 `Storage`를 공유하기 때문입니다.\n",
    "\n",
    "### `Tensor`와 `Storage`의 차이\n",
    "\n",
    "- **`Tensor`**: 데이터를 다차원 구조로 접근하기 위한 고수준 데이터 구조입니다. 텐서에는 `shape`, `stride` 등의 정보가 포함되어 있어, `Storage`를 다차원으로 참조할 수 있게 합니다.\n",
    "- **`Storage`**: 1차원 배열로, 텐서의 실제 데이터를 저장하는 역할을 합니다. 텐서는 이 `Storage`를 참조하여 데이터를 다루게 됩니다.\n",
    "\n",
    "### `Storage` 관련 메서드\n",
    "\n",
    "- **`Tensor.storage()`**: 텐서의 `Storage` 객체를 반환합니다.\n",
    "- **`Tensor.storage_offset()`**: 텐서가 `Storage`의 몇 번째 요소부터 데이터를 참조하는지 반환합니다.\n",
    "- **`Tensor.stride()`**: 텐서의 각 차원에서 데이터가 저장된 `Storage`에서 다음 요소로 가기 위해 건너뛰어야 하는 요소의 개수를 반환합니다.\n",
    "\n",
    "### 요약\n",
    "\n",
    "- **`Storage`**는 PyTorch에서 **데이터를 저장**하는 기본적인 공간입니다. 이는 1차원 배열로 구현되어 있으며, 텐서는 이 `Storage`를 참조하여 다차원 데이터를 구성합니다.\n",
    "- 여러 텐서가 동일한 `Storage`를 참조할 수 있으며, 이를 통해 메모리를 효율적으로 사용할 수 있습니다.\n",
    "- `Storage`에 직접 접근해 데이터를 변경하면, 이를 참조하는 텐서들에도 영향을 미칩니다.\n",
    "\n",
    "이와 같은 방식으로 PyTorch는 메모리 효율성과 데이터 접근의 유연성을 제공합니다. `Storage` 개념을 활용하면 메모리 사용량을 줄이면서도 여러 텐서가 같은 데이터를 다른 방식으로 다룰 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서 **텐서의 모양(Shape)**을 변경하는 것을 **\"Shape 변환(Reshaping)\"**이라고 하며, 이는 딥러닝 모델에서 매우 자주 사용되는 기능입니다. **Shape 변환**은 텐서의 데이터 배열 방식을 변경하여 데이터를 다차원 배열로 변환하거나, 다른 모양으로 재구성하는 과정입니다. 하지만 텐서에 포함된 **데이터의 값** 자체는 그대로 유지됩니다.\n",
    "\n",
    "다양한 방법으로 텐서의 모양을 변환할 수 있으며, PyTorch는 이를 위한 여러 유용한 함수들을 제공합니다. 가장 대표적인 함수는 **`view()`**, **`reshape()`**, **`squeeze()`**, **`unsqueeze()`** 등이 있습니다.\n",
    "\n",
    "### 텐서 Shape 변환을 위한 주요 메서드\n",
    "\n",
    "#### 1. **`view()`**\n",
    "- 텐서의 **크기(Shape)**를 변환할 때 가장 일반적으로 사용하는 함수입니다.\n",
    "- **새로운 모양**을 지정하면, PyTorch는 원래 데이터를 그대로 두고, 새로운 모양으로 텐서를 재구성합니다.\n",
    "- 주의할 점은 **연속된 메모리**(contiguous memory)에 저장된 텐서만 사용할 수 있다는 것입니다.\n",
    "\n",
    "##### 예시:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 2x3 텐서 생성\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 2x3 텐서를 3x2 텐서로 변환\n",
    "t_view = t.view(3, 2)\n",
    "print(t_view)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "```\n",
    "tensor([[1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6]])\n",
    "```\n",
    "\n",
    "- **`view(3, 2)`**: 원래 모양이 `2x3`인 텐서를 `3x2`로 변환합니다. 데이터 값은 그대로 유지되며, 단지 새로운 모양으로 배열됩니다.\n",
    "\n",
    "#### 2. **`reshape()`**\n",
    "- **`view()`와 비슷**하지만, 더 **유연**합니다.\n",
    "- `reshape()`는 입력 텐서가 연속된 메모리인지 확인하지 않으며, 필요할 경우 새로운 텐서를 생성하여 데이터를 복사할 수 있습니다.\n",
    "\n",
    "##### 예시:\n",
    "```python\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 2x3 텐서를 1차원 텐서로 변환\n",
    "t_reshape = t.reshape(6)\n",
    "print(t_reshape)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "```\n",
    "tensor([1, 2, 3, 4, 5, 6])\n",
    "```\n",
    "\n",
    "- **`reshape(6)`**: `2x3` 텐서를 1차원 벡터로 변환합니다. 결과는 `[1, 2, 3, 4, 5, 6]`입니다.\n",
    "\n",
    "#### 3. **`squeeze()`**\n",
    "- **불필요한 차원**(크기가 1인 차원)을 제거할 때 사용합니다.\n",
    "- 예를 들어, `(1, 3, 1, 5)` 형태의 텐서에서 크기가 1인 차원을 제거하면 `(3, 5)`가 됩니다.\n",
    "\n",
    "##### 예시:\n",
    "```python\n",
    "t = torch.tensor([[[1, 2, 3]]])\n",
    "\n",
    "# 차원 중 크기가 1인 차원을 제거\n",
    "t_squeeze = t.squeeze()\n",
    "print(t_squeeze)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "```\n",
    "tensor([1, 2, 3])\n",
    "```\n",
    "\n",
    "- **`squeeze()`**: 차원이 `(1, 3)`인 텐서에서 크기가 1인 차원을 제거하여 `(3,)` 모양으로 변환합니다.\n",
    "\n",
    "#### 4. **`unsqueeze()`**\n",
    "- 지정한 위치에 **새로운 차원**을 추가할 때 사용합니다.\n",
    "- 1차원 벡터를 2차원 또는 3차원으로 변환할 때 자주 사용됩니다.\n",
    "\n",
    "##### 예시:\n",
    "```python\n",
    "t = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 차원을 추가하여 (3,) -> (1, 3)\n",
    "t_unsqueeze = t.unsqueeze(0)\n",
    "print(t_unsqueeze)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "```\n",
    "tensor([[1, 2, 3]])\n",
    "```\n",
    "\n",
    "- **`unsqueeze(0)`**: 첫 번째 위치(0번째 차원)에 차원을 추가합니다. 원래 `(3,)` 모양의 텐서를 `(1, 3)`으로 변환합니다.\n",
    "\n",
    "#### 5. **`transpose()`**\n",
    "- 텐서의 **차원을 서로 교환**할 때 사용합니다.\n",
    "- 예를 들어, 2D 텐서에서 행(row)과 열(column)을 바꾸는 등의 작업을 할 수 있습니다.\n",
    "\n",
    "##### 예시:\n",
    "```python\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 행과 열을 교환\n",
    "t_transposed = t.transpose(0, 1)\n",
    "print(t_transposed)\n",
    "```\n",
    "\n",
    "**출력**:\n",
    "```\n",
    "tensor([[1, 4],\n",
    "        [2, 5],\n",
    "        [3, 6]])\n",
    "```\n",
    "\n",
    "- **`transpose(0, 1)`**: 첫 번째 차원(행)과 두 번째 차원(열)을 서로 교환하여, 원래 `2x3` 모양의 텐서를 `3x2`로 변환합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 다차원 텐서 변환 예시\n",
    "\n",
    "1. **차원 변환**:\n",
    "   - 3차원 텐서를 2차원으로 변환할 수 있습니다.\n",
    "\n",
    "```python\n",
    "t = torch.randn(2, 3, 4)  # 2x3x4 텐서 생성\n",
    "print(t.shape)  # 출력: torch.Size([2, 3, 4])\n",
    "\n",
    "# 2D로 변환\n",
    "t_reshaped = t.view(2, 12)\n",
    "print(t_reshaped.shape)  # 출력: torch.Size([2, 12])\n",
    "```\n",
    "\n",
    "2. **Batch 추가**:\n",
    "   - 모델 입력에 **배치(batch) 차원**을 추가하는 경우, `unsqueeze()`를 사용할 수 있습니다.\n",
    "\n",
    "```python\n",
    "# (3,) 형태의 텐서 생성\n",
    "t = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 배치 차원을 추가하여 (1, 3) 형태로 변환\n",
    "t_batched = t.unsqueeze(0)\n",
    "print(t_batched.shape)  # 출력: torch.Size([1, 3])\n",
    "```\n",
    "\n",
    "3. **고차원 데이터의 차원 교환**:\n",
    "   - 4차원 텐서에서 차원을 교환하여 순서를 바꿀 수 있습니다.\n",
    "\n",
    "```python\n",
    "t = torch.randn(10, 3, 28, 28)  # 10개의 3x28x28 이미지 텐서 생성\n",
    "print(t.shape)  # 출력: torch.Size([10, 3, 28, 28])\n",
    "\n",
    "# 2번째와 3번째 차원(채널, 높이) 교환\n",
    "t_transposed = t.transpose(1, 2)\n",
    "print(t_transposed.shape)  # 출력: torch.Size([10, 28, 3, 28])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 요약\n",
    "\n",
    "- **`view()`**: 텐서의 크기를 재조정하지만, 원래 데이터와 동일한 메모리 공간을 참조합니다. 데이터가 연속된 메모리 공간에 있어야 합니다.\n",
    "- **`reshape()`**: 텐서를 원하는 모양으로 재구성하며, 연속된 메모리가 아니더라도 사용할 수 있습니다.\n",
    "- **`squeeze()`**: 크기가 1인 차원을 제거합니다.\n",
    "- **`unsqueeze()`**: 새로운 차원을 추가합니다.\n",
    "- **`transpose()`**: 텐서의 두 차원을 교환합니다.\n",
    "\n",
    "이러한 방법들을 사용하면 텐서를 다양한 형태로 변환하여 데이터를 유연하게 처리할 수 있습니다. Shape 변환은 모델에 입력 데이터를 적절히 맞추거나, 연산을 최적화할 때 매우 유용하게 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 지금까지 설명한 **PyTorch의 주요 기능**을 종합적으로 보여주는 실용적인 코드입니다. 이 코드는 **데이터 생성, 텐서의 변환(Shape 변환), 모델 정의, 손실 함수 및 옵티마이저 설정, 학습 과정, 학습률 스케줄러 및 드롭아웃 적용** 등 PyTorch에서 자주 사용되는 요소들을 모두 포함하고 있습니다.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1. 데이터 생성 및 DataLoader 정의\n",
    "# -----------------------------------------------------\n",
    "# 100개의 샘플, 각 샘플은 3개의 입력값을 가짐\n",
    "data = torch.randn(100, 3)\n",
    "# 각 샘플에 대한 타겟 값 (100개의 출력)\n",
    "targets = torch.randn(100, 1)\n",
    "\n",
    "# Dataset 및 DataLoader 설정 (배치 크기는 10)\n",
    "dataset = TensorDataset(data, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# 2. 간단한 모델 정의\n",
    "# -----------------------------------------------------\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # 입력 3개, 출력 50개의 선형 레이어\n",
    "        self.fc1 = nn.Linear(3, 50)\n",
    "        # 드롭아웃 적용 (50% 확률로 뉴런 비활성화)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        # 최종 출력 1개의 선형 레이어\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ReLU 활성화 함수 및 드롭아웃 적용\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        # 최종 출력 계산\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 3. 손실 함수 및 옵티마이저 설정\n",
    "# -----------------------------------------------------\n",
    "model = SimpleModel()\n",
    "\n",
    "# 손실 함수: 평균 제곱 오차(MSE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 옵티마이저: SGD + 학습률 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습률 스케줄러: 10번의 에폭마다 학습률을 10% 감소시킴\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# 4. 학습 루프\n",
    "# -----------------------------------------------------\n",
    "epochs = 30  # 30번 반복 학습\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # 학습 모드\n",
    "    for batch_data, batch_targets in dataloader:\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        output = model(batch_data)  # 예측값 계산\n",
    "        loss = criterion(output, batch_targets)  # 손실 계산\n",
    "        loss.backward()  # 역전파로 기울기 계산\n",
    "        optimizer.step()  # 파라미터 업데이트\n",
    "    \n",
    "    scheduler.step()  # 학습률 갱신\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, LR: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "# 5. 모델 평가 모드로 전환\n",
    "# -----------------------------------------------------\n",
    "model.eval()  # 평가 모드\n",
    "\n",
    "# 평가를 위한 가상 입력 데이터 (크기 5x3)\n",
    "test_input = torch.randn(5, 3)\n",
    "\n",
    "# 모델 예측값 계산\n",
    "with torch.no_grad():  # 평가 시에는 기울기 계산 안함\n",
    "    test_output = model(test_input)\n",
    "    print(f\"Test Output: {test_output}\")\n",
    "\n",
    "# 6. 텐서의 Shape 변환 및 데이터 처리\n",
    "# -----------------------------------------------------\n",
    "# 텐서 생성\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 텐서 크기 변경: 2x3 -> 3x2\n",
    "t_view = t.view(3, 2)\n",
    "print(f\"Reshaped Tensor (view):\\n{t_view}\")\n",
    "\n",
    "# 1차원으로 변환\n",
    "t_reshape = t.reshape(6)\n",
    "print(f\"Flattened Tensor (reshape):\\n{t_reshape}\")\n",
    "\n",
    "# 차원 추가: (3,) -> (1, 3)\n",
    "t_unsqueeze = t_reshape.unsqueeze(0)\n",
    "print(f\"Tensor with extra dimension (unsqueeze):\\n{t_unsqueeze}\")\n",
    "\n",
    "# 차원 제거: (1, 3) -> (3,)\n",
    "t_squeeze = t_unsqueeze.squeeze()\n",
    "print(f\"Tensor with removed dimension (squeeze):\\n{t_squeeze}\")\n",
    "\n",
    "# 행과 열을 교환: 2x3 -> 3x2\n",
    "t_transposed = t.transpose(0, 1)\n",
    "print(f\"Transposed Tensor:\\n{t_transposed}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 코드 설명\n",
    "\n",
    "1. **데이터 생성 및 DataLoader 정의**:\n",
    "   - `data`와 `targets`는 학습 데이터입니다.\n",
    "   - `TensorDataset`을 이용해 데이터를 하나로 묶고, `DataLoader`로 배치(batch) 단위로 데이터를 나누고 **랜덤하게 섞어서** 처리합니다.\n",
    "\n",
    "2. **모델 정의**:\n",
    "   - `SimpleModel` 클래스는 두 개의 선형 레이어를 가진 간단한 신경망 모델입니다.\n",
    "   - 첫 번째 레이어는 3개의 입력을 받아 50개의 출력으로 변환하며, `ReLU` 활성화 함수와 **드롭아웃**이 적용됩니다.\n",
    "   - 두 번째 레이어는 50개의 출력을 1개의 출력으로 변환합니다.\n",
    "\n",
    "3. **손실 함수 및 옵티마이저**:\n",
    "   - **손실 함수**는 **MSELoss**(평균 제곱 오차)를 사용하여 회귀 문제를 해결합니다.\n",
    "   - **옵티마이저**는 SGD(확률적 경사 하강법)을 사용하고 있으며, 학습률을 **스케줄러**로 조정하여 학습 도중 학습률을 점차 감소시킵니다.\n",
    "\n",
    "4. **학습 루프**:\n",
    "   - 매 에폭(epoch)마다 데이터를 모델에 넣고, 예측값을 계산한 뒤 손실 함수로 손실을 계산합니다.\n",
    "   - 그 후 역전파(backpropagation)로 기울기를 계산하고, 옵티마이저로 파라미터를 업데이트합니다.\n",
    "   - 학습률 스케줄러는 매 에폭마다 학습률을 조정합니다.\n",
    "\n",
    "5. **모델 평가**:\n",
    "   - 학습이 끝나면 **평가 모드**로 전환(`model.eval()`)하고, 기울기를 계산하지 않도록 `torch.no_grad()`로 감싸서 예측값을 계산합니다.\n",
    "\n",
    "6. **텐서의 Shape 변환**:\n",
    "   - `view()`, `reshape()`, `squeeze()`, `unsqueeze()` 및 `transpose()`를 사용하여 텐서의 모양을 변경하고, 차원을 추가하거나 제거하는 방법을 보여줍니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 실용적인 적용\n",
    "\n",
    "- **학습**: 데이터를 학습시키고, 손실 함수를 줄여가면서 파라미터를 최적화하는 과정을 포함합니다.\n",
    "- **텐서 변환**: 데이터 전처리나 모델 입출력의 차원을 조정할 때 자주 사용됩니다.\n",
    "- **모델 평가**: 학습 후 모델을 평가하여 예측 결과를 얻습니다.\n",
    "\n",
    "이 코드는 PyTorch의 여러 기능들을 종합적으로 보여주며, 딥러닝 학습의 전반적인 흐름을 이해하는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
