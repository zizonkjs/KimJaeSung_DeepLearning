{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**활성화 함수(Activation Function)**는 인공신경망에서 **뉴런의 출력 값을 결정**하는 비선형 함수입니다. 입력 데이터가 가중치와 더해져 합산된 값이 활성화 함수를 통과하여 **비선형 변환**을 이루고, 이를 통해 신경망이 복잡한 패턴을 학습할 수 있게 합니다. 활성화 함수는 신경망의 중요한 요소로, 데이터의 선형적인 특성뿐만 아니라 비선형적인 특성까지 학습할 수 있도록 해줍니다.\n",
    "\n",
    "### 활성화 함수의 역할\n",
    "1. **비선형성 추가**: 인공신경망이 단순한 선형 모델을 넘어 **복잡한 문제**를 학습할 수 있게 합니다.\n",
    "2. **출력값 스케일링**: 활성화 함수는 출력값을 **특정 범위로 제한**하여, 모델의 안정성을 유지하고 학습이 잘 이루어지도록 돕습니다.\n",
    "3. **다층 신경망 구성 가능**: 활성화 함수가 없다면, 신경망의 각 층은 선형 함수로만 구성되어 최종 모델도 선형 모델에 불과합니다. 활성화 함수가 비선형성을 추가함으로써 다층 구조의 신경망이 가능해집니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 주요 활성화 함수들\n",
    "\n",
    "#### 1. **Sigmoid (시그모이드) 함수**\n",
    "   - **수식**: \n",
    "     \\[\n",
    "     f(x) = \\frac{1}{1 + e^{-x}}\n",
    "     \\]\n",
    "   - **출력 값 범위**: 0 ~ 1\n",
    "   - **특징**: \n",
    "     - **출력값을 0과 1 사이**로 압축합니다. 이진 분류 문제에서 자주 사용됩니다.\n",
    "     - **단점**: **기울기 소실(Vanishing Gradient)** 문제가 발생할 수 있으며, 출력이 극단적으로 0 또는 1에 가까워지면 미분 값이 거의 0이 되어 학습이 어려워질 수 있습니다.\n",
    "   - **사용 예시**: 이진 분류 모델에서 마지막 출력층의 활성화 함수로 사용됩니다.\n",
    "\n",
    "#### 2. **Tanh (하이퍼볼릭 탄젠트) 함수**\n",
    "   - **수식**:\n",
    "     \\[\n",
    "     f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "     \\]\n",
    "   - **출력 값 범위**: -1 ~ 1\n",
    "   - **특징**:\n",
    "     - 출력 값이 **-1과 1 사이**로 압축되며, 시그모이드 함수보다 **0 중심**에 가깝습니다. 즉, 입력값이 0이면 출력도 0이 됩니다.\n",
    "     - **단점**: 시그모이드 함수와 마찬가지로 **기울기 소실** 문제가 발생할 수 있습니다.\n",
    "   - **사용 예시**: 중간층에서 많이 사용되며, 이진 분류 문제에서도 활용됩니다.\n",
    "\n",
    "#### 3. **ReLU (Rectified Linear Unit) 함수**\n",
    "   - **수식**:\n",
    "     \\[\n",
    "     f(x) = \\max(0, x)\n",
    "     \\]\n",
    "   - **출력 값 범위**: 0 ~ ∞\n",
    "   - **특징**:\n",
    "     - **0 이상의 값은 그대로 출력**, 음수 값은 0으로 출력합니다. 이로 인해 연산이 간단하고, 학습 속도가 빠릅니다.\n",
    "     - **장점**: 기울기 소실 문제가 상대적으로 적습니다.\n",
    "     - **단점**: **죽은 ReLU 문제(Dead ReLU)**가 발생할 수 있습니다. 즉, 뉴런이 항상 0을 출력하는 상태가 될 수 있으며, 이 경우 학습이 더 이상 진행되지 않습니다.\n",
    "   - **사용 예시**: **대부분의 신경망**에서 기본적으로 사용됩니다.\n",
    "\n",
    "#### 4. **Leaky ReLU 함수**\n",
    "   - **수식**:\n",
    "     \\[\n",
    "     f(x) = \n",
    "     \\begin{cases} \n",
    "     x & \\text{if } x > 0 \\\\ \n",
    "     \\alpha x & \\text{if } x \\leq 0 \n",
    "     \\end{cases}\n",
    "     \\]\n",
    "   - **출력 값 범위**: -∞ ~ ∞\n",
    "   - **특징**:\n",
    "     - ReLU와 유사하지만, **음수 입력에 대해 작은 기울기**(α)를 적용하여 완전히 0으로 출력되지 않도록 합니다.\n",
    "     - **장점**: 죽은 ReLU 문제를 해결할 수 있습니다.\n",
    "   - **사용 예시**: **음수 값**도 어느 정도 학습에 반영해야 할 때 유용합니다.\n",
    "\n",
    "#### 5. **ELU (Exponential Linear Unit) 함수**\n",
    "   - **수식**:\n",
    "     \\[\n",
    "     f(x) = \n",
    "     \\begin{cases} \n",
    "     x & \\text{if } x > 0 \\\\ \n",
    "     \\alpha (e^x - 1) & \\text{if } x \\leq 0 \n",
    "     \\end{cases}\n",
    "     \\]\n",
    "   - **출력 값 범위**: -α ~ ∞\n",
    "   - **특징**:\n",
    "     - ReLU와 유사하지만, **음수 값에 대해 지수 함수**를 적용하여 출력합니다.\n",
    "     - **장점**: **0을 중심으로** 출력을 조정하므로 학습이 빠르게 진행될 수 있습니다.\n",
    "     - **단점**: ReLU보다 연산이 복잡할 수 있습니다.\n",
    "   - **사용 예시**: ReLU와 유사한 상황에서 사용되며, 죽은 ReLU 문제를 해결할 수 있습니다.\n",
    "\n",
    "#### 6. **Softmax 함수**\n",
    "   - **수식**:\n",
    "     \\[\n",
    "     f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "     \\]\n",
    "   - **출력 값 범위**: 0 ~ 1 (모든 출력값의 합은 1)\n",
    "   - **특징**:\n",
    "     - 입력값들을 **확률 분포**로 변환합니다.\n",
    "     - **장점**: **다중 클래스 분류** 문제에서 매우 유용합니다. 각 클래스의 예측 확률을 출력합니다.\n",
    "   - **사용 예시**: **다중 클래스 분류 문제**의 마지막 출력층에서 사용됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 활성화 함수 비교 및 사용 예시\n",
    "\n",
    "| 함수      | 출력 범위   | 비선형성  | 장점 | 단점 |\n",
    "|-----------|-------------|------------|-------|-------|\n",
    "| **Sigmoid** | 0 ~ 1       | 있음       | 이진 분류에 유용 | 기울기 소실 문제 |\n",
    "| **Tanh**    | -1 ~ 1      | 있음       | 0 중심 | 기울기 소실 문제 |\n",
    "| **ReLU**    | 0 ~ ∞       | 있음       | 연산 간단, 기울기 소실 적음 | 죽은 ReLU 문제 |\n",
    "| **Leaky ReLU** | -∞ ~ ∞  | 있음       | 죽은 ReLU 문제 해결 | 출력이 항상 0에 가까운 경우 문제 |\n",
    "| **ELU**     | -α ~ ∞     | 있음       | 0 중심 | 연산 복잡 |\n",
    "| **Softmax** | 0 ~ 1       | 있음       | 확률 출력 | 다중 클래스 분류에서만 사용 |\n",
    "\n",
    "---\n",
    "\n",
    "### 활성화 함수 선택 기준\n",
    "\n",
    "1. **ReLU**는 대부분의 **신경망**에서 기본 활성화 함수로 널리 사용되며, 그 단점인 죽은 ReLU 문제를 해결하고 싶을 때는 **Leaky ReLU**나 **ELU**를 선택합니다.\n",
    "2. **Sigmoid**와 **Tanh**는 기울기 소실 문제 때문에 주로 **출력층**에서 사용되며, 이진 분류 문제에서는 **Sigmoid**, **다중 클래스 분류**에서는 **Softmax**가 많이 사용됩니다.\n",
    "3. 특정한 문제나 데이터셋에 맞게 **활성화 함수**를 실험적으로 선택하는 것도 중요합니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
