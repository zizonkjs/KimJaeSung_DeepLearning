{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 정의 모델 클래스\n",
    "- 부모클래스 : nn.Module\n",
    "- 필수오버라이딩 :   \n",
    "    *__int__() : 모델 층 구성, 설계  \n",
    "    *forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈로딩\n",
    "import torch    # 텐서 관련 모듈\n",
    "import torch.nn as nn # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F # 인공신경망 관련 함수들 모듈 ( 손실 함수, 활성화함수 등등 )\n",
    "import torch.optim as optim # 최적화 관련 모듈 ( 가중치 , 절편 빠르게 찾아주는 알고리즘 )\n",
    "from torchinfo import summary # 모델 구조 및 저보 관련 모듈\n",
    "from torchmetrics.regression import * # 회귀 성능 관련\n",
    "from torchmetrics.classification import * # 분류 성능 관련\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "# 텐서 저장 및 실행 위치설정\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망클래스  \n",
    "    * 입력층 - 입력 피쳐 고정  \n",
    "    * 출력층 - 출력 타겟 고정  \n",
    "    * 은닉층 - 고정  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 사용할 데이터셋 : if 피쳐 4개 , 타겟 1개 , 회귀\n",
    "# 입력층 : 입력 4개     출력 : 20개     AF  ReLU\n",
    "# 은닉층 : 입력 20개    출력 : 100개    AF  ReLU\n",
    "# 출력층 : 입력 100개   출력 : 1개      AF  X, Sigmoid & SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    # 이느턴스 객체 생성 시 자동 호출 메서드 (콜백 함수)\n",
    "    def __init__(self):\n",
    "        # 1. 부모클래스 생성\n",
    "        super().__init__()\n",
    "\n",
    "        # 2. 자식클래스의 인스턴스 속성 설정\n",
    "    \n",
    "        self.input_layer = nn.Linear(4, 20)     # W4 + b1 => 1p, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer= nn.Linear(20, 100)   # W 20 + b1 => 21*100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100, 1)   # W 100 + b1 => 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백 함수 call back func : 시스템에서 호출되는 함수)# y\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x) # y= x1w1+w2w2+x3w3+x4w4 + b\n",
    "        y = F.relu(y)      #  0 <= y ---> 죽은 relu ==> leakyReLu\n",
    "        y = self.hidden_layer(y) # \n",
    "        y = F.relu(y)\n",
    "        return  self.output_layer(y)\n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델\n",
    "class MyModel2(nn.Module):\n",
    "    # 이느턴스 객체 생성 시 자동 호출 메서드 (콜백 함수)\n",
    "    def __init__(self, in_feature):\n",
    "        # 1. 부모클래스 생성\n",
    "        super().__init__()\n",
    "\n",
    "        # 2. 자식클래스의 인스턴스 속성 설정\n",
    "    \n",
    "        self.input_layer = nn.Linear(in_feature, 20)     # W4 + b1 => 1p, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer= nn.Linear(20, 100)   # W 20 + b1 => 21*100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(100, 1)   # W 100 + b1 => 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백 함수 call back func : 시스템에서 호출되는 함수)# y\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x) # y= x1w1+w2w2+x3w3+x4w4 + b\n",
    "        y = F.relu(y)      #  0 <= y ---> 죽은 relu ==> leakyReLu\n",
    "        y = self.hidden_layer(y) # \n",
    "        y = F.relu(y)\n",
    "        return  self.output_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐수, 은칙층 퍼렙트론수가 동적인 모델\n",
    "# 입력 피쳐 수가 동적인 모델\n",
    "class MyModel3(nn.Module):\n",
    "    # 이느턴스 객체 생성 시 자동 호출 메서드 (콜백 함수)\n",
    "    def __init__(self, in_feature, in_out, h_out):\n",
    "        # 1. 부모클래스 생성\n",
    "        super().__init__()\n",
    "\n",
    "        # 2. 자식클래스의 인스턴스 속성 설정\n",
    "    \n",
    "        self.input_layer = nn.Linear(in_feature, in_out)     # W4 + b1 => 1p, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer= nn.Linear(in_out, h_out)   # W 20 + b1 => 21*100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(h_out, 1)   # W 100 + b1 => 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백 함수 call back func : 시스템에서 호출되는 함수)# y\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x) # y= x1w1+w2w2+x3w3+x4w4 + b\n",
    "        y = F.relu(y)      #  0 <= y ---> 죽은 relu ==> leakyReLu\n",
    "        y = self.hidden_layer(y) # \n",
    "        y = F.relu(y)\n",
    "        return  self.output_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module):\n",
    "    # 이느턴스 객체 생성 시 자동 호출 메서드 (콜백 함수)\n",
    "    def __init__(self, in_feature, in_out, h_out):\n",
    "        # 1. 부모클래스 생성\n",
    "        super().__init__()\n",
    "\n",
    "        # 2. 자식클래스의 인스턴스 속성 설정\n",
    "    \n",
    "        self.input_layer = nn.Linear(in_feature, in_out)     # W4 + b1 => 1p, 5 * 20 = 100개 변수\n",
    "        self.hidden_layer= nn.Linear(in_out, h_out)   # W 20 + b1 => 21*100 = 2100개 변수\n",
    "        self.output_layer = nn.Linear(h_out, 1)   # W 100 + b1 => 101 * 1 = 101개 변수\n",
    "\n",
    "    # 순방향/전방향 학습 진행 시 자동 호출되는 메서드 (콜백 함수 call back func : 시스템에서 호출되는 함수)# y\n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        print('calling forward()')\n",
    "        y = self.input_layer(x) # y= x1w1+w2w2+x3w3+x4w4 + b\n",
    "        y = F.relu(y)      #  0 <= y ---> 죽은 relu ==> leakyReLu\n",
    "        y = self.hidden_layer(y) # \n",
    "        y = F.relu(y)\n",
    "        return  self.output_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDynamicModel(\n",
      "  (input_layer): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=30, bias=True)\n",
      "    (2): Linear(in_features=30, out_features=40, bias=True)\n",
      "    (3): Linear(in_features=40, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=60, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=60, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyDynamicModel(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layers, out_feature=1):\n",
    "        super(MyDynamicModel, self).__init__()\n",
    "\n",
    "        # 1. 입력층 정의\n",
    "        self.input_layer = nn.Linear(in_feature, hidden_layers[0] if hidden_layers else out_feature)\n",
    "        \n",
    "        # 2. 은닉층을 동적으로 정의 (빈 리스트가 들어오면 은닉층을 생략)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "\n",
    "        # 3. 출력층 정의 (은닉층이 없으면 입력층에서 바로 출력층으로 연결)\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1] if hidden_layers else in_feature, out_feature)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력층 처리\n",
    "        x = F.relu(self.input_layer(x))\n",
    "\n",
    "        # 은닉층 처리 (은닉층이 있다면 처리)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        # 출력층 처리\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# 은닉층 없이 모델을 구성 (입력층 -> 출력층만 존재)\n",
    "model = MyDynamicModel(in_feature=10, hidden_layers=[10,20,30,40,50,60])\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247],\n",
      "        [ 0.1826, -0.1949, -0.0365, -0.0450],\n",
      "        [ 0.0725, -0.0020,  0.4371,  0.1556],\n",
      "        [-0.1862, -0.3020, -0.0838, -0.2157],\n",
      "        [-0.1602,  0.0239,  0.2981,  0.2718],\n",
      "        [-0.4888,  0.3100,  0.1397,  0.4743],\n",
      "        [ 0.3300, -0.4556, -0.4754, -0.2412],\n",
      "        [ 0.4391, -0.0833,  0.2140, -0.2324],\n",
      "        [ 0.4906, -0.2115,  0.3750,  0.0059],\n",
      "        [-0.2634,  0.2570, -0.2654,  0.1471],\n",
      "        [-0.1444, -0.0548, -0.4807, -0.2384],\n",
      "        [ 0.2713, -0.1215,  0.4980,  0.4008],\n",
      "        [-0.0234, -0.3337,  0.3045,  0.1552],\n",
      "        [-0.3232,  0.3248,  0.3036,  0.4434],\n",
      "        [-0.2803, -0.0823, -0.0097,  0.0730],\n",
      "        [-0.3795, -0.3548,  0.2720, -0.1172],\n",
      "        [ 0.2442,  0.0285,  0.1642,  0.1099],\n",
      "        [ 0.1818,  0.2479, -0.4631,  0.2517]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([-0.3516, -0.3773,  0.0304, -0.0852,  0.2937, -0.2896, -0.4445,  0.3639,\n",
      "        -0.0741,  0.2812,  0.1607, -0.3749,  0.1004,  0.1201, -0.3348, -0.2372,\n",
      "         0.1705,  0.0896, -0.2127, -0.1514], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2048, -0.0414,  0.1261,  ...,  0.1389, -0.1618, -0.1610],\n",
      "        [-0.1352,  0.0281,  0.2229,  ...,  0.0578, -0.1529, -0.1878],\n",
      "        [-0.1025, -0.0260, -0.1371,  ..., -0.0956,  0.1590, -0.0731],\n",
      "        ...,\n",
      "        [-0.1844, -0.1471, -0.1923,  ...,  0.1118,  0.0691,  0.0738],\n",
      "        [-0.2086, -0.0753, -0.1438,  ...,  0.1593, -0.0429,  0.0194],\n",
      "        [-0.1867,  0.1260,  0.0129,  ...,  0.0369, -0.1090, -0.0607]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([-0.0588,  0.1044,  0.1574,  0.1782, -0.1521, -0.0239,  0.0525, -0.0874,\n",
      "         0.1470, -0.0249,  0.0239, -0.1912,  0.1616, -0.1482, -0.0054, -0.1824,\n",
      "         0.1701,  0.0129,  0.1007, -0.1758,  0.0190,  0.0916,  0.1579,  0.1643,\n",
      "        -0.0399, -0.0227,  0.0743, -0.0804, -0.0910, -0.2059, -0.2193, -0.1177,\n",
      "         0.0279,  0.0180,  0.0179,  0.0335, -0.0413, -0.2209,  0.1997, -0.1636,\n",
      "        -0.0026,  0.0888,  0.0573, -0.0971, -0.1943,  0.1790,  0.0064,  0.2039,\n",
      "         0.1043,  0.0955,  0.0484, -0.0504, -0.1311,  0.0272,  0.2149,  0.1341,\n",
      "         0.2031,  0.2097,  0.1747, -0.1255, -0.1218, -0.0077, -0.1156, -0.0607,\n",
      "        -0.0766,  0.1957, -0.0150,  0.2164, -0.1734, -0.2206,  0.0320,  0.0446,\n",
      "        -0.0902, -0.1132,  0.0637,  0.0112, -0.0300,  0.1135, -0.0951,  0.1759,\n",
      "        -0.1525, -0.1164,  0.0774,  0.1374, -0.0027, -0.1755, -0.0089, -0.0755,\n",
      "        -0.0714, -0.0413,  0.0538, -0.0520, -0.1568, -0.1116, -0.1873, -0.2233,\n",
      "         0.1081,  0.1249,  0.1690,  0.0177], requires_grad=True))\n",
      "('output_layer.weight', Parameter containing:\n",
      "tensor([[-0.0411, -0.0601,  0.0498, -0.0604,  0.0747, -0.0542, -0.0933, -0.0720,\n",
      "          0.0814,  0.0755,  0.0244,  0.0023,  0.0857,  0.0603,  0.0592, -0.0681,\n",
      "          0.0160,  0.0906, -0.0704,  0.0996, -0.0655, -0.0294, -0.0047,  0.0260,\n",
      "          0.0307, -0.0572,  0.0458,  0.0265, -0.0111, -0.0995,  0.0990, -0.0370,\n",
      "         -0.0533,  0.0990, -0.0507,  0.0668,  0.0610, -0.0651, -0.0806, -0.0544,\n",
      "          0.0313,  0.0943, -0.0056, -0.0299, -0.0288, -0.0696,  0.0999, -0.0011,\n",
      "         -0.0791, -0.0683,  0.0622,  0.0484,  0.0645, -0.0105, -0.0198,  0.0328,\n",
      "         -0.0010, -0.0322,  0.0238, -0.0570, -0.0692, -0.0221, -0.0713,  0.0025,\n",
      "          0.0157,  0.0979,  0.0288, -0.0006, -0.0727, -0.0402, -0.0163, -0.0720,\n",
      "          0.0195,  0.0720,  0.0572, -0.0415, -0.0244,  0.0997, -0.0160, -0.0898,\n",
      "         -0.0009,  0.0051, -0.0372, -0.0109, -0.0815,  0.0308, -0.0694,  0.0344,\n",
      "          0.0753, -0.0646, -0.0392,  0.0096,  0.0002,  0.0586, -0.0060,  0.0162,\n",
      "         -0.0276,  0.0330,  0.0302, -0.0805]], requires_grad=True))\n",
      "('output_layer.bias', Parameter containing:\n",
      "tensor([0.0531], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "## 모델 인스턴스 생성\n",
    "m1 = MyModel(4)\n",
    "for m in m1.named_parameters(): print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling forward()\n",
      "tensor([[-0.1395],\n",
      "        [-0.1858]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델 인스턴스명 (데이터)\n",
    "# 임이의 데이터\n",
    "datats = torch.FloatTensor([[1,3,5,7], [2,4,6,8]])\n",
    "targetts=torch.FloatTensor([[4],[5]])\n",
    "\n",
    "# 학습\n",
    "pre_y=m1(datats)\n",
    "\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
