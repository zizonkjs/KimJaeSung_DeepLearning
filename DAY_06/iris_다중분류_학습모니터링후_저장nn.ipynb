{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN 기반 다중분류 모델 구현\n",
    "- 사용되는 데이터셋 : iris.csv\n",
    "- feature : 4개\n",
    "- target : 1개 Setosa와 나머지\n",
    "- 학습방법 : 지도학습 -> 분류 > 다중분류\n",
    "- 알고리즘 : 인공신경망(ANN) -> 심층(은닉층) 신경망 -> MLP(층이여러개), DNN(은닉층이 많은 구성) \n",
    "- FramWork : Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "# 모델관련 모듈\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchinfo import summary\n",
    "\n",
    "# 데이터 전처리 및 시각화 모듈\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch  2.4.1\n",
      " pandas  2.0.3\n"
     ]
    }
   ],
   "source": [
    "# 활용 패키지 버전 체크\n",
    "def versioncheck():\n",
    "    print(f' torch  {torch.__version__}')\n",
    "    print(f' pandas  {pd.__version__}')\n",
    "\n",
    "versioncheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width    variety\n",
       "0             5.1          3.5           1.4          0.2     Setosa\n",
       "1             4.9          3.0           1.4          0.2     Setosa\n",
       "2             4.7          3.2           1.3          0.2     Setosa\n",
       "3             4.6          3.1           1.5          0.2     Setosa\n",
       "4             5.0          3.6           1.4          0.2     Setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  Virginica\n",
       "146           6.3          2.5           5.0          1.9  Virginica\n",
       "147           6.5          3.0           5.2          2.0  Virginica\n",
       "148           6.2          3.4           5.4          2.3  Virginica\n",
       "149           5.9          3.0           5.1          1.8  Virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILE = r'C:\\Users\\zizonkjs\\머신러닝,딥러닝\\data\\iris.csv'\n",
    "irisdf=pd.read_csv(DATA_FILE)\n",
    "irisdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0             5.1          3.5           1.4          0.2        0\n",
       "1             4.9          3.0           1.4          0.2        0\n",
       "2             4.7          3.2           1.3          0.2        0\n",
       "3             4.6          3.1           1.5          0.2        0\n",
       "4             5.0          3.6           1.4          0.2        0\n",
       "..            ...          ...           ...          ...      ...\n",
       "145           6.7          3.0           5.2          2.3        2\n",
       "146           6.3          2.5           5.0          1.9        2\n",
       "147           6.5          3.0           5.2          2.0        2\n",
       "148           6.2          3.4           5.4          2.3        2\n",
       "149           5.9          3.0           5.1          1.8        2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelenco = LabelEncoder()\n",
    "a=labelenco.fit_transform(irisdf['variety'])\n",
    "irisdf['variety']= a\n",
    "irisdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] - 모델 클레스 설계 및 정의 - <hr>\n",
    "어떤 클래스를 만들까? 고려\n",
    "클래스 목적 : iris.DataSet 학습 후 추론\n",
    "클래스 이름 : IrisMCFModel\n",
    "부모 클래스 : nn.Module\n",
    "매개 변수   : 층별 입출력 갯수 고정하기 때문에 필요 없음\n",
    "클래스 속성 : featureDF, targetDF, n_rows, n_features\n",
    "클래스 기능 : __init__() : 모델 구조, forward() : 순방향 학습 <= 오버라이딩(overriding) 상속관계일 때\n",
    "\n",
    "클래스 구조  \n",
    "    -입력층 : 피쳐 4개  퍼셉트론 : 50개(보통 입력 때 많이 주고 갈수록 줄임) (4,10)\n",
    "    -은닉층 : 입력 10개     출력 5개   (10,5)\n",
    "    -출력층 : 입력5개      타겟(이진분류) 1개    (5,1)  \n",
    "  \n",
    "-손실 함수/ 활성화 함수\n",
    "    *클래스 형태 ==> nn.MESLoss, nn.ReLU ==> __init__() 메서드에 사용\n",
    "    *함수 형태 ==> torch.nn.functional => forward()메서드에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisMCFModel(nn.Module):\n",
    "\n",
    "    # 모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(4, 10)\n",
    "        self.hidden_layer=nn.Linear(10, 5)\n",
    "        self.out_layer=nn.Linear(5, 3)\n",
    "    \n",
    "    # 순방향 학습 진행 메서드\n",
    "    def forward(self, input_data):\n",
    "        # 입력층\n",
    "        y=self.in_layer(input_data) # f1w1+f2w2+f3w3+b 요런 식이 10개(숫자10개)\n",
    "        y=F.relu(y)                   # 범위 0이상\n",
    "        \n",
    "        # 은닉층 : 10개의 숫자 받아오기\n",
    "        y=self.hidden_layer(y)\n",
    "        y=F.relu(y)\n",
    "\n",
    "        # 출력층 : 5개의 숫자 값 => 다중 분류 : 손실함수 CrossEntropyLoss가 내부에서 SoftMax를 처리해줌 (케라스나, 텐서플로우 쓸땐 써야함)\n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisMCFModel(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model = IrisMCFModel()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "IrisMCFModel                             [100, 3]                  --\n",
       "├─Linear: 1-1                            [100, 10]                 50\n",
       "├─Linear: 1-2                            [100, 5]                  55\n",
       "├─Linear: 1-3                            [100, 3]                  18\n",
       "==========================================================================================\n",
       "Total params: 123\n",
       "Trainable params: 123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.02\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터셋 클래스 설계 및 정의 <hr>\n",
    "- 데이터셋 : iris.csv\n",
    "- 피쳐개수 : 3개\n",
    "- 타겟개수 : 1개\n",
    "- 클래스이름 : IrisDataSet\n",
    "- 부모클래스 : utils.data.DataSet\n",
    "- 속성__필드 : featureDF, targetDF, n_rows, n_featrues  \n",
    "- 필수 메서드:   \n",
    "    *__init__(self) : 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정  \n",
    "    *__len__(self) : 데이터의 개수 반환  \n",
    "    *__getItem__(self, index) : 특정 인덱스의 피쳐와 타겟 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, featureDF, targetDF):\n",
    "        self.featureDF=featureDF\n",
    "        self.targetDF=targetDF\n",
    "        self.n_rows=featureDF.shape[0]\n",
    "        self.n_features=featureDF.shape[1]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 텐서화\n",
    "        featureTS=torch.FloatTensor(self.featureDF.iloc[index].values)\n",
    "        targetTS=torch.FloatTensor(self.targetDF.iloc[index].values)\n",
    "        return featureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "# [테스트] 데이터셋 인스턴스 생성\n",
    "featureDF = irisdf[irisdf.columns[:-1]] # 2D (150,3)\n",
    "targetDF = irisdf[irisdf.columns[-1:]] # 1D(150,1)\n",
    "\n",
    "\n",
    "irisDS=IrisDataset(featureDF,targetDF)\n",
    "\n",
    "# 데이터로더 인스턴스 생성\n",
    "irisDL = DataLoader(irisDS, batch_size=1)\n",
    "for feature, label in irisDL:\n",
    "    print(feature.shape, label.shape, feature, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비\n",
    "- 학습 횟수 : EPOCH ( 처음부터 끝까지 공부할 횟수 )\n",
    "- 배치 크기 : BATCH_SIZE(학습량)\n",
    "- 위치 지정 : DEVICE (텐서 저장 및 실행 위치 GPU/CPU)\n",
    "- 학 습 률  : 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정 0.001~0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행 관련 설정 값\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = 10\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스 : 모델, 데이터 셋, 최적화 (, 성능지표) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 4) (38, 4) (28, 4)\n",
      "(84, 1) (38, 1) (28, 1)\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스\n",
    "# 학습용, 검증용, 테스트용 데이터 분리\n",
    "model = IrisMCFModel()\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(featureDF, targetDF, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1)\n",
    "print(f'{X_train.shape} {X_test.shape} {X_val.shape}')\n",
    "print(f'{y_train.shape} {y_test.shape} {y_val.shape}')\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
    "trainDS = IrisDataset(X_train, y_train)\n",
    "testDS = IrisDataset(X_test, y_test)\n",
    "valDS = IrisDataset(X_val, y_val)\n",
    "\n",
    "# 학습용 데이터 로더 인스턴스\n",
    "trainDL=DataLoader(trainDS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화 & 손실함수 인스턴스 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 최적화 인스턴스 => w, b model.parameter 전달\n",
    "optimizer=optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "# 바이너리용 손실함수 인스턴스 => 다중분류\n",
    "# 예측값을 선형식 결과 값 전달함 ==> AF 처리 안함\n",
    "crossLoss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainDL), trainDL.__len__()\n",
    "\n",
    "### models 폴더 아래 프로젝트 폴더 아래 모델 파일을 저장\n",
    "import os\n",
    "\n",
    "# 저장 경로\n",
    "SAVE_PATH = '../models/iris/MCF/'\n",
    "\n",
    "# 저장 파일명\n",
    "SAVE_FILE = SAVE_PATH + 'model_train_wbs.pth'\n",
    "\n",
    "# 모델 구조 및 파라미터 모두 저장 파일명명\n",
    "SAVE_MODEL=SAVE_PATH+'model_all.pth'\n",
    "\n",
    "# 경로상 폴더 존재 여부 체크\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH) # 폴더 / 폴더 / ... 하위 폴더까지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT => 15.0\n",
      "성능 및 손실 개선이 없어서 학습 중단\n"
     ]
    }
   ],
   "source": [
    "# 학습의 효과 확인 손실값과 성능평가값 저장 필요\n",
    "LOSS_HISTORY, SCORE_HISTORY=[[],[]], [[],[]]\n",
    "CNT = irisDS.n_rows / BATCH_SIZE\n",
    "print(f'CNT => {CNT}')\n",
    "\n",
    "# 학습 모니터링 / 스케쥴링 설정\n",
    "# => LOSS_HISTORY, SCORE_HISTORY 활용\n",
    "# => 임계기준 : 10번\n",
    "BREAK_CNT = 0\n",
    "LIMIT_VALUE = 9\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    # 학습 모드로 모델 설정\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기만큼 데이터 로딩해서 학습 진행\n",
    "    loss_total, score_total = 0,0\n",
    "\n",
    "    for featureTS, targetTS in trainDL:\n",
    "\n",
    "        #학습 진행\n",
    "        pre_y=model(featureTS)\n",
    "\n",
    "        #손실 계산 : nn.CrossEntropyLoss 요구사항 : 정답/타겟은 0D 또는 1D, 타입은 long\n",
    "        loss=crossLoss(pre_y, targetTS.reshape(-1).long())\n",
    "        loss_total += loss.item()\n",
    "\n",
    "\n",
    "        #성능평가 계산\n",
    "        score=MulticlassF1Score(num_classes=3)(pre_y, targetTS.reshape(-1))\n",
    "        score_total += score.item()\n",
    "\n",
    "        #최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에폭당 검증 기능을 키겠다.\n",
    "    # 모델 검증 모드 설정\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 검증 데이터셋\n",
    "        val_featureTS=torch.FloatTensor(valDS.featureDF.values)\n",
    "        val_targetTS=torch.FloatTensor(valDS.targetDF.values)\n",
    "        \n",
    "        #평가\n",
    "        pre_val=model(val_featureTS)\n",
    "\n",
    "        #손실\n",
    "        loss_val=crossLoss(pre_val, val_targetTS.reshape(-1).long())\n",
    "\n",
    "        # 성능평가\n",
    "        score_val = MulticlassF1Score(num_classes=3)(pre_val, val_targetTS.reshape(-1))\n",
    "\n",
    "    # 에폭당 손실값과 성능평가 값 저장\n",
    "    LOSS_HISTORY[0].append(loss_total/BATCH_SIZE)\n",
    "    SCORE_HISTORY[0].append(score_total/BATCH_SIZE)\n",
    "\n",
    "    LOSS_HISTORY[1].append(loss_val)\n",
    "    SCORE_HISTORY[1].append(score_val)\n",
    "    # # 손실 기준\n",
    "    if len(LOSS_HISTORY[0]) >= 2:\n",
    "       if LOSS_HISTORY[1][-1] <= LOSS_HISTORY[1][-2] : BREAK_CNT +=1\n",
    "\n",
    "    # 성능 기준\n",
    "    if len(SCORE_HISTORY[1]) == 1: # 첫번째 횟수 저장\n",
    "       torch.save(model.state_dict(), SAVE_FILE)\n",
    "\n",
    "       # 모델 전체 저장\n",
    "       torch.save(model, SAVE_MODEL)\n",
    "    else:\n",
    "        if SCORE_HISTORY[1][-1] > max(SCORE_HISTORY[1][:-1]) : # 첫번째 점수랑 두번째 점수 비교 후 더 성능이 큰쪽을 저장\n",
    "            torch.save(model.state_dict(), SAVE_FILE)\n",
    "            torch.save(model, SAVE_MODEL)\n",
    "\n",
    "    # 성능이 좋은 학습 가중치 저장\n",
    "\n",
    "    # 학습중단\n",
    "    if BREAK_CNT >10:\n",
    "        print('성능 및 손실 개선이 없어서 학습 중단') \n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0515037894248962, 1.020784044265747, 0.9943773806095123, 0.9707448720932007, 0.9494476675987243, 0.9293474376201629, 0.9071545481681824, 0.8827544808387756, 0.8610588490962983, 0.8397171795368195, 0.8179551839828492, 0.7959527850151062, 0.7735464334487915, 0.7504265666007995, 0.7263639807701111, 0.7018619060516358, 0.6773197889328003, 0.6530724108219147, 0.6290122091770172, 0.6052096486091614, 0.5819330632686615, 0.5598554313182831, 0.5383984029293061, 0.5177782654762269, 0.4985188990831375, 0.4804923713207245, 0.46343210339546204, 0.4474304229021072, 0.43213953673839567, 0.41796465814113615, 0.4043865233659744, 0.39125095009803773, 0.3788309782743454, 0.3668397724628448, 0.3552189439535141, 0.34398168325424194, 0.33306857049465177, 0.3225870668888092, 0.3125759452581406, 0.3029536485671997, 0.29361981749534605, 0.2846150740981102, 0.275923889875412, 0.2676573500037193, 0.259690622985363, 0.2520039454102516, 0.24473300129175185, 0.2375758945941925, 0.23105234950780867, 0.22435995638370515, 0.21846287250518798, 0.21236534118652345, 0.2068829745054245, 0.20136866271495818, 0.1961175575852394, 0.19124168008565903, 0.18654575794935227, 0.18206732198596, 0.17784997522830964, 0.17376918643712996, 0.16995515301823616, 0.16637062579393386, 0.16284820958971977, 0.15958002507686614, 0.15647233799099922, 0.1534907639026642, 0.15065738409757615, 0.14797316938638688, 0.14541928693652154, 0.14298491328954696, 0.14066769555211067, 0.13842033892869948, 0.13641650453209878, 0.13446795083582402, 0.13237388357520102, 0.13045232817530633, 0.12875213138759137, 0.12704570107162, 0.12541139423847197, 0.12389092445373535, 0.12247132807970047, 0.12108825892210007, 0.11974974870681762, 0.11848110780119896, 0.11724391207098961, 0.11614051498472691, 0.11498549617826939, 0.1139509130269289, 0.11289487630128861, 0.11195329874753952, 0.11097856238484383, 0.11010794714093208, 0.10920836739242076, 0.10840673744678497, 0.10757508035749197, 0.10681424159556627, 0.10610538721084595, 0.10534834526479245, 0.10468559078872204, 0.10400213059037924, 0.10339244455099106, 0.1027503963559866, 0.10215695202350616, 0.10161921437829732, 0.10103441514074803, 0.100527049228549, 0.10000029355287551, 0.09951626379042863, 0.09904578439891339, 0.09858401827514171, 0.09813568498939276, 0.09770170152187348, 0.09728300981223584, 0.09688158556818963, 0.09649231042712927, 0.09611235298216343, 0.09574878066778184, 0.09539089743047953, 0.09504394792020321, 0.09471252858638764, 0.09438805636018514, 0.09407246727496385, 0.09376500826328993, 0.09346886947751046, 0.09318549213930964, 0.09290603008121252, 0.09263460217043758, 0.09237294821068645, 0.0921156695112586, 0.09186764936894179, 0.0916203242726624, 0.09138778317719698, 0.09116227831691504, 0.0909326495602727, 0.09071674104779959, 0.09051028033718467, 0.09029908580705523, 0.09009783864021301, 0.08990626744925975, 0.08971312176436186, 0.08952945731580257, 0.08935189172625542, 0.08917008275166154, 0.08900067377835512, 0.08883012467995285, 0.08866815604269504, 0.08851151661947369, 0.08835034435614944, 0.08819871498271822, 0.0880536699667573, 0.08790298756211996, 0.08776386268436909, 0.08762426674365997, 0.08748942352831364, 0.08735746080055833, 0.08722914680838585, 0.08709692768752575, 0.08697624467313289, 0.08685468854382634, 0.08673673309385777, 0.08662101123481988, 0.08650774769484997, 0.08639303557574748, 0.0862882467918098, 0.08617913015186787, 0.08607513578608632, 0.08597411122173071, 0.0858746406622231, 0.08577307593077421, 0.08568048542365432, 0.08558387272059917, 0.08549207355827093, 0.08540181107819081, 0.08531503742560745, 0.08522559627890587, 0.08514498821459711, 0.085051101539284, 0.08497894923202694, 0.08489099484868348, 0.08481062646023929, 0.08474319279193879, 0.0846583301667124, 0.08458973858505488, 0.08450803337618709, 0.08444592999294401, 0.08436606414616107, 0.084303327370435, 0.08423489700071514, 0.08415765352547169, 0.08410190269351006, 0.08403859836980701, 0.08396506090648473, 0.08391255848109722, 0.0838484420441091, 0.08378076804801822, 0.0837307594716549, 0.0836694116704166, 0.0836106896866113, 0.0835515064187348, 0.0835019770078361, 0.08344621509313584, 0.08339070505462587, 0.08333571096882224, 0.08328654891811312, 0.08323296760208905, 0.08318303967826068, 0.0831355644389987, 0.0830802295356989, 0.08304193099029362, 0.08299133302643895, 0.0829414107836783, 0.08289598254486918, 0.08285363633185625, 0.08280725921504199, 0.08276208252646029, 0.08271985896863043, 0.08267648750916123, 0.08263593316078185, 0.08259256691671908, 0.08255219119600951, 0.08251220448873937, 0.08247447395697236, 0.0824328443966806, 0.08239407893270254, 0.08235781695693731, 0.0823178815189749, 0.0822846848051995, 0.08224597340449691, 0.08220860394649207, 0.08217241251841187, 0.08214279683306813, 0.08210313394665718, 0.08206703416071832, 0.0820387096144259, 0.08200480733066798, 0.08196785449981689, 0.08193603702820837, 0.08190878182649612, 0.08187358388677239, 0.08184076971374452, 0.08181193340569734, 0.08178318128921092, 0.0817501574754715, 0.0817167367786169, 0.08169580670073628, 0.08165994491428137, 0.08163004168309271, 0.08160746758803725, 0.08157659573480487, 0.08155437419191003, 0.08151932423934341, 0.0814944208599627, 0.08146984060294926, 0.08144223373383283, 0.08141341009177268, 0.08139157295227051, 0.08136565471068025, 0.08133639497682452, 0.08131671831943095, 0.08129091477021574, 0.08126140534877777, 0.08124075355008245, 0.08121861526742577, 0.08119740383699536, 0.08116559218615294, 0.08114489605650306, 0.08112471750937403, 0.08110182154923677, 0.08108085363637656, 0.0810558311874047, 0.08103262828662991, 0.08101178219076247, 0.08099182988516987, 0.08097105359192938, 0.08094741601962596, 0.08092569063883275, 0.08090603491291404, 0.08088629920966924, 0.08086585968267172, 0.08084758978802711, 0.08082511541433632, 0.08080407776869833, 0.08078530309721828, 0.08076676276978105, 0.08074763375334441, 0.08073041806928813, 0.08070905874483288, 0.08068902445957064, 0.08067129773553461, 0.08065380125772208, 0.0806356365326792, 0.08061949240509421, 0.08059911190066486, 0.08058010048698634, 0.08056251178495585, 0.08054653629660606, 0.08053217446431518, 0.08051267419941724, 0.08049414767883718, 0.08047792401630431, 0.0804642513860017, 0.08044581713620573, 0.08042800533585251, 0.08041428122669458, 0.08039683008100837, 0.08037983679678291, 0.08036446864716709, 0.08035139767453074, 0.080333804176189, 0.08031695182435214, 0.08030207143165172, 0.08028964125551283, 0.08027266315184534, 0.0802562490804121, 0.08024185567628592, 0.08022780367173254, 0.08021540392655879, 0.0801985650556162, 0.08018275792710483, 0.08016900538932532, 0.08015756160020829, 0.08014158781152218, 0.08012620219960809, 0.08011268726550043, 0.08010147116146982, 0.0800860223127529, 0.0800710617331788, 0.08005794193595647, 0.0800472108181566, 0.08003203698899597, 0.08001746081281454, 0.08000400308519602, 0.0799938730429858, 0.07997955784667284, 0.07996552349068224, 0.07995312195271254, 0.07994291321374476, 0.07992859105579556, 0.07991477507166564, 0.07990272988099605, 0.07989375223405659, 0.07987913805991412, 0.07986501692794264, 0.07985321921296418, 0.07984393215738236, 0.07983038621023297, 0.07981733693741262, 0.07980581605806947, 0.07979658599942922, 0.07978320792317391, 0.07977044489234686, 0.07975930338725448, 0.07975038327276707, 0.07973734969273209, 0.07972476170398295, 0.07971394334454089, 0.07970518940128386, 0.07969247535802423, 0.07968020024709403, 0.07966956519521773, 0.07966109248809516, 0.07964862321969121, 0.07963664294220507, 0.07962625930085779, 0.07961624944582582, 0.07960758672561496, 0.0795950808795169, 0.0795833387877792, 0.0795733980834484, 0.07956548733636737, 0.07955360885243863, 0.07954213509801775, 0.07953224345110357, 0.07952448662836105, 0.07951277368701995, 0.079501598700881, 0.07949188621714712, 0.07948197356890888, 0.07947363371495157, 0.079462652746588, 0.07945226423908025, 0.07944372366182506, 0.07943420850206166, 0.07942354432307183, 0.0794140147510916, 0.07940488290041685, 0.07939806438516825, 0.07938650017604232, 0.07937485706061125, 0.07936616931110621, 0.07935872452799231, 0.07934956920798868, 0.0793389814440161, 0.07932951827533544, 0.07932103434577584, 0.07931419394444675, 0.07930415130686015, 0.07929368177428842, 0.07928394856862724, 0.07927590128965675, 0.07926780076231807, 0.0792597308056429, 0.07925036870874465, 0.07924198487307876, 0.07923186649568378, 0.07922291704453528, 0.07921557540539652, 0.07920725008007139, 0.0791976960375905, 0.07918826460372656, 0.0791823199018836, 0.07917314253281801, 0.07916399938985705, 0.07915581767447293, 0.07914884325582534, 0.07913931722287089, 0.07913089352659881, 0.07912308156955987, 0.07911517631728202, 0.07910708850249648, 0.0790988942841068, 0.07909076875075698, 0.07908275653608143, 0.07907571929972619, 0.07906631545629353, 0.07905821222811937, 0.07905079789925366, 0.07904327709693462, 0.07903543137945235, 0.07902751825749874, 0.07901963675394655, 0.07901191292330623, 0.07900423200335353, 0.07899657571688294, 0.07898888755589724, 0.07898125569336116, 0.07897459643427282, 0.07896556332707405, 0.07895791160408408, 0.07895087986253202, 0.07894372327718883, 0.0789362793089822, 0.07892868528142571, 0.0789212852017954, 0.07891386453993618, 0.07890650036279112, 0.07889929118100554, 0.07889197722543032, 0.07888469581957906, 0.07887740384321659, 0.07887024253141135, 0.07886303814593702, 0.07885589809156954, 0.078848752588965, 0.07884169928729534, 0.07883463632315398, 0.07882750034332275, 0.07882046902086585, 0.07881339557934552, 0.07880646688863635, 0.07879948734771461, 0.07879254915751517, 0.07878563809208572, 0.07877872628159821, 0.07877184839453548, 0.07876497393008322, 0.07875818223692477, 0.07875138237141073, 0.07874460604507476, 0.07873783928807825, 0.07873104463797062, 0.07872434176970274, 0.0787177101476118, 0.07871098816394806, 0.0787043236894533, 0.07869769535027445, 0.07869107630103826, 0.07868449399247765, 0.07867793722543866, 0.07867139079608024, 0.07866483521647752, 0.07865835290867836, 0.07865185232367367, 0.07864534920081497, 0.07863893043249845, 0.07863247003406286, 0.07862608372233808, 0.07861967573408038, 0.07861331191379577, 0.07860697053838521, 0.07860059086233377, 0.07859428662341088, 0.0785880048526451, 0.07858100631274283, 0.07857573309447616, 0.07857045244891196, 0.07856440742034465, 0.07855798702221364, 0.07855170690454542, 0.07854557943064719, 0.0785395368700847, 0.07853352243546396, 0.07852746255230159, 0.07852138911839575, 0.07851533037610352, 0.07850934963207692, 0.0785034307744354, 0.07849743396509438, 0.07849146155640482, 0.0784855238860473, 0.07847960307262838, 0.07847371934913099, 0.07846782938577235, 0.07846200787462294, 0.0784561330685392, 0.07845026773866266, 0.07844440876506269, 0.07843862336594612, 0.07843287556897849, 0.07842710837721825, 0.0784212989732623, 0.07841558924410492, 0.07840986128430813, 0.07840416398830712, 0.0783984491834417, 0.07839277614839375, 0.07838707591872662, 0.07838145107962191, 0.07837583671789616, 0.07837022575549782, 0.07836456731893122, 0.07835901139769703, 0.07835333966650068, 0.07834782090503722, 0.07834225718397647, 0.07833677830640226, 0.07833118243142963, 0.07832574581261724, 0.07832014127634465, 0.07831473478581756, 0.07830920906271785, 0.07830380029045045, 0.07829835063312203, 0.07829295857809485, 0.07828754745423794, 0.07828209204599261, 0.07827668497338891, 0.07827134262770415, 0.0782659683842212, 0.07826061730738729, 0.0782552735414356, 0.07824995717965066, 0.07824465692974628, 0.07823938033543527, 0.07823407985270023, 0.0782288106624037, 0.07822355041280389, 0.07821829135064036, 0.07821303298696876, 0.07820781474001706, 0.078202621685341, 0.07819739144761115, 0.07819224982522428, 0.07818707372061909, 0.07818190988618881, 0.07817676246631891, 0.07817158822435885, 0.07816648543812335, 0.07816134826280177, 0.07815627844538539, 0.07815116969868541, 0.07814607564359903, 0.07814097406808287, 0.07813595097977669, 0.07813087305985392, 0.07812583562918007, 0.07812080599833279, 0.07811577168758958, 0.07811075714416801, 0.07810576991178095, 0.07810080104973167, 0.07809580927714706, 0.07809085422195494, 0.07808592547662556, 0.07808094827923924, 0.07807601192034781, 0.07807109111454338, 0.07806617384776474, 0.07806132109835744, 0.07805642024613917, 0.078051503514871, 0.07804666131269186, 0.07804181615356356, 0.07803691460285336, 0.07803209747653454, 0.0780272554140538, 0.07802243269979954, 0.07801765787880868, 0.07801280058920383, 0.07800803894642741, 0.07800329471938312, 0.07799850152805447, 0.07799370614811778, 0.07798893735744059, 0.07798424891661852, 0.07797952080145479, 0.07797476656269282, 0.07797005705069751, 0.07796533589717, 0.07796064924914389, 0.07795593326445668, 0.07795265128370374, 0.07794671666342765, 0.07794127345550805, 0.07793693845160306, 0.07793277883902192, 0.07792829941026866, 0.07792355930432678, 0.07791885486803948, 0.07791422908194363, 0.07790968138724566, 0.07790513099171222, 0.07790052033960819, 0.07789600440301, 0.07789139624219388, 0.07788690258748829, 0.07788236127234996, 0.07787783574312926, 0.07787328367121518, 0.07786879348568618, 0.07786429550033062, 0.07785980904009193, 0.07785529324319214, 0.07785085351206363, 0.07784634488634765, 0.07784190766979009, 0.07783740337472408, 0.07783298082649708, 0.0778285430977121, 0.07782538183964789, 0.0778197071980685, 0.0778145189397037, 0.07781042561400682, 0.07780649464111775, 0.07780231772921979, 0.07779780707787723, 0.07779329298064112, 0.07778902016580105, 0.0777846782002598, 0.07778029055334627, 0.07777587489690632, 0.07777155988151208, 0.07776724826544523, 0.07776295262156055, 0.07775861828122288, 0.07775428418535739, 0.07775000676047057, 0.07774570276960731, 0.07774140940746292, 0.07773712541675196, 0.07773409120272845, 0.07772862303536385, 0.07772359090158716, 0.07771964058047161, 0.07771587192546577, 0.07771174914669246, 0.07770748009206727, 0.07770311434287577, 0.07769886262249201, 0.07769469803897663, 0.0776905128499493, 0.07768635437823831, 0.07768216895638033, 0.07767795637482777, 0.07767375338589773, 0.07766963971080258, 0.07766545680351555, 0.07766253933077678, 0.07765715613495558, 0.07765226808842271, 0.07764843757031485, 0.07764474195428192, 0.07764077689498664, 0.07763660838827491, 0.07763239584164694, 0.07762823150260374, 0.07762407531263307, 0.07762061741668731, 0.0776160730049014, 0.07761150595033542, 0.07760733873583377, 0.07760347588919103, 0.07759954539360478, 0.0775955580174923, 0.07759153610095382, 0.07758867123629898, 0.07758351827505976, 0.07757877432741225, 0.07757503594039009, 0.07757151203695685, 0.07756769744446501, 0.07756365110399202, 0.0775596356485039, 0.07755559977376833, 0.07755165792768821, 0.07754774411441759, 0.07754378316458314, 0.07753986525349318, 0.07753595990361646, 0.07753198891878128, 0.07752930930582806, 0.07752418883610517, 0.07751959601882845, 0.07751596288289875, 0.07751249631401151, 0.07750876549398526, 0.07750482967821881, 0.07750087019521743, 0.07749694881495088, 0.0774930840707384, 0.07748926564818248, 0.07748544622445479, 0.07748157951282338, 0.07747771901777015, 0.07747508108150214, 0.07747008233563975, 0.07746552859898656, 0.07746198816457764, 0.07745858493726701, 0.07745496627176181, 0.07745107350638136, 0.07744717609602958, 0.07744335897732527, 0.07743958908831701, 0.07743584734853357, 0.07743203530553729, 0.07742945372592658, 0.07742451457306743, 0.07742002819431945, 0.07741657689912244, 0.07741323828231543, 0.077409694774542, 0.07740585276624187, 0.07740204490255564, 0.07739828493213281, 0.07739455511327833, 0.07739082953194157, 0.07738715240266174, 0.07738460863474757, 0.07737976069329307, 0.07737533546751366, 0.07737195080844686, 0.07736867226194591, 0.0773651423631236, 0.07736142824869603, 0.07735760090872645, 0.07735393306938931, 0.07735025982838124, 0.07734664652962238, 0.07734413997968659, 0.07733934170100838, 0.07733499003807083, 0.07733168310951441, 0.07732845848659053, 0.07732492549112066, 0.07732125936308876, 0.07731754581909626, 0.07731386743253096, 0.07731031209696085, 0.07730673613259569, 0.07730428313370794, 0.07729935825336724, 0.0772950244951062, 0.07729186153737828, 0.07728883725358174, 0.07728542650584132, 0.07728180026169866, 0.07727806131588295, 0.07727451506070793, 0.07727103140205145, 0.07726746735861525, 0.07726394927594811, 0.07726045314921066, 0.07725686483317987, 0.07725334140704945, 0.07724982656072825, 0.07724747556494549, 0.07724284252617508, 0.07723863980500027, 0.07723534831311554, 0.07723235490266234, 0.07722897783387453, 0.07722541282419115, 0.07722088367445394, 0.07721873940899968, 0.07721456263680011, 0.07721120570786297, 0.0772089365287684, 0.0772042820812203, 0.07720064939931034, 0.0771984584396705, 0.07719482576940209, 0.07718988400883972, 0.07718683008570223, 0.07718485563527792, 0.0771802591625601, 0.07717658258043229, 0.07717434763908386, 0.07716987510211766, 0.07716725850477815, 0.07716286007780582, 0.07715947808464989, 0.07715749823255465, 0.07715300088748336, 0.07715029555838555, 0.07714589039096609, 0.07714258985361085, 0.07714062765007838, 0.077136144018732, 0.07713347919052467, 0.07712903952924535, 0.0771257473854348, 0.07712382121244446, 0.07711932219099253, 0.07711671511642634, 0.07711233848240226, 0.07710903638508171, 0.07710712720872834, 0.07710270585957915, 0.07710007936693727, 0.07709564621327444, 0.07709240549011157, 0.07709052449790761, 0.07708607765380293, 0.07708351404871791, 0.07707912749610842, 0.07707589773926884, 0.0770738514373079, 0.07706947417464108, 0.07706695129163563, 0.07706269173650071, 0.07705945076886564, 0.07705756644718349, 0.0770531413378194, 0.07705060417065397, 0.07704633716493844, 0.07704309179680421, 0.07704125058371573, 0.0770369192934595, 0.07703434475697576, 0.0770300134900026, 0.0770269144908525, 0.07702508548973128, 0.07702070700470358, 0.07701726355589926, 0.07701528870966286, 0.07701183578465134, 0.07700713496888056, 0.07700406332733109, 0.07700157949002459, 0.07699964654166251, 0.07699502182658761, 0.07699236287735403, 0.0769881529151462, 0.07698509381152689, 0.0769833731232211, 0.07697904400993139, 0.07697655962547287, 0.07697230082703754, 0.07696920838207007, 0.07696743615670129, 0.076963137218263, 0.07695975747192278, 0.07695784723619, 0.07695354874012991, 0.0769512218888849, 0.07694706816691906, 0.07694393178680911, 0.07694224434671923, 0.07693791629280895, 0.07693462995812297, 0.07693271968746558, 0.07692844587145373, 0.07692615168634802, 0.07692201600875706, 0.07691894538002089, 0.07691729553043843, 0.07691301841987297, 0.07690964923240244, 0.07690784786827862, 0.07690360092092305, 0.07690133118303492, 0.07689719969639555, 0.07689416373614222, 0.0768925284850411, 0.07688827170059084, 0.07688497137278319, 0.07688316635321826, 0.07687892781104892, 0.07687663871329278, 0.07687258995138109, 0.07686964825261385, 0.07686802461976186, 0.0768637910601683, 0.07686045810114592, 0.07685869380366057, 0.07685501511441543, 0.07685254500247538, 0.07684816900873556, 0.0768451594049111, 0.07684357727412135, 0.0768394380575046, 0.07683626735815778, 0.07683453455101699, 0.07683040627744049, 0.07682818396715448, 0.07682414792943745, 0.07682121237739921, 0.07681959539186209, 0.07681546239182353, 0.07681225014384836, 0.0768105153227225, 0.07680643658386543, 0.07680330061120913, 0.07680158937582746, 0.07679749160306529, 0.07679534635972232, 0.07679132120683789, 0.07678841578308493, 0.07678690160391852, 0.07678277763770894, 0.07677960745058954, 0.07677789127919823, 0.07677378316875547, 0.07677170725073665, 0.07676768506644294, 0.07676481866510584, 0.07676330340327694, 0.0767592036863789, 0.07675606018165126, 0.07675439486047253, 0.07675041964976118, 0.07674727640114724, 0.0767456392175518, 0.07674160588067025, 0.07673853975720704, 0.07673691607778892, 0.07673285903874785, 0.07673073625192046, 0.07672680106479675, 0.07672394043765962, 0.07672249518800527, 0.07671843867283315, 0.07671532947570085, 0.07671369891613722, 0.07670967782614753, 0.07670664018951356, 0.07670507233124227, 0.0767015598830767, 0.07669845264172181, 0.07669659983366728, 0.07669251014012843, 0.07669047430390492, 0.07668663791846483, 0.07668387331068516, 0.07668243125081062, 0.07667841241927817, 0.07667531233746558, 0.07667378493351862, 0.07666977636981756, 0.07666678273817525, 0.07666533445008099, 0.07666131297592074, 0.07665830923942848, 0.0766568245482631, 0.07665282571688295, 0.07664983052527533, 0.07664836950134486, 0.07664439374348149, 0.0766423459397629, 0.0766384897986427, 0.07663574422476813, 0.07663440111791715, 0.07663042366039008, 0.07662740364903584, 0.07662590291583911, 0.07662195805460215, 0.07661903633270413, 0.07661757322493941, 0.07661362471990288, 0.07661066707223654, 0.07660819510929287, 0.0766066635842435, 0.07660253075882792, 0.07659949846565724, 0.07659808069001883, 0.07659418638795615, 0.07659128191880882, 0.0765898396843113, 0.07658590541686863], [tensor(1.2806), tensor(1.2363), tensor(1.1969), tensor(1.1614), tensor(1.1273), tensor(1.0936), tensor(1.0489), tensor(1.0065), tensor(0.9678), tensor(0.9294), tensor(0.8915), tensor(0.8548), tensor(0.8194), tensor(0.7841), tensor(0.7484), tensor(0.7134), tensor(0.6795), tensor(0.6460), tensor(0.6132), tensor(0.5807), tensor(0.5490), tensor(0.5198), tensor(0.4923), tensor(0.4659), tensor(0.4415), tensor(0.4194), tensor(0.3988), tensor(0.3797), tensor(0.3617), tensor(0.3457), tensor(0.3316), tensor(0.3181), tensor(0.3060), tensor(0.2949), tensor(0.2844), tensor(0.2745), tensor(0.2651), tensor(0.2557), tensor(0.2468), tensor(0.2386), tensor(0.2307), tensor(0.2231), tensor(0.2156), tensor(0.2086), tensor(0.2019), tensor(0.1952), tensor(0.1890), tensor(0.1826), tensor(0.1772), tensor(0.1709), tensor(0.1660), tensor(0.1605), tensor(0.1559), tensor(0.1512), tensor(0.1465), tensor(0.1423), tensor(0.1382), tensor(0.1343), tensor(0.1306), tensor(0.1270), tensor(0.1237), tensor(0.1207), tensor(0.1175), tensor(0.1146), tensor(0.1118), tensor(0.1091), tensor(0.1065), tensor(0.1041), tensor(0.1017), tensor(0.0995), tensor(0.0974), tensor(0.0951), tensor(0.0934), tensor(0.0917), tensor(0.0897), tensor(0.0876), tensor(0.0860), tensor(0.0844), tensor(0.0829), tensor(0.0814), tensor(0.0801), tensor(0.0789), tensor(0.0777), tensor(0.0765), tensor(0.0752), tensor(0.0742), tensor(0.0731), tensor(0.0721), tensor(0.0711), tensor(0.0702), tensor(0.0692), tensor(0.0684), tensor(0.0675), tensor(0.0667), tensor(0.0659), tensor(0.0651), tensor(0.0645), tensor(0.0638), tensor(0.0631), tensor(0.0624), tensor(0.0618), tensor(0.0612), tensor(0.0606), tensor(0.0601), tensor(0.0595), tensor(0.0590), tensor(0.0584), tensor(0.0579), tensor(0.0575), tensor(0.0570), tensor(0.0566), tensor(0.0562), tensor(0.0557), tensor(0.0553), tensor(0.0549), tensor(0.0545), tensor(0.0542), tensor(0.0538), tensor(0.0534), tensor(0.0531), tensor(0.0528), tensor(0.0524), tensor(0.0521), tensor(0.0518), tensor(0.0515), tensor(0.0512), tensor(0.0509), tensor(0.0506), tensor(0.0504), tensor(0.0501), tensor(0.0499), tensor(0.0496), tensor(0.0494), tensor(0.0491), tensor(0.0489), tensor(0.0487), tensor(0.0484), tensor(0.0482), tensor(0.0480), tensor(0.0478), tensor(0.0476), tensor(0.0474), tensor(0.0472), tensor(0.0470), tensor(0.0468), tensor(0.0466), tensor(0.0465), tensor(0.0463), tensor(0.0461), tensor(0.0460), tensor(0.0458), tensor(0.0456), tensor(0.0455), tensor(0.0453), tensor(0.0452), tensor(0.0451), tensor(0.0449), tensor(0.0448), tensor(0.0446), tensor(0.0445), tensor(0.0444), tensor(0.0442), tensor(0.0441), tensor(0.0440), tensor(0.0439), tensor(0.0437), tensor(0.0436), tensor(0.0435), tensor(0.0434), tensor(0.0433), tensor(0.0432), tensor(0.0431), tensor(0.0430), tensor(0.0429), tensor(0.0428), tensor(0.0427), tensor(0.0426), tensor(0.0425), tensor(0.0424), tensor(0.0423), tensor(0.0422), tensor(0.0421), tensor(0.0420), tensor(0.0419), tensor(0.0419), tensor(0.0418), tensor(0.0417), tensor(0.0416), tensor(0.0415), tensor(0.0415), tensor(0.0414), tensor(0.0413), tensor(0.0413), tensor(0.0412), tensor(0.0411), tensor(0.0410), tensor(0.0410), tensor(0.0409), tensor(0.0408), tensor(0.0408), tensor(0.0407), tensor(0.0406), tensor(0.0406), tensor(0.0405), tensor(0.0405), tensor(0.0404), tensor(0.0403), tensor(0.0403), tensor(0.0402), tensor(0.0402), tensor(0.0401), tensor(0.0401), tensor(0.0400), tensor(0.0400), tensor(0.0399), tensor(0.0399), tensor(0.0398), tensor(0.0398), tensor(0.0397), tensor(0.0397), tensor(0.0396), tensor(0.0396), tensor(0.0395), tensor(0.0395), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0393), tensor(0.0393), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0391), tensor(0.0391), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0367), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0368), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0369), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0370), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0371), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0372), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0373), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0374), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0375), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0376), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0377), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0378), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0379), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0380), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0381), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0382), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0383), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0384), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0385), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0386), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0387), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0388), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0389), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0390), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0391), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0392), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0393), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0394), tensor(0.0395), tensor(0.0395), tensor(0.0395), tensor(0.0395), tensor(0.0395), tensor(0.0395), tensor(0.0395), tensor(0.0395), tensor(0.0395)]]\n",
      "[[0.14797924607992172, 0.14797924607992172, 0.14797924607992172, 0.14797924607992172, 0.14797924607992172, 0.14797924607992172, 0.14797924607992172, 0.18787823542952536, 0.3744059681892395, 0.4628956288099289, 0.5627130389213562, 0.714066869020462, 0.7757768273353577, 0.7757768273353577, 0.7757768273353577, 0.7757768273353577, 0.7757768273353577, 0.7572054028511047, 0.7572054028511047, 0.7757768273353577, 0.7757768273353577, 0.7685185313224793, 0.7870899558067321, 0.7964021265506744, 0.7964021265506744, 0.8160073399543762, 0.8160073399543762, 0.8160073399543762, 0.8160073399543762, 0.8160073399543762, 0.8160073399543762, 0.8160073399543762, 0.8160073399543762, 0.8160073399543762, 0.8160073399543762, 0.8308221518993377, 0.8400140643119812, 0.8492448329925537, 0.8492448329925537, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8633862495422363, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617, 0.8411640286445617], [tensor(0.1010), tensor(0.1010), tensor(0.1010), tensor(0.1010), tensor(0.1010), tensor(0.1010), tensor(0.1010), tensor(0.2540), tensor(0.4140), tensor(0.4848), tensor(0.8755), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(0.9485), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.)]]\n"
     ]
    }
   ],
   "source": [
    "print(LOSS_HISTORY)\n",
    "print(SCORE_HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 18.9187,   5.2703, -18.8482],\n",
      "        [ -1.9278,   4.2148,  -4.2200],\n",
      "        [ -7.2223,   4.9612,  -1.9384],\n",
      "        [ 17.0353,   4.7828, -17.0270],\n",
      "        [-18.3448,   3.6361,   5.3195],\n",
      "        [-10.4195,   4.2423,   0.4458],\n",
      "        [-18.0172,   2.5583,   6.4323],\n",
      "        [ 13.6307,   4.5253, -14.5874],\n",
      "        [ 14.1597,   4.1194, -14.3081],\n",
      "        [-21.2071,   1.9552,   8.7501],\n",
      "        [ -8.5818,   3.9609,  -0.1405],\n",
      "        [ 15.6935,   4.6575, -16.0098],\n",
      "        [-20.8517,   2.0887,   8.4725],\n",
      "        [ -8.4800,   4.6791,  -1.0274],\n",
      "        [-10.4382,   3.8449,   0.9597],\n",
      "        [ 14.4395,   4.1021, -14.5259],\n",
      "        [ -6.4030,   4.5034,  -1.8473],\n",
      "        [-11.0613,   3.5591,   1.5430],\n",
      "        [ 15.2688,   4.4610, -15.3658],\n",
      "        [ 15.7006,   4.4668, -15.7070],\n",
      "        [ -9.4698,   3.6928,   0.6326],\n",
      "        [-11.5397,   3.3477,   2.0101],\n",
      "        [-13.0852,   3.6622,   2.5997],\n",
      "        [ 15.7946,   4.4695, -15.8195],\n",
      "        [-17.9538,   3.0099,   5.8789],\n",
      "        [ -7.4215,   4.3271,  -1.1698],\n",
      "        [ 18.4284,   5.1078, -18.4097],\n",
      "        [ 16.2136,   4.5595, -16.2432],\n",
      "        [ -9.1288,   4.2313,  -0.1363],\n",
      "        [-17.4298,   2.1107,   6.6906],\n",
      "        [ -9.5930,   4.1859,   0.1190],\n",
      "        [-21.8630,   2.1686,   8.9400],\n",
      "        [ -5.7685,   4.4998,  -2.1750],\n",
      "        [-20.9046,   1.6287,   8.9202],\n",
      "        [-20.2828,   1.4555,   8.7334],\n",
      "        [ 16.7041,   4.7039, -16.7000],\n",
      "        [ -8.9815,   3.5534,   0.6072],\n",
      "        [ 16.0612,   4.5592, -16.0566]])\n"
     ]
    }
   ],
   "source": [
    "# 모델 테스트 모드 설정\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 검증 데이터셋\n",
    "    test_featureTS=torch.FloatTensor(testDS.featureDF.values)\n",
    "    test_targetTS=torch.FloatTensor(testDS.targetDF.values)\n",
    "        \n",
    "    #평가\n",
    "    pre_test=model(test_featureTS)\n",
    "    print(pre_test)\n",
    "\n",
    "    #손실\n",
    "    loss_test=crossLoss(pre_test, test_targetTS.reshape(-1).long())\n",
    "\n",
    "    # 성능평가\n",
    "    score_test = MulticlassF1Score(num_classes=3)(pre_test, test_targetTS.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
