{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torchmetrics 패키치\n",
    "- Pytorch에서 모델 성능평가 기능 제공 패키지\n",
    "- 추가 설치 필요\n",
    "    * pip : pip install torchmetrics\n",
    "    * conda : conda install -c conda-forge torchmetrics\n",
    "- 사용법\n",
    "    * 클래스 방식 : 인스턴스 생성 후 사용\n",
    "    * 함수 방식 : 바로 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1] 회귀(Regression) 성능 지표 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "from torchmetrics.functional.regression import r2_score\n",
    "from torchmetrics.functional.regression import mean_squared_error, mean_absolute_error\n",
    "from torchmetrics.regression import MeanSquaredError, MeanAbsoluteError\n",
    "from torchmetrics.regression import R2Score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9486)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 클래스 방식\n",
    "# 데이터\n",
    "target = torch.tensor([3, -0.5, 2, 7]) \n",
    "preds = torch.tensor([2.5, 0.0, 2, 8])\n",
    "\n",
    "# 성능평가 => R2Score :  0~1사이 (1에 가까울수록 좋음) 근사값\n",
    "r2score= R2Score()\n",
    "r2score(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9486)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R2Score 상속받기\n",
    "class MYR2(R2Score):\n",
    "    pass\n",
    "gg=MYR2()\n",
    "gg(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8750)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차계산 (0에 가까울수록 좋음)\n",
    "target = torch.tensor([2.5, 5.0, 4.0, 8.0])\n",
    "preds = torch.tensor([3.0, 5.0, 2.5, 7.0])\n",
    "mean_squared_error = MeanSquaredError()\n",
    "mean_squared_error(preds, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분류 (Classification) 성능평가지표\n",
    "    * Accuracy : TP, FN => 불균형 데이터 일 경우 신뢰성이 없음\n",
    "    * Precision : predict TP  \n",
    "    * Recall : 정답(P) 예측(T)\n",
    "    * F1-Score : 분류볼때\n",
    "    * confusionMatrix \n",
    "    * ROC-AUC -> 의료분야/ 2진분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝이나 머신러닝 모델을 평가할 때 사용하는 **성능 평가 지표**(Performance Metrics)는 모델이 얼마나 잘 동작하는지를 정량적으로 측정하는 기준입니다. 평가 지표는 모델의 목적과 문제의 유형(분류, 회귀 등)에 따라 달라집니다. 적절한 평가 지표를 선택하는 것이 매우 중요하며, 모델의 성능을 올바르게 이해하고 개선할 수 있는 방향을 제시합니다.\n",
    "\n",
    "### 1. **분류(Classification) 문제에서 사용되는 성능 평가 지표**\n",
    "\n",
    "#### 1.1 **정확도 (Accuracy)**\n",
    "- **정의**: 모델이 맞춘 샘플의 비율을 나타냅니다.\n",
    "- **계산**: `(올바르게 예측한 샘플 수) / (전체 샘플 수)`\n",
    "  \n",
    "  예를 들어, 100개의 샘플 중 90개를 맞췄다면 정확도는 90%입니다.\n",
    "\n",
    "- **특징**: 클래스가 불균형한 경우(예: 클래스 A가 전체 데이터의 99%를 차지하는 경우)에는 정확도만으로는 모델의 성능을 제대로 평가하기 어려울 수 있습니다.\n",
    "\n",
    "#### 1.2 **정밀도 (Precision)**\n",
    "- **정의**: 모델이 **양성**(Positive)이라고 예측한 샘플 중 실제로 **양성**인 비율입니다.\n",
    "- **계산**: `TP / (TP + FP)`\n",
    "  - **TP (True Positive)**: 양성 샘플을 양성으로 정확히 예측한 경우\n",
    "  - **FP (False Positive)**: 실제 음성 샘플을 양성으로 잘못 예측한 경우\n",
    "\n",
    "- **특징**: 정밀도는 **False Positive**를 줄이는 데 중점을 둔 지표입니다. 예를 들어, 스팸 필터에서 정밀도는 중요합니다. 스팸이 아닌 이메일을 스팸으로 분류하지 않는 것이 중요할 때 사용됩니다.\n",
    "\n",
    "#### 1.3 **재현율 (Recall, Sensitivity)**\n",
    "- **정의**: 실제로 **양성**인 샘플 중에서 모델이 **양성**으로 올바르게 예측한 비율입니다.\n",
    "- **계산**: `TP / (TP + FN)`\n",
    "  - **FN (False Negative)**: 실제 양성 샘플을 음성으로 잘못 예측한 경우\n",
    "\n",
    "- **특징**: 재현율은 **False Negative**를 줄이는 데 중점을 둡니다. 예를 들어, 질병 진단에서 실제로 질병이 있는 환자를 놓치지 않는 것이 중요할 때 사용됩니다.\n",
    "\n",
    "#### 1.4 **F1 Score**\n",
    "- **정의**: 정밀도와 재현율의 조화 평균을 구한 값으로, 두 지표 간의 균형을 평가합니다.\n",
    "- **계산**: `2 * (Precision * Recall) / (Precision + Recall)`\n",
    "  \n",
    "- **특징**: F1 Score는 정밀도와 재현율이 모두 중요할 때, 즉, 클래스 불균형 문제가 있는 경우 유용한 평가 지표입니다.\n",
    "\n",
    "#### 1.5 **ROC Curve 및 AUC (Area Under the ROC Curve)**\n",
    "- **ROC 곡선**: 모델의 **양성 클래스에 대한 예측 성능**을 시각적으로 표현하는 곡선입니다. **True Positive Rate**와 **False Positive Rate**의 관계를 나타냅니다.\n",
    "  \n",
    "  - **True Positive Rate (TPR, 재현율)**: `TP / (TP + FN)`\n",
    "  - **False Positive Rate (FPR)**: `FP / (FP + TN)`\n",
    "\n",
    "- **AUC (Area Under the ROC Curve)**: ROC 곡선 아래의 면적으로, AUC 값이 1에 가까울수록 성능이 좋음을 나타냅니다.\n",
    "\n",
    "- **특징**: 분류 경계가 달라질 때, 모델의 성능 변화를 분석하는 데 유용합니다.\n",
    "\n",
    "### 2. **회귀(Regression) 문제에서 사용되는 성능 평가 지표**\n",
    "\n",
    "#### 2.1 **Mean Squared Error (MSE, 평균 제곱 오차)**\n",
    "- **정의**: 예측값과 실제 값의 차이를 제곱한 후 평균을 구한 값입니다.\n",
    "- **계산**: `(1/n) * Σ(y_true - y_pred)^2`\n",
    "  - **y_true**: 실제 값\n",
    "  - **y_pred**: 예측 값\n",
    "\n",
    "- **특징**: 오차가 클수록 그 영향이 제곱에 의해 커지기 때문에, 큰 오차에 민감합니다.\n",
    "\n",
    "#### 2.2 **Root Mean Squared Error (RMSE, 평균 제곱근 오차)**\n",
    "- **정의**: MSE의 제곱근을 취한 값입니다.\n",
    "- **계산**: `sqrt(MSE)`\n",
    "  \n",
    "- **특징**: MSE와 비슷하지만, 단위가 예측값과 동일해 해석이 더 쉬울 수 있습니다.\n",
    "\n",
    "#### 2.3 **Mean Absolute Error (MAE, 평균 절대 오차)**\n",
    "- **정의**: 예측값과 실제 값 사이의 절대 오차의 평균입니다.\n",
    "- **계산**: `(1/n) * Σ|y_true - y_pred|`\n",
    "  \n",
    "- **특징**: MAE는 MSE에 비해 큰 오차에 덜 민감하며, **오차의 크기**에 대한 직관적인 정보를 제공합니다.\n",
    "\n",
    "#### 2.4 **R² Score (결정 계수)**\n",
    "- **정의**: 모델이 데이터를 얼마나 잘 설명하는지를 나타내는 지표입니다.\n",
    "- **계산**: `1 - (SS_res / SS_tot)`\n",
    "  - **SS_res**: 예측값과 실제 값의 차이에 대한 제곱합 (Residual Sum of Squares)\n",
    "  - **SS_tot**: 실제 값과 평균 값의 차이에 대한 제곱합 (Total Sum of Squares)\n",
    "\n",
    "- **특징**: R² 값은 0에서 1 사이의 값을 가지며, 1에 가까울수록 모델이 데이터를 잘 설명함을 의미합니다. \n",
    "\n",
    "### 3. **클러스터링(Clustering) 문제에서 사용되는 성능 평가 지표**\n",
    "\n",
    "#### 3.1 **Silhouette Score (실루엣 점수)**\n",
    "- **정의**: 클러스터링의 품질을 평가하는 지표로, 한 클러스터 내의 데이터들이 얼마나 가깝게 모여 있는지와 다른 클러스터와 얼마나 떨어져 있는지를 측정합니다.\n",
    "- **계산**: `[(b - a) / max(a, b)]`\n",
    "  - **a**: 같은 클러스터 내에서의 평균 거리\n",
    "  - **b**: 가장 가까운 다른 클러스터와의 평균 거리\n",
    "\n",
    "- **특징**: 실루엣 점수는 -1에서 1 사이의 값을 가지며, 1에 가까울수록 클러스터링이 잘 되었음을 의미합니다.\n",
    "\n",
    "#### 3.2 **Adjusted Rand Index (조정된 랜드 지수)**\n",
    "- **정의**: 클러스터링 결과와 실제 레이블 간의 일치성을 평가하는 지표입니다.\n",
    "  \n",
    "- **특징**: 랜덤 클러스터링에 대해 0의 값을 가지며, 완벽한 일치에 대해서는 1의 값을 가집니다.\n",
    "\n",
    "### 4. **기타 평가 지표**\n",
    "\n",
    "#### 4.1 **Log Loss (로그 손실)**\n",
    "- **정의**: 이진 또는 다중 클래스 분류 문제에서 예측 확률이 얼마나 정확한지를 평가하는 지표입니다.\n",
    "- **계산**: `- (1/n) * Σ(y_true * log(y_pred))`\n",
    "  \n",
    "- **특징**: 확률을 기반으로 평가하며, 잘못된 예측에 대해 페널티를 부여합니다.\n",
    "\n",
    "#### 4.2 **Confusion Matrix (혼동 행렬)**\n",
    "- **정의**: 분류 문제에서 모델의 예측 결과를 시각화하는 도구입니다. 각 클래스에 대한 **True Positive, False Positive, True Negative, False Negative** 값을 정리하여 성능을 평가합니다.\n",
    "\n",
    "- **특징**: 모델의 예측 오류 패턴을 파악하는 데 유용하며, 특히 불균형 데이터에서 중요합니다.\n",
    "\n",
    "### 5. **사용 상황에 따른 지표 선택**\n",
    "\n",
    "#### 5.1 **불균형 데이터**\n",
    "- 클래스가 불균형한 경우에는 **정확도**보다는 **정밀도, 재현율, F1 Score** 등의 지표가 중요합니다. 정확도만으로 모델의 성능을 평가하면, 작은 클래스에 대한 성능을 제대로 반영하지 못할 수 있기 때문입니다.\n",
    "\n",
    "#### 5.2 **회귀 문제**\n",
    "- 회귀 문제에서는 **MSE**, **MAE**, **R² Score**가 주로 사용됩니다. MSE는 큰 오차에 민감하고, MAE는 오차 크기 자체에 대한 해석이 쉽습니다.\n",
    "\n",
    "#### 5.3 **확률 예측 모델**\n",
    "- 확률 예측을 수행하는 모델에서는 **Log Loss**나 **ROC-AUC** 지표가 중요합니다. 특히 다중 클래스 분류에서는 **Cross Entropy**와 **Log Loss**가 주로 사용됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 요약\n",
    "\n",
    "- **분류 문제**에서는 **\n",
    "\n",
    "정확도**, **정밀도**, **재현율**, **F1 Score**, **ROC-AUC** 등의 지표가 사용됩니다.\n",
    "- **회귀 문제**에서는 **MSE**, **MAE**, **R² Score** 같은 지표가 사용됩니다.\n",
    "- **클러스터링 문제**에서는 **실루엣 점수**와 **조정된 랜드 지수** 등이 사용됩니다.\n",
    "- 상황에 맞는 **평가 지표**를 선택하는 것이 중요하며, 불균형 데이터나 확률 예측 모델에서는 특정 지표가 더 유용할 수 있습니다.\n",
    "\n",
    "성능 평가 지표를 잘 이해하면, 모델의 성능을 보다 정확하게 평가하고 개선할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈로딩\n",
    "from torchmetrics.classification import F1Score, Accuracy, Precision, Recall\n",
    "from torchmetrics.functional.classification import f1_score, accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 프레임워크 ==> 개발에 필요한 모든것을 포함하고 있는 집합체  \n",
    "ex) 머신러닝 알고리즘 패키지가 있고, 성능평가하는 평가 패키지도 있고, 전처리해주는 패키지도 있고, 데이터셋 패키지도 있고, 특성공학 패키지....  \n",
    "\n",
    "- 인터페이스  ==> 서로 다른 요소/장치/사람 연결 시켜주는 것  \n",
    "ex) UI(UserInterface) : 사람(사용자)와 컴퓨터/핸드폰/SW/HW/ 연결시켜줌 ==> 버튼, 글자 입력란, 그림  \n",
    "==> OOP(객체지향) 개념에서 서로 다른 객체들이 동일한 기능을 사용 할 수 있도록 해주는 기능  \n",
    "\n",
    "\n",
    "- 모듈 => 같은 목적의 변수, 함수, 클래스를 포함한 1개 파일(py)  \n",
    "ex) url 관련 중에서 웹에 요청 목적의 변수, 함수, 클래스  \n",
    "    웹에서 받은 응답을 처리하는 목적의 변수, 함수, 클래스  \n",
    "    웹 주소 분석 목적의 변수, 함수, 클래스\n",
    "- 패키지 => 같은 분야나 유사한 분야의 모듈을 모아서 하나로 합친 것  \n",
    "ex) 웹 패키지 => 요청 처리 모듈, 응답 처리 모듈, 주소 분석 모듈, 데이터 파일 분석 모듈\n",
    "\n",
    "- IDE 통합개발환경 => 개발에 필요한 모든 도구들을 제공해주는 SW  \n",
    "ex) 코드 작성, 디버깅, 필요 패키지 검색 및 설치 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8571)\n",
      "tensor(0.8571)\n"
     ]
    }
   ],
   "source": [
    "# 2진 분류\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "target = torch.tensor([0, 1, 1, 0, 1, 1])\n",
    "preds = torch.tensor([0, 1, 1, 0, 0, 1])\n",
    "\n",
    "f1 = F1Score(task=\"binary\")\n",
    "print(f1(preds, target))\n",
    "\n",
    "binaryf1 = BinaryF1Score()\n",
    "print(binaryf1(preds, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.8571)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import BinaryPrecision\n",
    "target = torch.tensor([0, 1, 1, 0, 1, 1])\n",
    "preds = torch.tensor([0, 1, 1, 0, 0, 1])\n",
    "\n",
    "pb = Precision(task=\"binary\", threshold=0.1)\n",
    "print(pb(preds, target))\n",
    "\n",
    "binaryp = BinaryPrecision(threshold=0.1)\n",
    "print(binaryf1(preds, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8286)\n",
      "tensor(0.8286)\n"
     ]
    }
   ],
   "source": [
    "# 다중 분류\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "target = torch.tensor([0, 1, 1, 0, 1, 1])\n",
    "preds = torch.tensor([0, 1, 1, 0, 0, 1])\n",
    "\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3, average='macro') # average='micro' 기본값이기 때문에 macro를 입력해서 값을 동일하게 맞춰줬음.\n",
    "print(f1(preds, target))\n",
    "\n",
    "binaryf1 = MulticlassF1Score(num_classes=3) # average = 'macro' 기본값\n",
    "print(binaryf1(preds, target))\n",
    "\n",
    "# 기본 값을 기준으로 출력하면 두 값이 다르게 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8333)\n",
      "tensor(0.8333)\n"
     ]
    }
   ],
   "source": [
    "# 다중 분류\n",
    "from torchmetrics.classification import MulticlassPrecision\n",
    "target = torch.tensor([0, 1, 1, 0, 1, 1])\n",
    "preds = torch.tensor([0, 1, 1, 0, 0, 1])\n",
    "\n",
    "pb1 = Precision(task=\"multiclass\", num_classes=3, average='macro') # average='micro' 기본값이기 때문에 macro를 입력해서 값을 동일하게 맞춰줬음.\n",
    "print(pb1(preds, target))\n",
    "\n",
    "mutlP = MulticlassPrecision(num_classes=3) # average = 'macro' 기본값\n",
    "print(mutlP(preds, target))\n",
    "\n",
    "# 기본 값을 기준으로 출력하면 두 값이 다르게 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
